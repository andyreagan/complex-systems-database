Calculation of the Autocorrelation Function of the Stochastic Single Machine Infinite Bus System:::Critical slowing down (CSD) is the phenomenon in which a system recovers more slowly from small perturbations. CSD, as evidenced by increasing signal variance and autocorrela- tion, has been observed in many dynamical systems approaching a critical transition, and thus can be a useful signal of proximity to transition. In this paper, we derive autocorrelation functions for the state variables of a stochastic single machine infinite bus system (SMIB). The results show that both autocorrelation and variance increase as this system approaches a saddle-node bifurcation. The autocorrelation functions help to explain why CSD can be used as an indicator of proximity to criticality in power systems revealing, for example, how nonlinearity in the SMIB system causes these signs to appear.::::::Unpublished::::::http://www.uvm.edu/~phines/publications/2013/ghanavati_2013.pdf
Packetized Plug-in Electric Vehicle Charge Management:::Plug-in electric vehicle (PEV) charging could cause significant strain on residential distribution systems, unless technologies and incentives are created to mitigate charging during times of peak residential consumption. This paper describes and evaluates a decentralized and ‘packetized’ approach to PEV charge management, in which PEV charging is requested and approved for time-limited periods. This method, which is adapted from approaches for bandwidth sharing in communication networks, simultaneously ensures that constraints in the distribution network are satisfied, that communication bandwidth requirements are relatively small, and that each vehicle has fair access to the available power capacity. This paper compares the performance of the packetized approach to an optimization method and a first-come, first- served (FCFS) charging scheme in a test case with a constrained 500 kVA distribution feeder and time-of-use residential electricity pricing. The results show substantial advantages for the packetized approach. The algorithm provides all vehicles with equal access to constrained resources and attains near optimal travel cost performance, with low complexity and communication requirements. The proposed method does not require that vehicles report or record driving patterns, and thus provides benefits over optimization approaches by preserving privacy and reducing computation and bandwidth requirements.::::::Unpublished::::::http://www.uvm.edu/~phines/publications/2014/rezaei_2014.pdf
Dual Graph and Random Chemistry methods for Cascading Failure Analysis:::This paper describes two new approaches to cascading failure analysis in power systems that can combine large amounts of data about cascading blackouts to produce information about the ways that cascades may propagate. In the first, we evaluate methods for representing cascading failure information in the form of a graph. We refer to these graphs as dual graphs because the vertices are the transmission lines (the physical links), rather than the more conventional approach of representing power system buses as vertices. Examples of these ideas using the IEEE 30 bus system indicate that the dual graph methods can provide useful insight into how cascades propagate. In the second part of the paper we describe a random chemistry algorithm that can search through the enormous space of possible combinations of potential component outages to efficiently find large collections of the most dangerous combinations. This method was applied to a power grid with 2896 transmission branches, and provides insight into component outages that are notably more likely than others to trigger a cascading failure. In the conclusions we discuss potential uses of these methods for power systems planning and operations.::::::Unpublished::::::http://www.uvm.edu/~phines/publications/2013/hines_2013_dg.pdf
Predicting Critical Transitions from Time Series Synchrophasor Data:::The dynamical behavior of power systems under stress frequently deviates from the predictions of deterministic models. Model-free methods for detecting signs of excessive stress before instability occurs would therefore be valuable. The mathematical frameworks of fast-slow systems and critical slowing down can describe the statistical behavior of dynamical systems that are subjected to random perturbations as they approach points of instability. This paper builds from existing literature on fast-slow systems to provide evidence that time series data alone can be useful to estimate the temporal distance of a power system to a critical transition, such as voltage collapse. Our method is based on identifying evidence of critical slowing down in a single stream of synchronized phasor measurements. Results from a single machine, stochastic infinite bus model, a three machine/nine bus system and the Western North American disturbance of 10 August 1996 illustrate the utility of the proposed method.::::::Unpublished::::::http://www.uvm.edu/~phines/publications/2012/cotilla-sanchez_2012.pdf
A Random Chemistry Algorithm for Identifying Collections of Multiple Contingencies that Initiate Cascading Failure:::This paper describes a stochastic Random Chemistry (RC) algorithm to identify large collections of multiple (n-k) contingencies that initiate large cascading failures in a simulated power system. The method requires only O(log (n)) simulations per contingency identified, which is orders of magnitude faster than random search of this combinatorial space. We applied the method to a model of cascading failure in a power network with n=2896 branches and identify 148243 unique, minimal n-k branch contingencies (2 &#8804; k &#8804; 5) that cause large cascades, many of which would be missed by using pre-contingency flows, linearized line outage distribution factors, or performance indices as screening factors. Within each n-k collection, the frequency with which individual branches appear follows a power-law (or nearly so) distribution, indicating that a relatively small number of components contribute disproportionately to system vulnerability. The paper discusses various ways that RC generated collections of dangerous contingencies could be used in power systems planning and operations.::::::Unpublished::::::http://www.uvm.edu/~phines/publications/2012/eppstein_2012.pdf
Comparing the Topological and Electrical Structure of the North American Electric Power Infrastructure:::The topological (graph) structure of complex networks often provides valuable information about the performance and vulnerability of the network. However, there are multiple ways to represent a given network as a graph. Electric power transmission and distribution networks have a topological structure that is straightforward to represent and analyze as a graph. However, simple graph models neglect the comprehensive connections between components that result from Ohm's and Kirchhoff's laws. This paper describes the structure of the three North American electric power interconnections, from the perspective of both topological and electrical connectivity. We compare the simple topology of these networks with that of random (Erdos and Renyi, 1959), preferential-attachment (Barabasi and Albert, 1999) and small-world (Watts and Strogatz, 1998) networks of equivalent sizes and find that power grids differ substantially from these abstract models in degree distribution, clustering, diameter and assortativity, and thus conclude that these topological forms may be misleading as models of power systems. To study the electrical connectivity of power systems, we propose a new method for representing electrical structure using electrical distances rather than geographic connections. Comparisons of these two representations of the North American power networks reveal notable differences between the electrical and topological structure of electric power networks.::::::Unpublished:::http://arxiv.org/abs/1105.0214:::
Risk Assessment of Cascading Outages: Methodologies and Challenges:::Cascading outages can cause large blackouts, and a variety of methods are emerging to study this challenging topic. The Task Force on Understanding, Prediction, Mitigation, and Restoration of Cascading Failures, under the IEEE PES Computer Analytical Methods Subcommittee (CAMS), seeks to consolidate and review the progress of the field towards methods and tools of assessing the risk of cascading failure. This paper discusses the challenges of cascading failure and summarizes a variety of state-of-the-art analysis and simulation methods, including analyzing observed data, and simulations relying on various probabilistic, deterministic, approximate, and heuristic approaches. Limitations to the interpretation and application of analytical results are highlighted, and directions and challenges for future developments are discussed.::::::Unpublished::::::http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6112807
Cascading Failures: Extreme Properties of Large Blackouts in the Electric Grid:::Power grids are almost universally agreed to be complex systems, which means that it is not possible to fully understand the grid by just looking at its parts. Power grids, which we define here to include all of the physical infrastructure and human individuals and organizations that jointly work to produce, distribute and consume electricity, have many properties that are common to other complex systems. Like the international financial system, power grids are operated by many millions of physical (hardware/software) and human agents. Like the Internet, power systems are frequently subjected to both random failure and malicious attack. Like the weather systems interacting to form hurricanes, there are strong, non-linear connections among the components, and between the components and society at large. And power systems occasionally exhibit spectacularly large, and costly, failures. This essay attempts to help us to understand these failures by highlighting key mathematical properties of cascading failures in complex systems in general, and in power grids in particular. We focus particularly on the mathematical challenges of measuring cascading failure risk in large power grids, and discuss some techniques that may provide better information to power grid operators regarding cascading failure risk.::::::Published::::::http://www.mathaware.org/mam/2011/essays/complexsystemsHines.pdf
Do topological models provide good information about vulnerability in electric power networks?:::In order to identify the extent to which results from topological graph models are useful for modeling vulnerability in electricity infrastructure, we measure the susceptibility of power networks to random failures and directed attacks using three measures of vulnerability: characteristic path lengths, connectivity loss and blackout sizes. The first two are purely topological metrics. The blackout size calculation results from a model of cascading failure in power networks. Testing the response of 40 areas within the Eastern US power grid and a standard IEEE test case to a variety of attack/failure vectors indicates that directed attacks result in larger failures using all three vulnerability measures, but the attack vectors that appear to cause the most damage depend on the measure chosen. While our topological and power grid model results show some trends that are similar, there is only a mild correlation between the vulnerability measures for individual simulations. We conclude that evaluating vulnerability in power networks using purely topological metrics can be misleading.::::::Published:::http://arxiv.org/abs/1002.2268:::
Cascading failures in power grids:::Power grids are complex dynamical systems, and because of this complexity it is unlikely that we will completely eliminate blackouts. However, there are things that can be done to reduce the average size and cost of these blackouts. In this article we described two strategies that hold substantial promise for reducing the size and cost of blackouts. Both reciprocal altruism and survivability respect the necessarily decentralized nature of power grids. Both strategies can be implemented within the context of the existing physical infrastructure of the power grids,which is important because dramatic changes to the physical infrastructure are prohibitively expensive. However, additional engineering and innovation will be needed to bring strategies such as these to implementation and to create power grids with smaller, less costly blackouts.::::::Published::::::http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5235532
Reciprocally Altruistic Agents for the Mitigation of Cascading Failures in Power Grids:::Cascading failures in electrical power networks often come with disastrous consequences. A variety of schemes for mitigating cascading failures exist, but the vast majority depend upon centralised control architectures. Centralised designs are frequently more susceptible to communications latency and bandwidth limitations and can be vulnerable to random failures and directed attacks. This paper proposes a decentralised approach. We place control agents at each substation in a power network, each of which uses decentralised Model Predictive Control (MPC) to select emergency control actions. When making decisions, the control agents consider not only their own goals, but also the goals of nearby agents. Thus the agents act with Reciprocal Altruism (RA). Results from simulations of extreme cascading failures within the IEEE 300 bus test network indicate that this approach can dramatically reduce the average size and social cost of large cascading failures. Simulations also show that the bandwidth required for message passing is well within the limits of current technology.::::::Published::::::http://www.inderscience.com/offer.php?id=29113
Large Blackouts in North America: Historical Trends and Policy Implications:::Using data from the North American Electric Reliability Council (NERC) for 1984–2006, we find several trends. We find that the frequency of large blackouts in the United States has not decreased over time, that there is a statistically significant increase in blackout frequency during peak hours of the day and during late summer and mid-winter months (although non-storm-related risk is nearly constant through the year) and that there is strong statistical support for the previously observed power-law statistical relationship between blackout size and frequency. We do not find that blackout sizes and blackout durations are significantly correlated. These trends hold even after controlling for increasing demand and population and after eliminating small events, for which the data may be skewed by spotty reporting. Trends in blackout occurrences, such as those observed in the North American data, have important implications for those who make investment and policy decisions in the electricity industry. We provide a number of examples that illustrate how these trends can inform benefit-cost analysis calculations. Also, following procedures used in natural disaster planning we use the observed statistical trends to calculate the size of the 100-year blackout, which for North America is 186,000 MW.::::::Published::::::http://www.sciencedirect.com/science/article/pii/S0301421509005667
Controlling Cascading Failures with Cooperative Autonomous Agents:::Cascading failures in electricity networks often result in large blackouts with severe social consequences. A cascading failure typically begins with one or more equipment outages that cause operating constraint violations. When violations persist in a network, they can trigger additional outages which in turn may cause further violations. This paper proposes a method for limiting the social costs of cascading failures by eliminating violations before dependent outages occur. Specifically, our approach places one autonomous software agents at each bus of a power network, each of which is tasked with solving the global control problem with limited data and communication. Each agent builds a simplified model of the network based on locally available data and solves its local problem using model predictive control and cooperation. Through extensive simulations with IEEE test networks, we find that the autonomous agent design meets its goals with limited communication. Experiments also demonstrate that allowing agents to cooperate can vastly improve system performance.::::::Published::::::http://www.inderscience.com/offer.php?id=11551
Estimating the Impact of Electric Vehicle Smart Charging on Distribution Transformer Aging:::This paper describes a method for estimating the impact of plug-in electric vehicle (PEV) charging on overhead distribution transformers, based on detailed travel demand data and under several different schemes for mitigating overloads by shifting PEV charging times (smart charging). The paper also presents a new smart charging algorithm that manages PEV charging based on estimated transformer temperatures. We simulated the varied behavior of drivers from the 2009 National Household Transportation Survey, and transformer temperatures based an IEEE standard dynamic thermal model. Results are shown for Monte Carlo simulation of a 25 kVA overhead distribution transformer, with ambient temperature data from hot and cold climate locations, for uncontrolled and several smart-charging scenarios. These results illustrate the substantial impact of ambient temperatures on distribution transformer aging, and indicate that temperature-based smart charging can dramatically reduce both the mean and variance in transformer aging without substantially reducing the frequency with which PEVs obtain a full charge. Finally, the results indicate that simple smart charging schemes, such as delaying charging until after midnight can actually increase, rather than decrease, transformer aging.::::::Unpublished::::::http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6378418
Estimating the impact of fuel-switching between liquid fuels and electricity under electricity-sector carbon&#8208;pricing schemes:::Switching from liquid fuels to electricity in the transportation and heating sectors can result in greenhouse gas emissions reductions. These reductions are maximized when electricity-sector carbon emissions are constrained through policy measures. We use a linear optimization, generation expansion/dispatch model to evaluate the impact of increased electricity demand for plug-in electric vehicle charging on the generating portfolio, overall generating fuel mix, and the costs of electricity generation. We apply this model to the PJM Interconnect and ISO-New England Regional Transmission Organization service areas assuming a CO2 pricing scheme that is applied to the electricity sector but does not directly regulate emissions from other sectors. We find that a shift from coal toward natural gas and wind generation is sufficient to achieve a 50% reduction in electricity-sector CO2 emissions while supporting vehicle charging for 25% of the vehicle fleet. The price impacts of these shifts are sensitive to demand side price responsiveness and the capital costs of new wind construction.::::::Unpublished::::::http://dx.doi.org/10.1016/j.seps.2012.09.004
Modeling the Impact of Electric Vehicle Charging on Heat Transfer around Underground Cables:::While increased use of plug-in electric vehicles (PEVs) has environmental and economic benefits, the increased load is expected to strain components of the power delivery infrastructure. Within electric distribution systems, overloading of transformers and underground cables and associated thermal degradation is of particular concern. The current paper estimates the effect of different levels and types of PEV charging on transient heating of underground cables. Transportation survey data is used to estimate travel miles and arrival/departure times for a typical residential neighborhood, which is subsequently used to estimate the electric load curve with different levels of PEV penetration. The estimated load curves are used to perform transient heat transfer computations for a system of three buried cables using an overset grid finite-difference approach, the results of which are used to estimate acceleration of cable thermal degradation. Vehicle charging, even for a modest 30% PEV penetration, is found to nearly double peak temperature rise above ambient at the cable surface, increase the daily variance in cable temperatures, and significantly decrease the estimated time to failure for cables with thermally sensitive insulation.::::::Published::::::http://www.sciencedirect.com/science/article/pii/S0378779612003586
Travel Demand and Charging Capacity for Electric Vehicles in Rural States: A Vermont Case Study:::As the number of electric vehicles (EVs) increase we must consider not only how this fuel switch may affect electrical power infrastructure but also mobility. Specifically, the suitability and charging requirements of these vehicles may differ in rural areas, where the electrical grid may be less robust and miles driven higher. Although other studies have examined issues of regional power requirements of EVs, none have done so in conjunction with the spatial considerations of travel demand. We use three datasets to forecast the future spatial distribution of EVs, as well as these vehicles’ ability to meet current daily travel demand: the National Household Travel Survey (NHTS), geocoded Vermont vehicle fleet data, and an E911 geocoded dataset of every building statewide. We consider spatial patterns in daily travel and home-based tours to identify optimal EV charging locations, as well as any area-types that are unsuited for widespread electric vehicle adoption. We found that hybrid vehicles were more likely to be near other hybrids than conventional vehicles were. This suggestion of clustering of current hybrid vehicles, in both urban and rural areas, suggests that the distribution of future EVs may also cluster in rural areas. Our analysis suggests that between 69 and 84% of the state’s vehicles could be replaced by a 40-mile range EV, depending on the availability of workplace charging. Problematic areas for EV adoption may be suburban areas, where both residential density is high (and potential clustering of hybrids), as well as miles driven. Our results suggest EVs are viable for rural mobility demand but require special consideration for power supply and vehicle charging infrastructure.::::::Published::::::http://www.uvm.edu/~phines/publications/2012/Aultman-Hall_2012.pdf
Estimating the Impact of Electric Vehicle Charging on Electricity Costs Given Electricity Sector Carbon Cap:::A model estimates the short-run effect of plug-in hybrid electric vehicle (PHEV) charging on electricity costs, given a cap on carbon dioxide (CO2) emissions that covers only the electricity sector. In the short run, cap-and-trade systems that cover the electricity sector increase the marginal cost of electricity production. The magnitude of the increase in cost depends on several factors, including the stringency of the cap in relation to the demand for electricity. The use of PHEVs, which also has the potential to decrease net greenhouse gas emissions, would increase demand for electricity and thus would increase the upward pressure on marginal costs. The model described examines this effect for the New England electricity market, which as of January 2009 operates under the Regional Greenhouse Gas Initiative, a cap-and-trade system for CO2. The model uses linear optimization to dispatch power plants to minimize fuel costs given inelastic electric demand and constraints on nitrogen oxide and CO2 emissions. The model is used to estimate costs for three fleet penetration levels (1%, 5%, and 10%) and three charging scenarios (evening charging, nighttime charging, and twice-a-day charging). The results indicate that PHEV charging demand increases the marginal cost of CO2 emissions as well as the average and marginal fuel costs for electricity generation. At all penetration levels the cost increases were minimized in the nighttime-charging scenario.::::::Published::::::http://trb.metapress.com/content/j28336176q41026h/
Comparing empirical and simulated wind speed and power data:::Using data from two large US wind interconnection studies and two grid-scale wind power plants, this paper provides evidence that mesoscale meteorological models under-predict the variability in wind speeds, but for large wind farms the power production data have more similar statistics. Specifically, the mesoscale models under-predict the high-frequency variability in wind speeds, as measured by the power spectral density and the probability of large changes in wind speeds. However, these differences only appear to translate into an under-prediction of power production variability when modeling small wind plants (less than 10 square miles in area), where the effect of geographic diversity is minimal. When modeling larger wind plants, the filtering of the power output due to geographic diversity roughly offsets the filtering effect of the mesoscale model on predicted wind speeds. The exception to this is that the simulated data consistently under-predict the probability of very large wind ramping events, such as a 50% change in power output over an hour. The results show some evidence that methods aimed at correcting the reduced variability may result in too much high-frequency variability. We conclude that while meteorological models are important for large-scale wind integration studies, caution is needed for analyses that could be sensitive to the probability of large ramping events and high-frequency variability.::::::Unpublished::::::http://www.uvm.edu/~phines/publications/2012/working_paper_june_2012.pdf
A Review Of Large-scale Renewable Electricity Integration Studies:::As a result of state renewable portfolio standards and federal tax credits, there is growing interest and investment in renewable sources of electricity in the United States and worldwide. Wind and solar energy are the fastest growing renewable sources of electric energy with U.S. wind power capacity increasing from 8.7 GW in 2005 to 33.5 GW 2009 and solar increasing from 211 MW to 603 MW over the same period [EIA:2010]. However wind and solar power plants are intermittent and variable: that is, they do not produce power at all times of day; and even when power is being produced, output can change rapidly. Biomass, geothermal and hydroelectric energy sources do not suffer from intermittency and variability to the same extent, however growth of these sources has been limited. The U.S. electric system, which was developed throughout the 20th century, was designed around power plants that are primarily intended to deliver constant power. In order to enable a ten-fold increase in the percentage of intermittent and variable resources from the present 2%, as envisioned in a number of state renewable portfolio standards, electricity systems require significant changes in technology, operating policies, and infrastructure. To understand this need, numerous government, academic, and electricity industry organizations have studied the challenges and opportunities for integrating wind, and to a lesser extent solar, resources into electricity infrastructures. This paper summarizes the conclusions from these studies and highlights a number of areas where additional research is needed to facilitate good decision-making regarding the increase of renewable power integration. Our review covers two DOE-sponsored national studies [DOE:2008], six regional studies covering Texas [ERCOT:2008], New York [NYSERDA:2005], Minnesota [MN:2006], California [CEC:2010], the South-central U.S. [SPP:2010], the Eastern U.S. [NREL:2010], four European reports [EWIS:2010, EWEA:2009, CEER:2009, EPRI:2010], and several academic reports. This paper provides an overview of the results from industry studies (Section 1), a brief discussion of related academic publications in this area (Section 2) and a more detailed analysis of several of the industry reports (Section 3).::::::Published::::::http://wpweb2.tepper.cmu.edu/rlang/RenewElec/Integration studies.pdf
Estimating Regulation Reserve Requirements As Wind Generation Increases - A Problem Definition:::This paper describes the problem of estimating the quantity of regulating reserves required to meet a threshold on Area Control Error. In contrast to existing studies, which have largely used Gaussian statistical assumptions in their approach to estimating reserves requirements, we seek to develop solutions that are based on the empirical statistical properties of wind farms. Rather than proposing a solution to this problem this paper bounds the problem, formulating it as a stochastic optimization problem, and discusses an approach to the development of solutions.::::::Published::::::http://wpweb2.tepper.cmu.edu/rlang/RenewElec/wind_and_reserves.v3.pdf
Shadow networks: Discovering hidden nodes with models of information flow:::Complex, dynamic networks underlie many systems, and understanding these networks is the concern of a great span of important scientific and engineering problems. Quantitative description is crucial for this understanding yet, due to a range of measurement problems, many real network datasets are incomplete. Here we explore how accidentally missing or deliberately hidden nodes may be detected in networks by the effect of their absence on predictions of the speed with which information flows through the network. We use Symbolic Regression (SR) to learn models relating information flow to network topology. These models show localized, systematic, and non-random discrepancies when applied to test networks with intentionally masked nodes, demonstrating the ability to detect the presence of missing nodes and where in the network those nodes are likely to reside.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2013/bagrow2013a_fig3_400px.jpg:::Unpublished:::http://arxiv.org/abs/1312.6122:::http://www.uvm.edu/~pdodds/research/papers/files/2013/bagrow2013a.pdf
Direct, physically motivated derivation of triggering probabilities for spreading processes on generalized random networks:::We derive a general expression for the probability of global spreading starting from a single infected seed for contagion processes acting on generalized, correlated random networks. We employ a simple probabilistic argument that encodes the spreading mechanism in an intuitive, physical fashion. We use our approach to directly and systematically obtain triggering probabilities for contagion processes acting on a series of interrelated random network families.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2011/dodds2011d_fig1_400px.jpg:::Unpublished:::http://arxiv.org/abs/1108.5398:::http://www.uvm.edu/~pdodds/research/papers/files/2011/dodds2011d.pdf
Collective Philanthropy: Describing and Modeling the Ecology of Giving:::Reflective of income and wealth distributions, philanthropic gifting appears to follow an approximate power-law size distribution as measured by the size of gifts received by individual institutions. We explore the ecology of gifting by analysing data sets of individual gifts for a diverse group of institutions dedicated to education, medicine, art, public support, and religion. We find that the detailed forms of gift-size distributions differ across but are relatively constant within charity categories. We construct a model for how a donor's income affects their giving preferences in different charity categories, offering a mechanistic explanation for variations in institutional gift-size distributions. We discuss how knowledge of gift-sized distributions may be used to assess an institution's gift-giving profile, to help set fundraising goals, and to design an institution-specific giving pyramid.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2013/gottesman2013a_fig1_400px.jpg:::Unpublished:::http://arxiv.org/abs/1307.2278:::http://www.uvm.edu/~pdodds/research/papers/files/2013/gottesman2013a.pdf
An evolutionary algorithm approach to link prediction in dynamic social networks:::Many real world, complex phenomena have underlying structures of evolving networks where nodes and links are added and removed over time. A central scientific challenge is the description and explanation of network dynamics, with a key test being the prediction of short and long term changes. For the problem of short-term link prediction, existing methods attempt to determine neighborhood metrics that correlate with the appearance of a link in the next observation period. Recent work has suggested that the incorporation of topological features and node attributes can improve link prediction. We provide an approach to predicting future links by applying the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to optimize weights which are used in a linear combination of sixteen neighborhood and node similarity indices. We examine a large dynamic social network with over 106 nodes (Twitter reciprocal reply networks), both as a test of our general method and as a problem of scientific interest in itself. Our method exhibits fast convergence and high levels of precision for the top twenty predicted links. Based on our findings, we suggest possible factors which may be driving the evolution of Twitter reciprocal reply networks.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2013/bliss2013a_fig1_400px.jpg:::Unpublished:::http://arxiv.org/abs/1304.6257:::http://www.uvm.edu/~pdodds/research/papers/files/2013/bliss2013a.pdf
Happiness and the Patterns of Life: A Study of Geolocated Tweets:::The patterns of life exhibited by large populations have been described and modeled both as a basic science exercise and for a range of applied goals such as reducing automotive congestion, improving disaster response, and even predicting the location of individuals. However, these studies previously had limited access to conversation content, rendering changes in expression as a function of movement invisible. In addition, they typically use the communication between a mobile phone and its nearest antenna tower to infer position, limiting the spatial resolution of the data to the geographical region serviced by each cellphone tower. We use a collection of 37 million geolocated tweets to characterize the movement patterns of 180,000 individuals, taking advantage of several orders of magnitude of increased spatial accuracy relative to previous work. Employing the recently developed sentiment analysis instrument known as the 'hedonometer', we characterize changes in word usage as a function of movement, and find that expressed happiness increases logarithmically with distance from an individual's average location.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2013/frank2013a_fig3_400px.jpg:::Published:::http://arxiv.org/abs/1304.1296:::http://www.nature.com/srep/2013/130911/srep02625/full/srep02625.html
Dynamics of influence processes on networks: Complete mean-field theory; the roles of response functions, connectivity, and synchrony; and applications to social contagion:::We study binary state dynamics on a network where each node acts in response to the average state of its neighborhood. Allowing varying amounts of stochasticity in both the network and node responses, we find different outcomes in random and deterministic versions of the model. In the limit of a large, dense network, however, we show that these dynamics coincide. We construct a general mean field theory for random networks and show this predicts that the dynamics on the network are a smoothed version of the average response function dynamics. Thus, the behavior of the system can range from steady state to chaotic depending on the response functions, network connectivity, and update synchronicity. As a specific example, we model the competing tendencies of imitation and non-conformity by incorporating an off-threshold into standard threshold models of social contagion. In this way we attempt to capture important aspects of fashions and societal trends. We compare our theory to extensive simulations of this 'limited imitation contagion' model on Poisson random graphs, finding agreement between the mean-field theory and stochastic simulations.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2013/harris2013a_fig7_400px.jpg:::Published:::http://arxiv.org/abs/1303.1414:::http://pre.aps.org/abstract/PRE/v88/i2/e022816
The Geography of Happiness: Connecting Twitter sentiment and expression, demographics, and objective characteristics of place:::We conduct a detailed investigation of correlations between real-time expressions of individuals made across the United States and a wide range of emotional, geographic, demographic, and health characteristics. We do so by combining (1) a massive, geo-tagged data set comprising over 80 million words generated over the course of several recent years on the social network service Twitter and (2) annually-surveyed characteristics of all 50 states and close to 400 urban populations. Among many results, we generate taxonomies of states and cities based on their similarities in word use; estimate the happiness levels of states and cities; correlate highly-resolved demographic characteristics with happiness levels; and connect word choice and message length with urban characteristics such as education levels and obesity rates. Our results show how social media may potentially be used to estimate real-time levels and changes in population-level measures such as obesity rates.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2013/mitchell2013a_fig1_400px.jpg:::Published:::http://arxiv.org/abs/1302.3299:::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0064417
Limited Imitation Contagion on Random Networks: Chaos, Universality, and Unpredictability:::We study a family of binary state, socially-inspired contagion models which incorporate imitation limited by an aversion to complete conformity. We uncover rich behavior in our models whether operating with either probabilistic or deterministic individual response functions on both dynamic and fixed random networks. In particular, we find significant variation in the limiting behavior of a population's infected fraction, ranging from steady-state to chaotic. We show that period doubling arises as we increase the average node degree, and that the universality class of this well known route to chaos depends on the interaction structure of random networks rather than the microscopic behavior of individual nodes. We find that increasing the fixedness of the system tends to stabilize the infected fraction, yet disjoint, multiple equilibria are possible depending solely on the choice of the initially infected node.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2012/dodds2012a_fig3_400px.jpg:::Published:::http://arxiv.org/abs/1208.0255:::http://prl.aps.org/abstract/PRL/v110/i15/e158701
Testing the metabolic theory of ecology:::The Metabolic Theory of Ecology (MTE) predicts the effects of body size and temperature on metabolism through considerations of vascular distribution networks and biochemical kinetics. MTE has also been extended to characterize processes from cellular to global levels. MTE has generated both enthusiasm and controversy across a broad range of research areas. However, most efforts that claim to validate or invalidate MTE have focused on testing predictions. We argue that critical evaluation of MTE also requires strong tests of both its theoretical foundations and simplifying assumptions. To this end we synthesize available information and find that MTE's original derivations are incomplete and require additional assumptions to obtain the full scope of attendant predictions. Moreover, although some of MTE's simplifying assumptions are well supported by data, others are inconsistent with empirical tests and even more remain untested. Further, though many predictions are empirically supported on average, work remains to explain the often large variability in data. We suggest that greater effort be focused on evaluating MTE's underlying theory and simplifying assumptions in order to help delineate the scope of MTE, generate new theory, and shed light on fundamental aspects of biological form and function.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2012/price2012a_figextra1_400px.jpg:::Published::::::http://onlinelibrary.wiley.com/doi/10.1111/j.1461-0248.2012.01860.x/abstract
Twitter reciprocal reply networks exhibit assortativity with respect to happiness:::The advent of social media has provided an extraordinary, if imperfect, 'big data' window into the form and evolution of social networks. Based on nearly 40 million message pairs posted to Twitter between September 2008 and February 2009, we construct and examine the revealed social network structure and dynamics over the time scales of days, weeks, and months. At the level of user behavior, we employ our recently developed hedonometric analysis methods to investigate patterns of sentiment expression. We find users' average happiness scores to be positively and significantly correlated with those of users one, two, and three links away. We strengthen our analysis by proposing and using a null model to test the effect of network topology on the assortativity of happiness. We also find evidence that more well connected users write happier status updates, with a transition occurring around Dunbar's number. More generally, our work provides evidence of a social sub-network structure within Twitter and raises several methodological points of interest with regard to social network reconstructions.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2012/bliss2012a_figA4_400px.jpg:::Published:::http://arxiv.org/abs/1112.1010:::http://www.sciencedirect.com/science/article/pii/S187775031200049X
Positivity of the English language:::Over the last million years, human language has emerged and evolved as a fundamental instrument of social communication and semiotic representation. People use language in part to convey emotional information, leading to the central and contingent questions: (1) What is the emotional spectrum of natural language? and (2) Are natural languages neutrally, positively, or negatively biased? Here, we report that the human-perceived positivity of over 10,000 of the most frequently used English words exhibits a clear positive bias. More deeply, we characterize and quantify distributions of word positivity for four large and distinct corpora, demonstrating that their form is broadly invariant with respect to frequency of word use.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2011/kloumann2011a_fig1_400px.jpg:::Published:::http://arxiv.org/abs/1108.5192:::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0029484
Temporal patterns of happiness and information in a global social network: Hedonometrics and Twitter:::Individual happiness is a fundamental societal metric. Normally measured through self-report, happiness has often been indirectly characterized and overshadowed by more readily quantifiable economic indicators such as gross domestic product. Here, we examine expressions made on the online, global microblog and social networking service Twitter, uncovering and explaining temporal variations in happiness and information levels over timescales ranging from hours to years. Our data set comprises over 46 billion words contained in nearly 4.6 billion expressions posted over a 33 month span by over 63 million unique users. In measuring happiness, we use a real-time, remote-sensing, non-invasive, text-based approach---a kind of hedonometer. In building our metric, made available with this paper, we conducted a survey to obtain happiness evaluations of over 10,000 individual words, representing a tenfold size improvement over similar existing word sets. Rather than being ad hoc, our word list is chosen solely by frequency of usage and we show how a highly robust metric can be constructed and defended.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2011/dodds2011a_fig2_400px.jpg:::Published:::http://arxiv.org/abs/1101.5120:::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0026752
Exact solutions for social and biological contagion models on mixed directed and undirected, degree-correlated random networks:::We derive analytic expressions for the possibility, probability, and expected size of global spreading events starting from a single infected seed for a broad collection of contagion processes acting on random networks with both directed and undirected edges and arbitrary degree-degree correlations. Our work extends previous theoretical developments for the undirected case, and we provide numerical support for our findings by investigating an example class of networks for which we are able to obtain closed-form expressions.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2011/payne2011a_fig1_400px.jpg:::Published:::http://arxiv.org/abs/1103.0056:::http://pre.aps.org/abstract/PRE/v84/i1/e016110
Direct, physically-motivated derivation of the contagion condition for spreading processes on generalized random networks:::For a broad range single-seed contagion processes acting on generalized random networks, we derive a unifying analytic expression for the possibility of global spreading events in a straightforward, physically intuitive fashion. Our reasoning lays bare a direct mechanical understanding of an archetypal spreading phenomena that is not evident in circuitous extant mathematical approaches.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2011/dodds2011b_fig1_400px.jpg:::Published:::http://arxiv.org/abs/1101.5591:::http://pre.aps.org/abstract/PRE/v83/i5/e056122
On the optimal form of branching supply and collection networks:::For the problem of efficiently supplying material to a spatial region from a single source, we present a simple scaling argument based on branching network volume minimization that identifies limits to the scaling of sink density. We discuss implications for two fundamental and unresolved problems in organismal biology and geomorphology: how basal metabolism scales with body size for homeotherms and the scaling of drainage basin shape on eroding landscapes.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2010/dodds2010a_fig1_400px.jpg:::Published:::http://arxiv.org/abs/0909.1104:::http://prl.aps.org/abstract/PRL/v104/i4/e048702
Measuring the Happiness of Large-Scale Written Expression: Songs, Blogs, and Presidents:::The importance of quantifying the nature and intensity of emotional states at the level of populations is evident: we would like to know how, when, and why individuals feel as they do if we wish, for example, to better construct public policy, build more successful organizations, and, from a scientific perspective, more fully understand economic and social phenomena. Here, by incorporating direct human assessment of words, we quantify happiness levels on a continuous scale for a diverse set of large-scale texts: song titles and lyrics, weblogs, and State of the Union addresses. Our method is transparent, improvable, capable of rapidly processing Web-scale texts, and moves beyond approaches based on coarse categorization. Among a number of observations, we find that the happiness of song lyrics trends downward from the 1960s to the mid 1990s while remaining stable within genres, and that the happiness of blogs has steadily increased from 2005 to 2009, exhibiting a striking rise and fall with blogger age and distance from the Earth’s equator.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2009/dodds2009b_fig6_400px.jpg:::Published::::::http://link.springer.com/article/10.1007/s10902-009-9150-9
Information cascades on degree-correlated random networks:::We investigate by numerical simulation a threshold model of social contagion on degree-correlated random networks. We show that the class of networks for which global information cascades occur generally expands as degree-degree correlations become increasingly positive. However, under certain conditions, large-scale information cascades can paradoxically occur when degree-degree correlations are sufficiently positive or negative, but not when correlations are relatively small. We also show that the relationship between the degree of the initially infected vertex and its ability to trigger large cascades is strongly affected by degree-degree correlations.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2009/payne2009a_fig5_400px.jpg:::Published::::::http://pre.aps.org/abstract/PRE/v80/i2/e026125
Analysis of a threshold model of social contagion on degree-correlated networks:::We analytically determine when a range of abstract social contagion models permit global spreading from a single seed on degree-correlated random networks. We deduce the expected size of the largest vulnerable component, a network's tinderbox-like critical mass, as well as the probability that infecting a randomly chosen individual seed will trigger global spreading. In the appropriate limits, our results naturally reduce to standard ones for models of disease spreading and to the condition for the existence of a giant component. Recent advances in the distributed, infinite seed case allow us to further determine the final size of global spreading events, when they occur. To provide support for our results, we derive exact expressions for key spreading quantities for a simple yet rich family of random networks with bimodal degree distributions.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2009/dodds2009a_fig1_400px.jpg:::Published:::http://arxiv.org/abs/0903.0597:::http://pre.aps.org/abstract/PRE/v79/i6/e066115
Modeling social interactions: Identification, empirical methods and policy implications:::Social interactions occur when agents in a network affect other agents’ choices directly, as opposed to via the intermediation of markets. The study of such interactions and the resultant outcomes has long been an area of interest across a wide variety of social sciences. With the advent of electronic media that facilitate and record such interactions, this interest has grown sharply in the business world as well. In this paper, we provide a brief summary of what is known so far, discuss the main challenges for researchers interested in this area, and provide a common vocabulary that will hopefully engender future (cross disciplinary) research. The paper considers the challenges of distinguishing actual causal social interactions from other phenomena that may lead to a false inference of causality. Further, we distinguish between two broadly defined types of social interactions that relate to how strongly interactions spread through a network. We also provide a very selective review of how insights from other disciplines can improve and inform modeling choices. Finally, we discuss how models of social interaction can be used to provide guidelines for marketing policy and conclude with thoughts on future research directions.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2008/hartmann2008a_figextra1_400px.jpg:::Published::::::http://link.springer.com/article/10.1007/s11002-008-9048-z
Influentials, networks, and public opinion formation:::A central idea in marketing and diffusion research is that influentials—a minority of individuals who influence an exceptional number of their peers—are important to the formation of public opinion. Here we examine this idea, which we call the ``influentials hypothesis,'' using a series of computer simulations of interpersonal influence processes. Under most conditions that we consider, we &#64257;nd that large cascades of influence are driven not by influentials but by a critical mass of easily influenced individuals. Although our results do not exclude the possibility that influentials can be important, they suggest that the influentials hypothesis requires more careful speci&#64257;cation and testing than it has received.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2007/watts2007a_fig1_400px.jpg:::Published::::::http://www.uvm.edu/~pdodds/research/papers/files/2007/watts2007a.pdf
Cooperation in evolving social networks:::We study the problem of cooperative behavior emerging in an environment where individual behaviors and interaction structures coevolve. Players not only learn which strategy to adopt by imitating the strategy of the best-performing player they observe, but also choose with whom they should interact by selectively creating and/or severing ties with other players based on a myopic cost-benefit comparison. We find that scalable cooperation—that is, high levels of cooperation in large populations—can be achieved in sparse networks, assuming that individuals are able to sever ties unilaterally and that new ties can only be created with the mutual consent of both parties. Detailed examination shows that there is an important trade-off between local reinforcement and global expansion in achieving cooperation in dynamic networks. As a result, networks in which ties are costly and local structure is largely absent tend to generate higher levels of cooperation than those in which ties are made easily and friends of friends interact with high probability, where the latter result contrasts strongly with the usual intuition.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2007/hanaki2007a_fig1_400px.jpg:::Published::::::http://www.uvm.edu/~pdodds/research/papers/files/2007/hanaki2007a.pdf
Experimental study of inequality and unpredictability in an artificial cultural market:::Hit songs, books, and movies are many times more successful than average, suggesting that 'the best' alternatives are qualitatively different from 'the rest'; yet experts routinely fail to predict which products will succeed. We investigated this paradox experimentally, by creating an artificial 'music market' in which 14,341 participants downloaded previously unknown songs either with or without knowledge of previous participants' choices. Increasing the strength of social influence increased both inequality and unpredictability of success. Success was also only partly determined by quality: The best songs rarely did poorly, and the worst rarely did well, but any other result was possible.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2006/salganik2006a_figextra1_400px.jpg:::Published::::::http://www.uvm.edu/~pdodds/research/papers/files/2006/salganik2006a.pdf
A generalized model of social and biological contagion:::We present a model of contagion that unifies and generalizes threshold models of social contagion and epidemiological models of disease spreading. Our model incorporates individual memory of exposure to a contagious entity (e.g., a rumor or disease), variable magnitudes of exposure (dose sizes), and heterogeneity in the susceptibility of individuals. Through analysis and simulation, we examine in detail the case where individuals may recover from an infection and then immediately become susceptible again (analogous to the so-called SIS model). We identify three basic classes of contagion models which we call epidemic threshold, vanishing critical mass, and critical mass classes respectively, where each class of models corresponds to different strategies for prevention or facilitation. We find that the conditions for a particular contagion model to belong to one of the these three classes depend only on memory length and the probabilities of being infected by one and two exposures respectively. These parameters are in principle measurable for real contagious influences or entities, thus yielding empirical implications for our model. We also study the case where individuals attain permanent immunity once recovered, finding that epidemics inevitably die out but may be surprisingly persistent when individuals possess memory.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2005/dodds2005a_fig1c_400px.jpg:::Published::::::http://www.uvm.edu/~pdodds/research/papers/files/2005/dodds2005a.pdf
Universal behavior in a generalized model of contagion:::Models of contagion arise broadly both in the biological and social sciences, with applications ranging from the transmission of infectious diseases to the diffusion of innovations and the spread of cultural fads. In this Letter, we introduce a general model of contagion which, by explicitly incorporating memory of past exposures to, for example, an infectious agent, rumor, or new product, includes the main features of existing contagion models and interpolates between them. We obtain exact solutions for a simple version of the model, finding that under general conditions only three classes of collective dynamics exist, two of which correspond to familiar epidemic threshold and critical mass dynamics, while the third is a distinct intermediate case. We find that for a given length of memory, the class into which a particular system falls is determined by two parameters, each of which ought to be measurable empirically. Our model suggests novel measures for assessing the susceptibility of a population to large contagion events, and also a possible strategy for inhibiting or facilitating them.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2004/dodds2004a_fig2_400px.jpg:::Published::::::http://www.uvm.edu/~pdodds/research/papers/files/2004/dodds2004a.pdf
Information exchange and the robustness of organizational networks:::The dynamics of information exchange is an important but understudied aspect of collective communication, coordination, and problem solving in a wide range of distributed systems, both physical (e.g., the Internet) and social (e.g., business firms). In this paper, we introduce a model of organizational networks according to which links are added incrementally to a hierarchical backbone and test the resulting networks under variable conditions of information exchange. Our main result is the identification of a class of multiscale networks that reduce, over a wide range of environments, the likelihood that individual nodes will suffer congestion-related failure and that the network as a whole will disintegrate when failures do occur. We call this dual robustness property of multiscale networks 'ultrarobustness'. Furthermore, we find that multiscale networks attain most of their robustness with surprisingly few link additions, suggesting that ultrarobust organizational networks can be generated in an efficient and scalable manner. Our results are directly relevant to the relief of congestion in communication networks and also more broadly to activities, like distributed problem solving, that require individuals to exchange information in an unpredictable manner.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2003/dodds2003c_fig2_400px.jpg:::Published::::::http://www.pnas.org/content/100/21/12516.abstract
An experimental study of search in global social networks:::We report on a global social-search experiment in which more than 60,000 e-mail users attempted to reach one of 18 target persons in 13 countries by forwarding messages to acquaintances. We find that successful social search is conducted primarily through intermediate to weak strength ties, does not require highly connected 'hubs' to succeed, and, in contrast to unsuccessful social search, disproportionately relies on professional relationships. By accounting for the attrition of message chains, we estimate that social searches can reach their targets in a median of five to seven steps, depending on the separation of source and target, although small variations in chain lengths and participation rates generate large differences in target reachability. We conclude that although global social networks are, in principle, searchable, actual success depends sensitively on individual incentives.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2003/dodds2003b_fig1_400px.jpg:::Published::::::http://www.sciencemag.org/content/301/5634/827.full
Packing-limited growth of irregular objects:::We study growth limited by packing for irregular objects in two dimensions. We generate packings by seeding objects randomly in time and space and allowing each object to grow until it collides with another object. The objects we consider, allow us to investigate the separate effects of anisotropy and non-unit aspect ratio. By means of a connection to the decay of pore-space volume, we measure power law exponents for the object size distribution. We carry out a mean field analysis, showing that it provides an upper bound for the size distribution exponent. We find that while the details of the growth mechanism are irrelevant, the exponent is strongly shape dependent. Potential applications lie in ecological and biological environments where sessile organisms compete for limited space as they grow.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2003/dodds2003a_fig2_400px.jpg:::Published::::::http://pre.aps.org/abstract/PRE/v67/i1/e016117
Identity and search in social networks:::Social networks have the surprising property of being 'searchable': Ordinary people are capable of directing messages through their network of acquaintances to reach a specific but distant target person in only a few steps. We present a model that offers an explanation of social network searchability in terms of recognizable personal identities: sets of characteristics measured along a number of social dimensions. Our model defines a class of searchable networks and a method for searching them that may be applicable to many network search problems, including the location of data files in peer-to-peer networks, pages on the World Wide Web, and information in distributed databases.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2002/watts2002b_fig1_400px.jpg:::Published::::::http://www.sciencemag.org/cgi/content/abstract/296/5571/1302
Packing-limited growth:::We consider growing spheres seeded by random injection in time and space. Growth stops when two spheres meet leading eventually to a jammed state. We study the statistics of growth limited by packing theoretically in d dimensions and via simulation in d=2, 3, and 4. We show how a broad class of such models exhibit distributions of sphere radii with a universal exponent. We construct a scaling theory that relates the fractal structure of these models to the decay of their pore space, a theory that we con&#64257;rm via numerical simulations. The scaling theory also predicts an upper bound for the universal exponent and is in exact agreement with numerical results for d=4.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2002/dodds2002a_fig3_400px.jpg:::Published::::::http://www.uvm.edu/~pdodds/research/papers/files/2002/dodds2002a.pdf
Geometry of river networks III: Characterization of component connectivity:::Essential to understanding the overall structure of river networks is a knowledge of their detailed architecture. Here we explore the presence of randomness in river network structure and the details of its consequences. We first show that an averaged view of network architecture is provided by a proposed self-similarity statement about the scaling of drainage density, a local measure of stream concentration. This scaling of drainage density is shown to imply Tokunaga’s law, a description of the scaling of side branch abundance along a given stream, as well as a scaling law for stream lengths. We then consider fluctuations in drainage density and consequently the numbers of side branches. Data are analyzed for the Mississippi River basin and a model of random directed networks. Numbers of side streams are found to follow exponential distributions, as are intertributary distances along streams. Finally, we derive a joint variation of side stream abundance with stream length, affording a full description of fluctuations in network structure. Fluctuations in side stream numbers are shown to be a direct result of fluctuations in stream lengths. This is the last paper in a series of three on the geometry of river networks.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2001/dodds2001c_fig1_400px.jpg:::Published::::::http://pre.aps.org/abstract/PRE/v63/i1/e016117
Geometry of river networks II: Distributions of component size and number:::The structure of a river network may be seen as a discrete set of nested subnetworks built out of individual stream segments. These network components are assigned an integral stream order via a hierarchical and discrete ordering method. Exponential relationships, known as Horton’s laws, between stream order and ensemble-averaged quantities pertaining to network components are observed. We extend these observations to incorporate fluctuations and all higher moments by developing functional relationships between distributions. The relationships determined are drawn from a combination of theoretical analysis, analysis of real river networks including the Mississippi, Amazon, and Nile, and numerical simulations on a model of directed, random networks. Underlying distributions of stream segment lengths are identified as exponential. Combinations of these distributions form single-humped distributions with exponential tails, the sums of which are in turn shown to give power-law distributions of stream lengths. Distributions of basin area and stream segment frequency are also addressed. The calculations identify a single length scale as a measure of size fluctuations in network components. This article is the second in a series of three addressing the geometry of river networks.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2001/dodds2001b_fig1_400px.jpg:::Published::::::http://pre.aps.org/abstract/PRE/v63/i1/e016116
Geometry of river networks I: Scaling, fluctuations, and deviations:::This paper is the first in a series of three papers investigating the detailed geometry of river networks. Branching networks are a universal structure employed in the distribution and collection of material. Large-scale river networks mark an important class of two-dimensional branching networks, being not only of intrinsic interest but also a pervasive natural phenomenon. In the description of river network structure, scaling laws are uniformly observed. Reported values of scaling exponents vary, suggesting that no unique set of scaling exponents exists. To improve this current understanding of scaling in river networks and to provide a fuller description of branching network structure, here we report a theoretical and empirical study of fluctuations about and deviations from scaling. We examine data for continent-scale river networks such as the Mississippi and the Amazon and draw inspiration from a simple model of directed, random networks. We center our investigations on the scaling of the length of a subbasin’s dominant stream with its area, a characterization of basin shape known as Hack’s law. We generalize this relationship to a joint probability density, and provide observations and explanations of deviations from scaling. We show that fluctuations about scaling are substantial, and grow with system size. We find strong deviations from scaling at small scales which can be explained by the existence of a linear network structure. At intermediate scales, we find slow drifts in exponent values, indicating that scaling is only approximately obeyed and that universality remains indeterminate. At large scales, we observe a breakdown in scaling due to decreasing sample space and correlations with overall basin shape. The extent of approximate scaling is significantly restricted by these deviations, and will not be improved by increases in network resolution.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2001/dodds2001a_fig8_400px.jpg:::Published::::::http://pre.aps.org/abstract/PRE/v63/i1/e016115
Scaling, universality, and geomorphology:::Theories of scaling apply wherever similarity exists across many scales. This similarity may be found in geometry and in dynamical processes. Universality arises when the qualitative character of a system is sufficient to quantitatively predict its essential features, such as the exponents that characterize scaling laws. Within geomorphology, two areas where the concepts of scaling and universality have found application are the geometry of river networks and the statistical structure of topography. We begin this review with a pedagogical presentation of scaling and universality. We then describe recent progress made in applying these ideas to networks and topography. This overview leads to a synthesis that attempts a classification of surface and network properties based on generic mechanisms and geometric constraints. We also briefly review how scaling and universality have been applied to related problems in sedimentology—specifically, the origin of stromatolites and the relation of the statistical properties of submarine-canyon topography to the size distribution of turbidite deposits. Throughout the review, our intention is to elucidate not only the problems that can be solved using these concepts, but also those that cannot.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/2000/dodds2000a_fig1_400px.jpg:::Published::::::http://www.annualreviews.org/doi/abs/10.1146/annurev.earth.28.1.571
Unified view of scaling laws for river networks:::Scaling laws that describe the structure of river networks are shown to follow from three simple assumptions. These assumptions are (1) river networks are structurally self-similar, (2) single channels are self-affine, and (3) overland flow into channels occurs over a characteristic distance (drainage density is uniform). We obtain a complete set of scaling relations connecting the exponents of these scaling laws and find that only two of these exponents are independent. We further demonstrate that the two predominant descriptions of network structure (Tokunaga’s law and Horton’s laws) are equivalent in the case of landscapes with uniform drainage density. The results are tested with data from both real landscapes and a special class of random networks.:::http://www.uvm.edu/~pdodds/research/papers/files/figures/1999/dodds1999a_fig2_400px.jpg:::Published::::::http://pre.aps.org/abstract/PRE/v59/i5/p4865_1
Crowdsourcing Novel Childhood Predictors of Adult Obesity:::Effective and simple screening tools are needed to detect behaviors that are established early in life and have a significant influence on weight gain later in life. Crowdsourcing could be a novel and potentially useful tool to assess childhood predictors of adult obesity. This exploratory study examined whether crowdsourcing could generate well-documented predictors in obesity research and, moreover, whether new directions for future research could be uncovered. Participants were recruited through social media to a question-generation website, on which they answered questions and were able to pose new questions that they thought could predict obesity. During the two weeks of data collection, 532 participants (62% female; age = 26.5±6.7; BMI = 29.0±7.0) registered on the website and suggested a total of 56 unique questions. Nineteen of these questions correlated with body mass index (BMI) and covered several themes identified by prior research, such as parenting styles and healthy lifestyle. More importantly, participants were able to identify potential determinants that were related to a lower BMI, but have not been the subject of extensive research, such as parents packing their children’s lunch to school or talking to them about nutrition. The findings indicate that crowdsourcing can reproduce already existing hypotheses and also generate ideas that are less well documented. The crowdsourced predictors discovered in this study emphasize the importance of family interventions to fight obesity. The questions generated by participants also suggest new ways to express known predictors.:::http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0087756.g001:::Published::::::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0087756
Environmental Influence on the Evolution of Morphological Complexity in Machines:::Whether, when, how, and why increased complexity evolves in biological populations is a longstanding open question. In this work we combine a recently developed method for evolving virtual organisms with an information-theoretic metric of morphological complexity in order to investigate how the complexity of morphologies, which are evolved for locomotion, varies across different environments. We first demonstrate that selection for locomotion results in the evolution of organisms with morphologies that increase in complexity over evolutionary time beyond what would be expected due to random chance. This provides evidence that the increase in complexity observed is a result of a driven rather than a passive trend. In subsequent experiments we demonstrate that morphologies having greater complexity evolve in complex environments, when compared to a simple environment when a cost of complexity is imposed. This suggests that in some niches, evolution may act to complexify the body plans of organisms while in other niches selection favors simpler body plans.:::http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1003399.g001:::Published::::::http://www.ploscompbiol.org/article/info:doi/10.1371/journal.pcbi.1003399;jsessionid=4022E23B582054F2A84FE40C54627838
Crowdsourcing Predictors of Behavioral Outcomes:::Generating models from large data sets -- and determining which subsets of data to mine -- is becoming increasingly automated. However choosing what data to collect in the first place requires human intuition or experience, usually supplied by a domain expert. This paper describes a new approach to machine science which demonstrates for the first time that non-domain experts can collectively formulate features, and provide values for those features such that they are predictive of some behavioral outcome of interest. This was accomplished by building a web platform in which human groups interact to both respond to questions likely to help predict a behavioral outcome and pose new questions to their peers. This results in a dynamically-growing online survey, but the result of this cooperative behavior also leads to models that can predict user's outcomes based on their responses to the user-generated survey questions. Here we describe two web-based experiments that instantiate this approach: the first site led to models that can predict users' monthly electric energy consumption; the other led to models that can predict users' body mass index. As exponential increases in content are often observed in successful online collaborative communities, the proposed methodology may, in the future, lead to similar exponential rises in discovery and insight into the causal factors of behavioral outcomes.::::::Published:::http://arxiv.org/abs/1203.1833:::
Morphological change in machines accelerates the evolution of robust behavior:::Most animals exhibit significant neurological and morphological change throughout their lifetime. No robots to date, however, grow new morphological structure while behaving. This is due to technological limitations but also because it is unclear that morphological change provides a benefit to the acquisition of robust behavior in machines. Here I show that in evolving populations of simulated robots, if robots grow from anguilliform into legged robots during their lifetime in the early stages of evolution, and the anguilliform body plan is gradually lost during later stages of evolution, gaits are evolved for the final, legged form of the robot more rapidly—and the evolved gaits are more robust—compared to evolving populations of legged robots that do not transition through the anguilliform body plan. This suggests that morphological change, as well as the evolution of development, are two important processes that improve the automatic generation of robust behaviors for machines. It also provides an experimental platform for investigating the relationship between the evolution of development and robust behavior in biological organisms.::::::Published::::::http://www.pnas.org/content/108/4/1234.abstract
Innocent Until Proven Guilty: Reducing Robot Shaping From Polynomial to Linear Time:::In evolutionary algorithms, much time is spent evaluating inferior phenotypes that produce no offspring. A common heuristic to address this inefficiency is to stop evaluations early if they hold little promise of attaining high fitness. However, the form of this heuristic is typically dependent on the fitness function used, and there is a danger of prematurely stopping evaluation of a phenotype that may have recovered in the remainder of the evaluation period. Here a stopping method is introduced that gradually reduces fitness over the phenotype's evaluation, rather than accumulating fitness. This method is independent of the fitness function used, only stops those phenotypes that are guaranteed to become inferior to the current offspring-producing phenotypes, and realizes significant time savings across several evolutionary robotics tasks. It was found that for many tasks, time complexity was reduced from polynomial to sublinear time, and time savings increased with the number of training instances used to evaluate a phenotype as well as with task difficulty.::::::Published::::::http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5703121
Self discovery enables robot social cognition: Are you my teacher?:::Infants exploit the perception that others are ‘like me’ to bootstrap social cognition (Meltzoff, 2007a). This paper demonstrates how the above theory can be instantiated in a social robot that uses itself as a model to recognize structural similarities with other robots; this thereby enables the student to distinguish between appropriate and inappropriate teachers. This is accomplished by the student robot first performing self-discovery, a phase in which it uses actuation–perception relationships to infer its own structure. Second, the student models a candidate teacher using a vision-based active learning approach to create an approximate physical simulation of the teacher. Third, the student determines that the teacher is structurally similar (but not necessarily visually similar) to itself if it can find a neural controller that allows its self model (created in the first phase) to reproduce the perceived motion of the teacher model (created in the second phase). Fourth, the student uses the neural controller (created in the third phase) to move, resulting in imitation of the teacher. Results with a physical student robot and two physical robot teachers demonstrate the effectiveness of this approach. The generalizability of the proposed model allows it to be used over variations in the demonstrator: The student robot would still be able to imitate teachers of different sizes and at different distances from itself, as well as different positions in its field of view, because change in the interrelations of the teacher’s body parts are used for imitation, rather than absolute geometric properties.:::http://ars.els-cdn.com/content/image/1-s2.0-S0893608010001486-gr6.jpg:::Published::::::http://www.sciencedirect.com/science/article/pii/S0893608010001486
The Utility of Evolving Simulated Robot Morphology Increases with Task Complexity for Object Manipulation:::Embodied artificial intelligence argues that the body and brain play equally important roles in the generation of adaptive behavior. An increasingly common approach therefore is to evolve an agent's morphology along with its control in the hope that evolution will find a good coupled system. In order for embodied artificial intelligence to gain credibility within the robotics and cognitive science communities, however, it is necessary to amass evidence not only for how to co-optimize morphology and control of adaptive machines, but why. This work provides two new lines of evidence for why this co-optimization is useful: Here we show that for an object manipulation task in which a simulated robot must accomplish one, two, or three objectives simultaneously, subjugating more aspects of the robot's morphology to selective pressure allows for the evolution of better robots as the number of objectives increases. In addition, for robots that successfully evolved to accomplish all of their objectives, those composed of evolved rather than fixed morphologies generalized better to previously unseen environmental conditions.::::::Published::::::http://www.mitpressjournals.org/doi/abs/10.1162/artl.2010.Bongard.024
Use of an artificial neural network to predict head injury outcome:::The authors describe the artificial neural network (ANN) as an innovative and powerful modeling tool that can be increasingly applied to develop predictive models in neurosurgery. They aimed to demonstrate the utility of an ANN in predicting survival following traumatic brain injury and compare its predictive ability with that of regression models and clinicians.:::http://thejns.org/na101/home/literatum/publisher/jns/journals/content/jns/2010/jns.2010.113.issue-3/2009.11.jns09857/production/images/medium/jns09857f1.jpg:::Published::::::http://thejns.org/doi/full/10.3171/2009.11.JNS09857
Accelerating Self-Modeling in Cooperative Robot Teams:::One of the major obstacles to achieving robots capable of operating in real-world environments is enabling them to cope with a continuous stream of unanticipated situations. In previous work, it was demonstrated that a robot can autonomously generate self-models, and use those self-models to diagnose unanticipated morphological change such as damage. In this paper, it is shown that multiple physical quadrupedal robots with similar morphologies can share self-models in order to accelerate modeling. Further, it is demonstrated that quadrupedal robots which maintain separate self-modeling algorithms but swap self-models perform better than quadrupedal robots that rely on a shared self-modeling algorithm. This finding points the way toward more robust robot teams: a robot can diagnose and recover from unanticipated situations faster by drawing on the previous experiences of the other robots.::::::Published::::::http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=
Automated reverse engineering of nonlinear dynamical systems:::Complex nonlinear dynamics arise in many fields of science and engineering, but uncovering the underlying differential equations directly from observations poses a challenging task. The ability to symbolically model complex networked systems is key to understanding them, an open problem in many disciplines. Here we introduce for the first time a method that can automatically generate symbolic equations for a nonlinear coupled dynamical system directly from time series data. This method is applicable to any system that can be described using sets of ordinary nonlinear differential equations, and assumes that the (possibly noisy) time series of all variables are observable. Previous automated symbolic modeling approaches of coupled physical systems produced linear models or required a nonlinear model to be provided manually. The advance presented here is made possible by allowing the method to model each (possibly coupled) variable separately, intelligently perturbing and destabilizing the system to extract its less observable characteristics, and automatically simplifying the equations during modeling. We demonstrate this method on four simulated and two real systems spanning mechanics, ecology, and systems biology. Unlike numerical models, symbolic models have explanatory value, suggesting that automated “reverse engineering” approaches for model-free symbolic nonlinear system identification may play an increasing role in our ability to understand progressively more complex systems in the future.:::http://www.pnas.org/content/104/24/9943/F1.small.gif:::Published::::::http://www.pnas.org/content/104/24/9943.abstract
Resilient Machines Through Continuous Self-Modeling:::Animals sustain the ability to operate after injury by creating qualitatively different compensatory behaviors. Although such robustness would be desirable in engineered systems, most machines fail in the face of unexpected damage. We describe a robot that can recover from such change autonomously, through continuous self-modeling. A four-legged machine uses actuation-sensation relationships to indirectly infer its own structure, and it then uses this self-model to generate forward locomotion. When a leg part is removed, it adapts the self-models, leading to the generation of alternative gaits. This concept may help develop more robust machines and shed light on self-modeling in animals.:::http://www.sciencemag.org/content/314/5802/1118/F1.small.gif:::Published::::::http://www.sciencemag.org/content/314/5802/1118.abstract
Co-evolutionary algorithm for structural damage identification using minimal physical testing:::The problem of damage identification using minimum test data is studied in this work. Data sparsity in damage identification applications commonly results in inverse problems that are mathematically ill-posed (e.g. non-unique solutions). Although solution non-uniqueness may be addressed by performing multiple tests on a structure, it is not trivial to decide which tests to carry out given that actual physical testing is costly. This problem is addressed in this work through a new co-evolutionary algorithm that interactively searches for damage scenarios and optimum physical tests. The algorithm is composed of two stages: the estimation phase, which searches for damage scenarios that can predict current physical tests, and the exploration phase, which searches for tests that increase the level of information about the damaged system. The feasibility of the methodology is demonstrated using numerical examples.::::::Published::::::http://onlinelibrary.wiley.com/doi/10.1002/nme.1803/abstract
Active Coevolutionary Learning of Deterministic Finite Automata:::This paper describes an active learning approach to the problem of grammatical inference, specifically the inference of deterministic finite automata (DFAs). We refer to the algorithm as the estimation-exploration algorithm (EEA). This approach differs from previous passive and active learning approaches to grammatical inference in that training data is actively proposed by the algorithm, rather than passively receiving training data from some external teacher. Here we show that this algorithm outperforms one version of the most powerful set of algorithms for grammatical inference, evidence driven state merging (EDSM), on randomly-generated DFAs. The performance increase is due to the fact that the EDSM algorithm only works well for DFAs with specific balances (percentage of positive labelings), while the EEA is more consistent over a wider range of balances. Based on this finding we propose a more general method for generating DFAs to be used in the development of future grammatical inference algorithms.::::::Published::::::http://jmlr.org/papers/v6/bongard05a.html
Nonlinear System Identification Using Coevolution of Models and Tests:::We present a coevolutionary algorithm for inferring the topology and parameters of a wide range of hidden nonlinear systems with a minimum of experimentation on the target system. The algorithm synthesizes an explicit model directly from the observed data produced by intelligently generated tests. The algorithm is composed of two coevolving populations. One population evolves candidate models that estimate the structure of the hidden system. The second population evolves informative tests that either extract new information from the hidden system or elicit desirable behavior from it. The fitness of candidate models is their ability to explain behavior of the target system observed in response to all tests carried out so far; the fitness of candidate tests is their ability to make the models disagree in their predictions. We demonstrate the generality of this estimation-exploration algorithm by applying it to four different problems—grammar induction, gene network inference, evolutionary robotics, and robot damage recovery—and discuss how it overcomes several of the pathologies commonly found in other coevolutionary algorithms. We show that the algorithm is able to successfully infer and/or manipulate highly nonlinear hidden systems using very few tests, and that the benefit of this approach increases as the hidden systems possess more degrees of freedom, or become more biased or unobservable. The algorithm provides a systematic method for posing synthesis or analysis tasks to a coevolutionary system.::::::Published::::::http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1492385
New Robotics: Design Principles for Intelligent Systems:::New Robotics designates an approach to robotics that, in contrast to traditional robotics, employs ideas and principles from biology. While in the traditional approach there are generally accepted methods (e.g. from control theory), designing agents in the New Robotics approach is still largely considered an art. In recent years, we have been developing a set of heuristics or design principles, that on the one hand capture theoretical insights about intelligent – adaptive – behavior, and on the other provide guidance in actually designing and building systems. In this paper we provide an overview of all the principles but focus on the principles of 'ecological balance' which concerns the relation between environment, morphology, materials, and control, and 'sensory-motor coordination' which concerns self-generated sensory stimulation as the agent interacts with the environment and which is a key to the development of high-level intelligence. As we will argue, artificial evolution together with morphogenesis is not only 'nice to have' but is in fact a necessary tool for designing embodied agents.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_ALife_Pfeifer.pdf
Improving Genetic Programming Based Symbolic Regression Using Deterministic Machine Learning:::Symbolic regression (SR) is a well studied method in genetic programming (GP) for discovering free-form mathematical models from observed data. However, it has not been widely accepted as a standard data science tool. The reluctance is in part due to the hard to analyze random nature of GP and scalability issues. On the other hand, most popular deterministic regression algorithms were designed to generate linear models and therefore lack the flexibility of GP based SR (GP-SR). Our hypothesis is that hybridizing these two techniques will create a synergy between the GP-SR and deterministic approaches to machine learning, which might help bring the GP based techniques closer to the realm of big learning. In this paper, we show that a hybrid deterministic/GP-SR algorithm outperforms GP-SR alone and the state-of-the-art deterministic regression technique alone on a set of multivariate polynomial symbolic regression tasks as the system to be modeled becomes more multivariate.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2013_CEC_Icke_1.pdf
Modeling Hierarchy using Symbolic Regression:::Symbolic Regression is an attractive modeling approach because it can capture and present, mathematically, relationships between variables of interest. However, given n variables to model, symbolic regression returns a flat list of n equations. As the number of state variables to be modeled scales, interpretation of such a list becomes difficult. Here we present a symbolic regression method that detects and captures hidden hierarchy in a given system. The method returns the equations in a hierarchical dependency graph, which increases the interpretability of the results. We demonstrate that two variations of this hierarchical modeling approach outperform non-hierarchical symbolic regression on a synthetic data suite.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2013_CEC_Icke_2.pdf
Avoiding Local Optima with User Demonstrations and Low-level Control:::Interactive Evolutionary Algorithms (IEAs) use human input to help drive a search process. Traditionally, IEAs allow the user to exhibit preferences among some set of individuals. Here we present a system in which the user directly demonstrates what he or she prefers. Demonstration has an advantage over preferences because the user can provide the system with a solution that would never have been presented to a user who can only provide preferences. However, demonstration exacerbates the user fatigue problem because it is more taxing than exhibiting preferences. The system compensates for this by retaining and reusing the user demonstration, similar in spirit to user modeling. The system is exercised on a robot locomotion and obstacle avoidance task that has an obvious local optimum. The user demonstration is provided through low-level control. The system is compared against a high-level fitness function that is susceptible to becoming trapped by a local optimum and a mid-level fitness function designed to remove the local optimum. We show that our proposed system outperforms most variants of these completely automatic methods, providing further evidence that Evolutionary Robotics (ER) can benefit by combining the intuitions of inexpert human users with the search capabilities of computers.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2013_CEC_Celis.pdf
Combining Fitness-based Search and User Modeling in Evolutionary Robotics:::Methodologies are emerging in many branches of computer science that demonstrate how human users and automated algorithms can collaborate on a problem such that their combined solutions outperform those produced by either humans or algorithms alone. The problem of behavior optimization in robotics seems particularly well-suited for this approach because humans have intuitions about how animals—and thus robots—should and should not behave, and can visually detect non-optimal behaviors that are trapped in local optima. Here we introduce a multiobjective approach in which a surrogate user (which stands in for a human user) deflects search away from local optima and a traditional fitness function eventually leads search toward the global optimum. We show that this approach produces superior solutions for a deceptive robotics problem compared to a similar search method that is guided by just a surrogate user or just a fitness function.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2013_GECCO_Bongard.pdf
On the Relationship Between Environmental and Morphological Complexity in Evolved Robots:::The principles of embodied cognition dictate that intelligent behavior must arise out of the coupled dynamics of an agent’s brain, body, and environment. While the relationship between controllers and morphologies (brains and bodies) has been investigated, little is known about the interplay between morphological complexity and the complexity of a given task environment. It is hypothesized that the morphological complexity of a robot should increase commensurately with the complexity of its task environment. Here this hypothesis is tested by evolving robot morphologies in a simple environment and in more complex environments. More complex robots tend to evolve in the more complex environments lending support to this hypothesis. This suggests that gradually increasing the complexity of task environments may provide a principled approach to evolving more complex robots.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2012_GECCO_Auerbach.pdf
On the Relationship Between Environmental and Mechanical Complexity in Evolved Robots:::According to the principles of embodied cognition, intelligent behavior must arise out of the coupled dynamics of an agent’s brain, body, and environment. This suggests that the morphological complexity of a robot should scale in relation to the complexity of its task environment. This idea is supported by recent work, which demonstrated that when evolving robot morphologies in simple and complex environments more complex robot morphologies do tend to evolve in more complex environments. Here this idea is extended to examining the mechanical complexity of evolved robots. Counter to intuition it is found that the mechanical complexity decreases in more complex environments.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2012_ALife_Auerbach.pdf
Accelerating Human-Computer Collaborative Search through Learning Comparative and Predictive User Models:::Interactive Evolutionary Algorithms (IEAs) are one of the few systems in which a human user and a computer algorithm are collaboratively working on a problem. To turn a basic IEA into the start of a Human-Computer Collaborative Computational system we have developed a system called The Approximate User (TAU). With TAU, as the user interacts with the IEA a model of the user’s preferences is constructed and continually refined and it is this user-model which drives search. Here two variations of a user-modeling approach are compared to determine if this approach can accelerate IEA search. The two user-modeling approaches compared are: 1. learning a classifier which correctly determines which of two designs is better; and 2. learning a model which predicts a fitness score. Rather than having people do the user-testing, we propose the use of a simulated user as an easier means to test IEAs. Both variants of the TAU IEA are compared against a basic IEA and it is shown that TAU is up to 2.7 times faster and 15 times more reliable at producing near optimal results.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2012_GECCO_Hornby.pdf
Learning Comparative User Models for Accelerating Human-Computer Collaborative Search:::Interactive Evolutionary Algorithms (IEAs) are a powerful explorative search technique that utilizes human input to make subjective decisions on potential problem solutions. But humans are slow and get bored and tired easily, limiting the usefulness of IEAs. Here we describe our system which works toward overcoming these problems, The Approximate User (TAU), and also a simulated user as a means to test IEAs. With TAU, as the user interacts with the IEA a model of the user’s preferences is constructed and continually refined and this model is what is used as the fitness function to drive evolutionary search. The resulting system is a step toward our longer term goal of building a human-computer collaborative search system. In comparing the TAU IEA against a basic IEA it is found that TAU is 2.5 times faster and 15 times more reliable at producing near optimal results.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2012_Evostar_Hornby.pdf
Evolving Complete Robots with CPPN-NEAT: The Utility of Recurrent Connections:::This paper extends prior work using Compositional Pattern Producing Networks (CPPNs) as a generative encoding for the purpose of simultaneously evolving robot morphology and control. A method is presented for translating CPPNs into complete robots including their physical topologies, sensor placements, and embedded, closed-loop, neural network control policies. It is shown that this method can evolve robots for a given task. Additionally it is demonstrated how the performance of evolved robots can be significantly improved by allowing recurrent connections within the underlying CPPNs. The resulting robots are analyzed in the hopes of answering why these recurrent connections prove to be so beneficial in this domain. Several hypotheses are discussed, some of which are refuted from the available data while others will require further examination.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2011_GECCO_Auerbach.pdf
Spontaneous Evolution of Structural Modularity in Robot Neural Network Controllers:::In order to evolve large robot controllers for increasingly complex tasks, fully connected neural networks are not feasible. However, manually designing sparse neural connectivity is not intuitive, and thus should be placed under evolutionary control. Here I show how spontaneous structural modularity can arise in the connectivity of evolved robot controllers if the controllers are boolean networks, and are selected to converge on point attractors that correspond to successful robot behaviors.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2011_GECCO1_Bongard.pdf
Morphological and Environmental Scaffolding Synergize when Evolving Robot Controllers:::Scaffolding—initially simplifying the task environment of autonomous robots—has been shown to increase the probability of evolving robots capable of performing in more complex task environments. Recently, it has been shown that changes to the body of a robot may also scaffold the evolution of non trivial behavior. This raises the question of whether two different kinds of scaffolding (environmental and morphological) synergize with one another when combined. Here it is shown that, for legged robots evolved to perform phototaxis, synergy can be achieved, but only if morphological and environmental scaffolding are combined in a particular way: The robots must first undergo morphological scaffolding, followed by environmental scaffolding. This suggests that additional kinds of scaffolding may create additional synergies that lead to the evolution of increasingly complex robot behaviors.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2011_GECCO2_Bongard.pdf
Dynamic Resolution in the Co-Evolution of Morphology and Control:::Evolutionary robotics is a promising approach to overcoming the limitations and biases of human designers in producing control strategies for autonomous robots. However, most work in evolutionary robotics remains solely concerned with optimizing control strategies for existing morphologies. By contrast, natural evolution, the only process that has produced intelligent agents to date, may modify both the control (brain) and morphology (body) of organisms. Therefore, coevolving morphology along with control may provide a better path towards realizing intelligent robots. This paper presents a novel method for coevolving morphology and control using CPPN-NEAT. This method is capable of dynamically adjusting the resolution at which components of the robot are created: a large number of small sized components may be present in some body locations while a smaller number of larger sized components is present in other locations. Advantages of this capability are demonstrated on a simple task, and implications for using this methodology to create more complex robots are discussed.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2010_ALife_Auerbach.pdf
Ensemble Pruning via Individual Contribution Ordering:::An ensemble is a set of learned models that make decisions collectively. Although an ensemble is usually more accurate than a single learner, existing ensemble methods often tend to construct unnecessarily large ensembles, which increases the memory consumption and computational cost. Ensemble pruning tackles this problem by selecting a subset of ensemble members to form subensembles that are subject to less resource consumption and response time with accuracy that is similar to or better than the original ensemble. In this paper, we analyze the accuracy/diversity trade-off and prove that classifiers that are more accurate and make more predictions in the minority group are more important for subensemble construction. Based on the gained insights, a heuristic metric that considers both accuracy and diversity is proposed to explicitly evaluate each individual classifier’s contribution to the whole ensemble. By incorporating ensemble members in decreasing order of their contributions, subensembles are formed such that users can select the top p percent of ensemble members, depending on their resource availability and tolerable waiting time, for predictions. Experimental results on 26 UCI data sets show that subensembles formed by the proposed EPIC (Ensemble Pruning via Individual Contribution ordering) algorithm outperform the original ensemble and a state-of- the-art ensemble pruning method, Orientation Ordering (OO) [16].::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2010_KDD_Lu.pdf
Evolving CPPNs to Grow Three-Dimensional Physical Structures:::The majority of work in the field of evolutionary robotics concerns itself with evolving control strategies for human designed or bio-mimicked robot morphologies. However, there are reasons why co-evolving morphology along with control may provide a better path towards realizing intelligent agents. Towards this goal, a novel method for evolving three-dimensional physical structures using CPPN-NEAT is introduced which is capable of producing artifacts that capture the non-obvious yet close relationship between function and physical structure. Moreover, it is shown how more fit solutions can be achieved with less computational effort by using growth and environmental CPPN input parameters as well as incremental changes in resolution.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2010_GECCO_Auerbach.pdf
A Probabilistic Functional Crossover Operator for Genetic Programming:::The original mechanism by which evolutionary algorithms were to solve problems was to allow for the gradual discovery of sub-solutions to sub-problems, and the automated combination of these sub-solutions into larger solutions. This latter property is particularly challenging when recombination is performed on genomes encoded as trees, as crossover events tend to greatly alter the original genomes and therefore greatly reduce the chance of the crossover event being beneficial. A number of crossover operators designed for tree-based genetic encodings have been proposed, but most consider crossing genetic components based on their structural similarity. In this work we introduce a tree-based crossover operator that probabilistically crosses branches based on the behavioral similarity between the branches. It is shown that this method outperforms genetic programming without crossover, random crossover, and a deterministic form of the crossover operator in the symbolic regression domain.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2010_GECCO_Bongard1.pdf
Guarding Against Premature Convergence while Accelerating Evolutionary Search:::The fundamental dichotomy in evolutionary algorithms is that between exploration and exploitation. Recently, several algorithms [8, 9, 14, 16, 17, 20] have been introduced that guard against premature convergence by allowing both exploration and exploitation to occur simultaneously. However, continuous exploration greatly increases search time. To reduce the cost of continuous exploration we combine one of these methods (the age-layered population structure (ALPS) algorithm [8, 9]) with an early stopping (ES) method [2] that greatly accelerates the time needed to evaluate a candidate solution during search. We show that this combined method outperforms an equivalent algorithm with neither ALPS nor ES, as well as regimes in which only one of these methods is used, on an evolutionary robotics task.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2010_GECCO_Bongard2.pdf
Adaptive Informative Sampling for Active Learning:::Many approaches to active learning involve periodically training one classifier and choosing data points with the lowest confidence, but designing a confidence measure is nontrivial. An alternative approach is to periodically choose data instances that maximize disagreement among the label predictions across an ensemble of classifiers. Many classifiers with different underlying structures could fit this framework, but some ensembles are more suitable for some data sets than others. The question then arises as to how to find the most suitable ensemble for a given data set. In this work we introduce a method that begins with a heterogeneous ensemble composed of multiple instances of different classifier types, which we call adaptive informative sampling. The algorithm periodically adds data points to the training set, adapts the ratio of classifier types in the heterogeneous ensemble in favor of the better classifier type, and optimizes the classifiers in the ensemble using stochastic methods. Experimental results show that the proposed method performs consistently better than homogeneous ensembles. Comparison with random sampling and uncertainty sampling shows that the algorithm effectively draws informative data points for training.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2010_SDM_Lu.pdf
Active Learning with Adaptive Heterogeneous Ensembles:::One common approach to active learning is to iteratively train a single classifier by choosing data points based on its uncertainty, but it is nontrivial to design uncertainty measures unbiased by the choice of classifier. Query by committee [1] suggests that given an ensemble of diverse but accurate classifiers, the most informative data points are those that cause maximal disagreement among the predictions of the ensemble members. However the method for finding ensembles appropriate to a given data set remains an open question. In this paper, the random subspace method is combined with active learning to create multiple instances of different classifier types, and an algorithm is introduced that adapts the ratio of different classifier types in the ensemble towards better overall accuracy. Here we show that the proposed algorithm outperforms C4.5 with uncertainty sampling, Naive Bayes with uncertainty sampling, bagging, boosting and the random subspace method with random sampling. To the best of our knowledge, our work is the first to adapt the ratio of classifiers in a heterogeneous ensemble for active learning.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2009_ICDM_Lu.pdf
Combined Structure and Motion Extraction from Visual Data Using Evolutionary Active Learning:::We present a novel stereo vision modeling framework that generates approximate, yet physically-plausible representations of objects rather than creating accurate models that are computationally expensive to generate. Our approach to the modeling of target scenes is based on carefully selecting a small subset of the total pixels available for visual processing. To achieve this, we use the estimation-exploration algorithm (EEA) to create the visual models: a population of three-dimensional models is optimized against a growing set of training pixels, and periodically a new pixel that causes disagreement among the models is selected from the observed stereo images of the scene and added to the training set. We show here that using only 5% of the available pixels, the algorithm can generate approximate models of compound objects in a scene. Our algorithm serves the dual goals of extracting the 3D structure and relative motion of objects of interest by modeling the target objects in terms of their physical parameters (e.g., position, orientation, shape, etc.), and tracking how these parameters vary with time. We support our claims with results from simulation as well from a real robot lifting a compound object.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2009_GECCO_Krishnanand.pdf
Evolution of Functional Specialization in a Morphologically Homogeneous Robot:::A central tenet of embodied artificial intelligence is that intelligent behavior arises out of the coupled dynamics between an agent’s body, brain and environment. It follows that the complexity of an agents’s controller and morphology must match the complexity of a given task. However, more complex task environments require the agent to exhibit different behaviors, which raises the question as to how to distribute responsibility for these behaviors across the agents’s controller and morphology. In this work a robot is trained to locomote and manipulate an object, but the assumption of functional specialization is relaxed: the robot has a segmented body plan in which the front segment may participate in locomotion and object manipulation, or it may specialize to only participate in object manipulation. In this way, selection pressure dictates the presence and degree of functional specialization rather than such specialization being enforced a priori. It is shown that for the given task, evolution tends to produce functionally specialized controllers, even though successful generalized controllers can also be evolved. Moreover, the robot’s initial conditions and training order have little effect on the frequency of finding specialized controllers, while the inclusion of additional proprioceptive feedback increases this frequency.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2009_GECCO_Auerbach.pdf
How Robot Morphology and Training Order Affect the Learning of Multiple Behaviors:::Automatically synthesizing behaviors for robots with articulated bodies poses a number of challenges beyond those encountered when generating behaviors for simpler agents. One such challenge is how to optimize a controller that can orchestrate dynamic motion of different parts of the body at different times. This paper presents an incremental shaping method that addresses this challenge: it trains a controller to both coordinate a robot’s leg motions to achieve directed locomotion toward an object, and then coordinate gripper motion to achieve lifting once the object is reached. It is shown that success is dependent on the order in which these behaviors are learned, and that despite the fact that one robot can master these behaviors better than another with a different morphology, this learning order is invariant across the two robot morphologies investigated here. This suggests that aspects of the task environment, learning algorithm or the controller dictate learning order more than the choice of morphology.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2009_CEC_Auerbach.pdf
Behavior Chaining: Incremental Behavior Integration for Evolutionary Robotics:::One of the open problems in autonomous robotics is how to consistently and scalably integrate new behaviors into a robot with an existing behavioral repertoire. In this work a new technique called behavior chaining is introduced, which allows for gradually expanding the behavioral repertoire of a dynamically behaving robot. The approach relies heavily on scaffolding: gradually restructuring the robot’s environment such that selection pressure favors the incorporation of a new behavior. This method teaches a robot a compound behavior not yet reported in the literature: dynamic legged locomotion toward an object followed by grasping, lifting and holding of that object in a physically-realistic three-dimensional environment. The method assumes that success is dependent on the order in which behaviors are learned. This is justified by results which show that if a robot is forced to learn lifting first and then incorporate locomotion, it eventually succeeds at both more often than a robot forced to learn locomotion first and then lifting.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2008_ALifeXI_Bongard.pdf
Synthesizing Physically-Realistic Environmental Models from Robot Exploration:::In previous work a framework was demonstrated that allows an autonomous robot to automatically synthesize physically-realistic models of its own body. Here it is demonstrated how the same approach can be applied to empower a robot to synthesize physically-realistic models of its surroundings. Robots which build numerical or other non-physical models of their environments are limited in the kinds of predictions they can make about the repercussions of future actions. In this paper it is shown that a robot equipped with a self-made, physically-realistic model can extrapolate: a slow-moving robot consistently predicts the much faster top speed at which it can safely drive across a terrain.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2007_ECAL_Bongard.pdf
Action-Selection and Crossover Strategies for Self-Modeling Machines:::In previous work a computational framework was demonstrated that employs evolutionary algorithms to automatically model a given system. This is accomplished by alternating the evolution of models with the evolutionary search for new training data. Theory predicts that the best new training data is that which induces maximum disagreement across the current model set. Here it is demonstrated that in a robot application this is not the case, and alternative fitness functions are developed that seek other, better training data. Also, it is shown that although crossover successfully reduces the mean error of the model set, it compromises the ability of the framework to find new, informative training data. This has implications for how to create adaptive, self-modeling machines, and suggests how competitive processes in the brain underlie the generation of intelligent behavior.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2007_GECCO_Bongard1.pdf
Exploiting Multiple Robots to Accelerate Self-Modeling:::In previous work a computational framework was demonstrated that allows a mobile robot to autonomously evolve models its own body for the purposes of adaptive behavior generation or recovery from damage. Conceivably, robots working in tandem could share their experiences such that one robot, when faced with a situation already encountered by another robot, could draw on that experience and adapt more rapidly. A first demonstration of this is given here: multiple robots with the same or similar body plan, but acting independently, combine self-models such that they accelerate modeling. Two approaches are investigated: the robots feed their experiences back into a common modeling engine, or they maintain their own modeling engine but share their best self-models with each other. It was found that the latter approach achieves a significant improvement in modeling compared to a single robot and compared to the former approach. This finding has implications for how to design autonomous robots acting in concert to achieve large-scale tasks.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2007_GECCO_Bongard2.pdf
Evolutionary Robotics for Legged Machines: From Simulation to Physical Reality:::This talk will outline challenges and opportunities in translating evolutionary learning of autonomous robotics from simulation to reality. It covers evolution and adaptation of both morphology and control, hybrid co-evolution of reality and simulation, handling noise and uncertainty, and morphological adaptation in hardware.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2006_IAS9_Lipson.pdf
Automated Synthesis of Body Schema using Multiple Sensor Modalities:::The way in which organisms create body schema, based on their interactions with the real world, is an unsolved problem in neuroscience. Similarly, in evolutionary robotics, a robot learns to behave in the real world either without re-course to an internal model (requiring at least hundreds of interactions), or a model is hand designed by the experimenter (requiring much prior knowledge about the robot and its environment). In this paper we present a method that allows a physical robot to automatically synthesize a body schema, using multimodal sensor data that it obtains through interaction with the real world. Furthermore, this synthesis can be either parametric (the experimenter provides an approximate model and the robot then refines the model) or topological: the robot synthesizes a predictive model of its own body plan using little prior knowledge. We show that this latter type of synthesis can occur when a physical quadrupedal robot performs only nine, 5-second interactions with its environment.::::::Published::::::http://creativemachines.cornell.edu/papers/Alife06_Bongard_Zykov.pdf
Reinventing the Wheel: Experiments in Evolutionary Geometry:::In the domain of design, there are two ways of viewing the competitiveness of evolved structures: they either improve in some manner on previous solutions; they produce alternative designs that were not previously considered; or they achieve both. In this paper we show that the way in which designs are genetically encoded influences which alternative structures are discovered, for problems in which a set of more than one optimal solution exists. The problem considered is one of the most ancient known to humanity: design a two-dimensional shape that, when rolled across flat ground, maintains a constant height. It was not until the late 19th century - roughly 7000 years after the discovery of the wheel - that Franz Reuleaux showed that a circle is not the only optimal solution. Here we demonstrate that artificial evolution repeats this discovery in under one hour.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2005_GECCO_Bongard2.pdf
'Managed Challenge' Alleviates Disengagement in Co-evolutionary System Identification:::In previous papers we have described a co-evolutionary algorithm (EEA), the estimation-exploration algorithm, that infers the hidden inner structure of systems using minimal testing. In this paper we introduce the concept of 'managed challenge' to alleviate the problem of disengagement in this and other co-evolutionary algorithms. A known problem in co-evolutionary dynamics occurs when one population systematically outperforms the other, resulting in a loss of selection pressure for both populations. In system identification (which deals with determining the inner structure of a system using only input/output data), multiple trials (a test that causes the system to produce some output) on the system to be identified must be performed. When such trials are costly, this disengagement results in wasted data that is not utilized by the evolutionary process. Here we propose that data from futile interactions should be stored during disengagement and automatically re-introduced later, when the population re-engages: we refer to this as the test bank. We demonstrate that the advantage of the test bank is two- fold: it allows for the discovery of more accurate models, and it reduces the amount of required training data for both parametric identification – parameterizing inner structure – and symbolic identification – approximating inner structure using symbolic equations – of nonlinear systems.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/GECCO05_Bongard.pdf
Three Dimensional Stochastic Reconfiguration of Modular Robots:::Here we introduce one simulated and two physical three-dimensional stochastic modular robot systems, all capable of self-assembly and self-reconfiguration. We assume that individual units can only draw power when attached to the growing structure, and have no means of actuation. Instead they are subject to random motion induced by the surrounding medium when unattached. We present a simulation environment with a flexible scripting language that allows for parallel and serial self-assembly and self-reconfiguration processes. We explore factors that govern the rate of assembly and reconfiguration, and show that self-reconfiguration can be exploited to accelerate the assembly of a particular shape, as compared with static self-assembly. We then demonstrate the ability of two different physical three-dimensional stochastic modular robot systems to self-reconfigure in a fluid. The second physical implementation is only composed of technologies that could be scaled down to achieve stochastic self-assembly and self-reconfiguration at the microscale.::::::Published::::::http://creativemachines.cornell.edu/papers/RSS05_White.pdf
Co-evolutionary Variance Can Guide Physical Testing in Evolutionary System Identification:::Co-evolution of system models and system tests can be used for exploratory system identification of physical platforms. Here we demonstrate how the amount of physical testing can be reduced by managing the difficulty that a population of tests poses to a population of candidate models. If test difficulty is not managed, then disengagement between the two populations occurs: The difficulty of the evolved test data supplied to the model population may grow faster than the ability of the models to explain them. Here we use variance of model outputs for a given test as a predictor of the tests’ difficulty. Proper engagement of the co-evolving populations is achieved by evolving tests that induce a particular amount of variance. We demonstrate this claim by identifying nonlinear dynamical systems using nonlinear models and linear approximation models.::::::Published::::::http://creativemachines.cornell.edu/papers/EH05_Zykov.pdf
An Exploration-Estimation Algorithm for Synthesis and Analysis of Engineering Systems Using Minimal Physical Testing:::We describe a new general algorithm for the automated design, analysis and repair of nonlinear physical systems. The process iterates a two-phase exploration-estimation cycle. The exploratory phase seeks a new improvement or test to perform to the system based on some initial internal model. The estimation phase performs the suggested operation and observes the outcomes; it then improves the internal model so as to explain all observations so far. This process relies on very few, targeted, and carefully planned interactions with the physical systems. We describe an implementation of this method using two evolutionary algorithms, where the exploratory phase uses a simulator to evolve improvements or tests, and the estimation phase uses observations to evolve the simulator itself. We demonstrate this algorithm for analysis, design and repair of electromechanical systems.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_DETC.pdf
Evolving Dynamics Gaits on a Physical Robot:::Here we introduce a method for the evolution of dynamic gaits on a physical robot requiring no prior assumptions about the locomotion pattern beyond the fact that it should be rhythmic. The dynamic gaits were physically evolved in hardware using a parallel-actuated pneumatic robot. We have formu- lated a genetic algorithm that evolves open-loop controllers; the encoding allows evolution to shape both the speed and pattern of locomotion while ensuring rhyth- micity. In future, we plan to evolve closed-loop controllers for the physical robot and integrate our previously developed methods to reduce the number of hard- ware trials.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_LateBreaking_Zykov.pdf
Once More Unto the Breach: Co-evolving a Robot and its Simulator:::One of the major challenges facing evolutionary robotics is crossing the reality gap: How to transfer evolved controllers from simulated robots to real robots while maintaining the behavior observed in simulation. Most attempts to cross the reality gap have either applied massive amounts of noise to the simulation, or conducted most or all of the evolution onboard the physical robot, an approach that can be prohibitively costly or slow. In this paper we present a new co-evolutionary approach, which we call the estimation-exploration algorithm. The algorithm automatically adapts the robot simulator using behavior of the target robot, and adapts the behavior of the robot using the robot simulator. This approach has four benefits: the process of simulator and controller evolution is automatic; it requires a minimum of hardware trials on the target robot; it could be used in conjunction with other approaches to automated behavior transferal from simulation to reality; and the algorithm itself is generalizable to other problem domains.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_ALife9.pdf
Automated Robot Function Recovery after Unanticipated Failure or Environmental Change using a Minimum of Hardware Trials:::Recovering functionality after unanticipated damage or environmental change, using a minimum amount of hardware testing, is a desirable and under-explored topic in evolutionary hardware and evolutionary robotics. In a previous paper we introduced a two-stage evolutionary algorithm, which we call the estimation-exploration algorithm, that evolves a robot simulator to accurately describe what damage a 'physical' robot has undergone, and then evolves a compensatory neural network in the evolved simulator that, when downloaded to the 'physical' robot, restores functionality. Here we introduce a new fitness metric that allows the algorithm to correctly describe not only complete but also partial failures, and also allows the algorithm to disambiguate between internal damage and external environmental change, based solely on sensory feedback. In most cases only four hardware evaluations are necessary in order to restore complete functionality to the 'physical' robot.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_EH_Bongard_Lipson.pdf
Automating Genetic Network Inference with Minimal Physical Experimentation Using Coevolution:::A major challenge in system biology is the automatic inference of gene regulation network topology - an instance of reverse engineering - based on limited local data whose collection is costly and slow. Reverse engineering implies the reconstruction of a hidden system based only on input and output data sets generated by the target system. Here we present a generalized evolutionary algorithm that can reverse engineer a hidden network based solely on input supplied to the network and the output obtained, using a minimal number of tests of the physical system. The algorithm has two stages: the first stage evolves a system hypothesis, and the second stage evolves a new experiment that should be carried out on the target system in order to extract the most information. We present the general algorithm, which we call the estimation-exploration algorithm, and demonstrate it both for the inference of gene regulatory networks without the need to perform expensive and disruptive knockout studies, and the inference of morphological properties of a robot without extensive physical testing.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_GECCO_Bongard.pdf
Automated Damage Diagnosis and Recovery for Remote Robotics:::Remote robotics applications, such as space exploration or operation in hazardous environments, would greatly benefit from automated recovery algorithms for unanticipated failure or damage. In this paper a two-stage evolutionary algorithm is introduced that forwards this aim by first evolving a damage hypothesis after failure and then re-evolving a compensatory neural controller to restore functionality. The algorithm pre-supposes that a continuous robot simulator is running onboard the physical robot; In this paper, the 'physical' robot is also simulated, but in future work the algorithm will be applied to a real, physical robot. Although evolutionary algorithms require a large number of evaluations to produce a useful solution, our preliminary results indicate that almost complete functionality can be restored after only three evaluations on the 'physical' robot, as opposed to over 3000 evaluations if the compensatory controller is evolved all on the physical robot. Our algorithm also has the benefit of producing a diagnostic model of the failure.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2004_ICRA_Bongard_Lipson.pdf
Evolved Sensor Fusion and Dissociation in an Embodied Agent:::W. Grey Walter first demonstrated that an autonomous robot could follow an environmental gradient to its source. In this paper, neural networks are evolved that allow a simulated, embodied quadrupedal agent to sense and follow an environmental gradient - in this case, local chemical concentration - to its source. Through a series of ablation experiments performed in silico, it is shown how artificial evolution gradually integrates and dissociates the different sensor modalities available to the agent in order to produce chemotacting behaviour. This work builds on that of Walter by indicating that evolutionary methods automatically generate chemotaxis by modulating simpler behaviours (here, forward locomotion) using a sensor modality (chemosensors) separate from those driving the simpler behaviour. This suggests that evolutionary methods are well suited for automatically generating behaviours more complex than chemotaxis by using it in turn as a base behaviour.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/BongardWGW02.pdf
Iterative Product Engineering: Evolutionary Robot Design:::This paper details an iterative, rapid method for digital mock-up and the evolutionary optimisation of a closed loop controller for highly dynamic gaits. The evolutionary approach in the virtual construction kit MorphEngine is investigated on its capacity to inspire and evolve behaviours for legged robots and non-biometric locomotion. The tool supports the engineer in finding, controlling and finally implementing gaits based on emergent dynamics on a real-world robot through the iterative exploitation of both the agents morphology and its physical environment.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/FrutigerClawar2002.pdf
A Method for Isolating Morphological Effects on Evolved Behaviour:::As the field of embodied cognitive science begins to mature, it is imperative to develop methods for identifying and quantifying the constraints and opportunities an agent’s body places on its possible behaviours. In this paper we present results from a set of experiments conducted on 10 different legged agents, in which we evolve neural controllers for locomotion. The genetic algorithm and neural network architecture were kept constant across the agent set, but the agents had different sizes, masses and body plans. It was found that increased mass has a negative effect on the evolution of locomotion, but that this does not hold for all of the agents tested. Also, the number of legs has an effect on evolved behaviours, with hexapedal agents being the easiest for which to evolve locomotion, and wormlike agents being the most difficult. Moreover, it was found that repeating the experiments with a larger neural network increased the evolutionary potential of some of the agents, but not for all of them. The results suggest that by employing this methodology we can test hypotheses about the behavioural effect of specific morphological features, which has to date eluded precise quantitative analysis.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/bongardPfeiferSAB2002.pdf
Relating Neural Network Performance to Morphological Differences in Embodied Agents:::::::::Published::::::
Evolving Modular Genetic Regulatory Networks:::In this paper we introduce a system that combines ontogenetic development and artificial evolution to automatically design robots in a physics-based, virtual environment. Through lesion experiments on the evolved agents, we demonstrate that the evolved genetic regulatory networks from successful evolutionary runs are more modular than those obtained from unsuccessful runs.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/BongardWCCI2002.pdf
The Road Less Travelled: Morphology in the Optimization of Biped Robot Locomotion:::In this paper, stable bipedal locomotion has been achieved using coupled evolution of morphology and control on a 5-link biped robot in a physics- based simulation environment. The robot was controlled by a closed loop recurrent neural network controller. The goal was to study the effect of macroscopic, midrange and microscopic changes in mass distribution along the biped skeleton to ascertain whether optimal morphology and control pairs could be discovered. The sensor-motor coupling determined that small changes in morphology manifest themselves as large changes in the performance of the biped, which were exploited by the optimization process. In this way, mechanical design and controller optimization were reduced to a single process, and more mutually optimized designs resulted. This work points to alternative routes for efficient automated and manual biped optimization.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/paulBongardIROS2001.pdf
Making Evolution an Offer It Can't Refuse: Morphology and the Extradimensional Bypass:::::::::Published::::::
Repeated Structure and Dissociation of Genotypic and Phenotypic Complexity in Artificial Ontogeny:::In this paper, a minimal model of ontogenetic development, combined with differential gene expression and a genetic algorithm, is used to evolve both the morphology and neural control of agents that perform a block-pushing task in a physically-realistic, virtual environment. We refer to this methodology as artificial ontogeny (AO). It is demonstrated that evolved genetic regulatory networks in AO give rise to hierarchical, repeated phenotypic structures. Moreover, it is shown that the indirect genotype to phenotype mapping results in a dissociation between the information content in the genome, and the complexity of the evolved agent. It is argued that these findings support the claim that artificial ontogeny is a useful design tool for the evolutionary design of virtual agents and real-world robots.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/geccoBongard2001.pdf
Investigating Morphological Symmetry and Locomotive Efficiency using Virtual Embodied Evolution:::::::::Published::::::
Reducing Collective Behavioural Complexity through Heterogeneity:::In this paper, the correlation between behavioural heterogeneity and behavioural complexity within groups of cooperating agents is investigated. this investigation is accomplished using the Legio system, a type of evolutionary algorithms for evolving group behaviours, and in which behavioural differences among agents in the group is subject to selection pressure. Two collective task domains are studied, and two types of control architecture for the agents are investigated. From the experiments reported here it is concluded that increased behavioural heterogeneity within a group leads to reduced control complexity, and also that limiting the size of control architectures within a group results in increased behavioural complexity within that group. It is argued that this correlation clarifies the relationship between robustness, division of labour and variation within cooperating agent populations, and also that heterogeneity can be a powerful design tool for robot group design.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2000_ALife_Bongard.pdf
The Legion System: A Novel Approach to Evolving Heterogeneity for Collective Problem Solving:::We investigate the dynamics of agent groups evolved to perform a collective task, and in which the behavioural heterogeneity of the group is under evolutionary control. Two task domains are studied: solutions are evolved for the two tasks using an evolutionary algorithm called the Legion system. A new metric of heterogeneity is also introduced, which measures the heterogeneity of any evolved group behaviour. It was found that the amount of heterogeneity evolved in an agent group is dependent of the given problem domain: for the first task, the Legion system evolved heterogenous groups; for the second task, primarily homogenous groups evolved. We conclude that the proposed system, in conjunction with the introduced heterogeneity measure, can be used as a tool for investigating various issues concerning redundancy, robustness and division of labour in the context of evolutionary approaches to collective problem solving.::::::Published::::::http://www.cs.uvm.edu/~jbongard/papers/2000_EuroGP_Bongard.pdf
Coevolutionary Dynamics of a Multi-Population Genetic Programming System:::::::::Published::::::
Standing Swells Surveyed Showing Surprisingly Stable Solutions for the Lorenz '96 Model:::The Lorenz '96 model is an adjustable dimension system of ODEs exhibiting chaotic behavior representative of dynamics observed in the Earth's atmosphere. In the present study, we characterize statistical properties of the chaotic dynamics while varying the degrees of freedom and the forcing. Tuning the dimensionality of the system, we find regions of parameter space with surprising stability in the form of standing waves traveling amongst the slow oscillators. The boundaries of these stable regions fluctuate regularly with the number of slow oscillators. These results demonstrate hidden order in the Lorenz '96 system, strengthening the evidence for its role as a hallmark representative of nonlinear dynamical behavior.:::http://www.uvm.edu/~cdanfort/main/publications_files/2013-frank-ijbc.jpg:::Published:::http://arxiv.org/abs/1312.5965:::
Chaotic flow in a 2D natural convection loop with heat flux boundaries:::This computational study investigates the nonlinear dynamics of unstable convection in a 2D thermal convection loop (i.e., thermosyphon) with heat flux boundary conditions. The lower half of the thermosyphon is subjected to a positive heat flux into the system while the upper half is cooled by an equal-but-opposite heat flux out of the system. Water is employed as the working fluid with fully temperature dependent thermophysical properties and the system of governing equations is solved using a finite volume method. Numerical simulations are performed for varying levels of heat flux and varying strengths of gravity to yield Rayleigh numbers ranging from 1.5 × 10^2 to 2.8 × 10^7. Simulation results demonstrate that multiple regimes are possible and include: (1) conduction, (2) damped, stable convection that asymptotes to steady-state, (3) unstable, Lorenz-like chaotic convection with flow reversals, and (4) high Rayleigh, aperiodic stable convection without flow reversals. Delineation of the various flow regimes, as characterized by the temporal evolution of bulk mass flow rate, is obtained in terms of heat flux, gravity, and the Rayleigh number. Temporal frequencies of the oscillatory behavior and residence time in a circulatory direction are explored and described for the various thermal and gravitational forcing (Rayleigh number) applied to the system.:::http://www.uvm.edu/~cdanfort/main/publications_files/2013-louisos-ijhmt.jpg:::Published::::::http://www.sciencedirect.com/science/article/pii/S0017931013001300
Nutrient enrichment alters dynamics in experimental plant populations:::Controlling weed populations requires an understanding of their underlying population dynamics which can be achieved through a combination of model development and long-term studies. In this paper, we develop models based on long-term data from experimental populations of the weedy annual plant Cardamine pensylvanica. Four replicate populations of C. pensylvanica were grown in growth chambers under three different nutrient levels but with all other environmental conditions held constant. We analyze the resulting time series using generalized additive models and perform stability analyses using Lyapunov exponents. Further, we test whether the proposed mechanism, delayed density dependence caused by maternal effects, is operating in our system by experimentally manipulating maternal density and assessing the resulting offspring quality. Our results show that that increasing the frequency of nutrients causes plant population dynamics to shift from stable to damped 2-point oscillations to longer cycles. This shift in population dynamics is due to a shift at high nutrients from populations being regulated by first order density feedbacks to being regulated by both first order and second order density feedbacks. A consequence of these first order and second order feedbacks was an increase in cycle lengths as demonstrated by the presence of complex eigenvalues. A short-term experiment confirmed that when grown under high nutrients, the density of maternal plants strongly affected offspring size, providing a mechanism whereby these second order density feedbacks could operate. Our results demonstrate that increasing nutrient frequency results in a qualitative shift in dynamics from stable to longer cycles.:::http://www.uvm.edu/~cdanfort/main/publications_files/2013-molofsky-ecology.jpg:::Published::::::http://link.springer.com/article/10.1007/s10144-013-0392-3
Estimating Distance to Critical Transitions from Time-series Synchrophasor Data:::The dynamical behavior of power systems under stress frequently deviates from the predictions of deterministic models. Model-free methods for detecting signs of excessive stress before instability occurs would therefore be valuable. The mathematical frameworks of 'fast-slow systems' and 'critical slowing down' can describe the statistical behavior of dynamical systems that are subjected to random perturbations as they approach points of instability. This paper builds from existing literature on fast-slow systems to provide evidence that time series data alone can be useful to estimate the temporal distance of a power system to a critical transition, such as voltage collapse. Our method is based on identifying evidence of critical slowing down in a single stream of synchronized phasor measurements. Results from a single machine, stochastic infinite bus model, a three machine/nine bus system and the Western North American disturbance of 10 August 1996 illustrate the utility of the proposed method.:::http://www.uvm.edu/~cdanfort/main/publications_files/2012-eduardo-ieee.jpg:::Published::::::http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true
Predicting &#64258;ow reversals in chaotic natural convection using data assimilation:::A simplified model of natural convection, similar to the Lorenz system, is compared to computational fluid dynamics simulations of a thermosyphon in order to test data assimilation (DA) methods and better understand the dynamics of convection. The thermosyphon is represented by a long time flow simulation, which serves as a reference ‘truth’. Forecasts are then made using the Lorenz-like model and synchronised to noisy and limited observations of the truth using DA. The resulting analysis is observed to infer dynamics absent from the model when using short assimilation windows. Furthermore, chaotic flow reversal occurrence and residency times in each rotational state are forecast using analysis data. Flow reversals have been successfully forecast in the related Lorenz system, as part of a perfect model experiment, but never in the presence of significant model error or unobserved variables. Finally, we provide new details concerning the fluid dynamical processes present in the thermosyphon during these flow reversals.:::http://www.uvm.edu/~cdanfort/main/publications_files/2012-harris-tellus.jpg:::Published::::::http://www.tellusa.net/index.php/tellusa/article/view/17598
Defining the Boundaries of Normal Thrombin Generation: Investigations into Hemostasis:::In terms of its soluble precursors, the coagulation proteome varies quantitatively among apparently healthy individuals. The significance of this variability remains obscure, in part because it is the backdrop against which the hemostatic consequences of more dramatic composition differences are studied. In this study we have defined the consequences of normal range variation of components of the coagulation proteome by using a mechanism-based computational approach that translates coagulation factor concentration data into a representation of an individual's thrombin generation potential. A novel graphical method is used to integrate standard measures that characterize thrombin generation in both empirical and computational models (e.g max rate, max level, total thrombin, time to 2 nM thrombin ('clot time')) to visualize how normal range variation in coagulation factors results in unique thrombin generation phenotypes. Unique ensembles of the 8 coagulation factors encompassing the limits of normal range variation were used as initial conditions for the computational modeling, each ensemble representing 'an individual' in a theoretical healthy population. These 'individuals' with unremarkable proteome composition was then compared to actual normal and 'abnormal' individuals, i.e. factor ensembles measured in apparently healthy individuals, actual coagulopathic individuals or artificially constructed factor ensembles representing individuals with specific factor deficiencies. A sensitivity analysis was performed to rank either individual factors or all possible pairs of factors in terms of their contribution to the overall distribution of thrombin generation phenotypes. Key findings of these analyses include: normal range variation of coagulation factors yields thrombin generation phenotypes indistinguishable from individuals with some, but not all, coagulopathies examined; coordinate variation of certain pairs of factors within their normal ranges disproportionately results in extreme thrombin generation phenotypes, implying that measurement of a smaller set of factors may be sufficient to identify individuals with aberrant thrombin generation potential despite normal coagulation proteome composition.:::http://www.uvm.edu/~cdanfort/main/publications_files/2012-danforth-plos.jpg:::Published::::::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0030385
Empirical correction of a toy climate model:::Improving the accuracy of forecast models for physical systems such as the atmosphere is a crucial ongoing effort. The primary focus of recent research on these highly nonlinear systems has been errors in state estimation, but as that error has been successfully diminished, the role of model error in forecast uncertainty has duly increased. The present study is an investigation of an empirical model correction procedure involving the comparison of short forecasts with a reference 'truth' system during a training period, in order to calculate systematic (1) state-independent model bias and (2) state-dependent error patterns. An estimate of the likelihood of the latter error component is computed from the current state at every time step of model integration. The effectiveness of this technique is explored in a realistic scenario, in which the model is structurally different (in dynamics, dimension, and parametrization) from the target system. Results suggest that the correction procedure is more effective for reducing error and prolonging forecast usefulness than parameter tuning. However, the cost of this increase in average forecast accuracy is the creation of substantial qualitative differences between the dynamics of the corrected model and the true system. A method to mitigate dynamical ramifications and further increase forecast accuracy is presented.:::http://www.uvm.edu/~cdanfort/main/publications_files/2012-allgaier-pre.jpg:::Published::::::http://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.026201
Aggressive shadowing of a low-dimensional model of atmospheric dynamics:::Predictions of the future state of the Earth’s atmosphere suffer from the consequences of chaos: numerical weather forecast models quickly diverge from observations as uncertainty in the initial state is amplified by nonlinearity. One measure of the utility of a forecast is its shadowing time, informally given by the period of time for which the forecast is a reasonable description of reality. The present work uses the Lorenz '96 coupled system, a simplified nonlinear model of atmospheric dynamics, to extend a recently developed technique for lengthening the shadowing time of a dynamical system. Ensemble forecasting is used to make forecasts with and without inflation, a method whereby the ensemble is regularly expanded artificially along dimensions whose uncertainty is contracting. The first goal of this work is to compare model forecasts, with and without inflation, to a true trajectory created by integrating a modified version of the same model. The second goal is to establish whether inflation can increase the maximum shadowing time for a single optimal member of the ensemble. In the second experiment the true trajectory is known a priori, and only the closest ensemble members are retained at each time step, a technique known as stalking. Finally, a targeted inflation is introduced to both techniques to reduce the number of instances in which inflation occurs in directions likely to be incommensurate with the true trajectory. Results varied for inflation, with success dependent upon the experimental design parameters (e.g. size of state space, inflation amount). However, a more targeted inflation successfully reduced the number of forecast degradations without significantly reducing the number of forecast improvements. Utilized appropriately, inflation has the potential to improve predictions of the future state of atmospheric phenomena, as well as other physical systems.:::http://www.uvm.edu/~cdanfort/main/publications_files/2012-ross-physica.jpg:::Published::::::http://www.sciencedirect.com/science/article/pii/S0167278911003411
A Numerical Investigation of 3-D Flow Regimes in a Toroidal Natural Convection Loop:::Transient laminar natural convection regimes occurring in a thermal convection loop heated from below and cooled from above are investigated numerically for a wide range of Rayleigh numbers spanning the interval from 10^3 to 2.6 × 10^7. In the model system, the lower half of the loop is heated and maintained at a constant high temperature, while the upper half is cooled and maintained at a constant low temperature. A three-dimensional numerical model based on the finite volume method is used to solve the system of governing flow equations. Simulations are performed using water as the working fluid (Pr = 5.83) and detailed numerical results are presented and discussed for conduction, steady convection, and unsteady flow regimes. Although this subject has attracted researchers for decades, there have been no detailed three-dimensional numerical simulations of the dynamics of flow in the thermal convection loop. The objective of the present study is to fill this gap by presenting the temporal evolution of the velocity and temperature fields at key locations within the system. Emphasis is given to the analysis of dynamical behavior of the flow during the unsteady regime. The complexity of flow in the loop, which is characterized by vertical structures and flow recirculation, is visualized for the first time by performing detailed 3-D numerical simulations.:::http://www.uvm.edu/~cdanfort/main/publications_files/2011-ridouane-ijhmt.jpg:::Published::::::http://www.sciencedirect.com/science/article/pii/S0017931011004510
The interplay of chaos between the terrestrial and giant planets:::We report on some simple experiments on the nature of chaos in our planetary system. We make the following interesting observations. First, we look at the system of Sun   four Jovian planets as an isolated five-body system interacting only via Newtonian gravity. We find that if we measure the Lyapunov time of this system across thousands of initial conditions all within observational uncertainty, then the value of the Lyapunov time seems relatively smooth across some regions of initial condition space, while in other regions it fluctuates wildly on scales as small as we can reliably measure using numerical methods. This probably indicates a fractal structure of Lyapunov exponents measured across initial condition space. Then, we add the four inner terrestrial planets and several post-Newtonian corrections such as general relativity into the model. In this more realistic Sun   eight-planet system, we find that the above structure of chaos for the outer planets becomes uniformly chaotic for almost all planets and almost all initial conditions, with a Lyapunov time-scale of about 5–20 Myr. This seems to indicate that the addition of the inner planets adds more chaos to the system. Finally, we show that if we instead remove the outer planets and look at the isolated five-body system of the Sun   four terrestrial planets, then the terrestrial planets alone show no evidence of chaos at all, over a large range of initial conditions inside the observational error volume. We thus conclude that the uniformity of chaos in the outer planets comes not from the inner planets themselves, but from the interplay between the outer and inner ones. Interestingly, however, there exist rare and isolated initial conditions for which one individual outer planetary orbit may appear integrable over a 200-Myr time-scale, while all the other planets simultaneously appear chaotic.:::http://www.uvm.edu/~cdanfort/main/publications_files/2010-hayes-mnras.jpg:::Published::::::http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2966.2010.17027.x/abstract
A 2-D numerical study of chaotic &#64258;ow in a natural convection loop:::This paper numerically investigates the nonlinear dynamics of the unstable convection regime of the thermal convection loop, an experimental analogue of the Lorenz model. The lower half of the toroidal loop is heated and maintained at a constant high temperature, while the upper half is cooled at a constant low temperature. Subject to the proper boundary conditions, the system of governing equations is solved using a finite volume method. The numerical simulations are performed for water corresponding to Pr = 5.83 and Rayleigh number varying from 1000 to 150,000. In the case of a loop heated from below and cooled from above, it has been demonstrated theoretically and experimentally in the literature that multiple flow regimes are possible. Numerical results in terms of streamlines, isotherms, and local heat flux distributions along the walls are presented for each flow regime. Although several studies have investigated the chaotic regime of convection loops, there have been no detailed numerical simulations of the dynamics of flow reversals. Fine-scale flow behavior during the transition from one flow direction to another is illustrated by the temporal evolution of temperature distribution, mass flow rate, and local heat flux at selected locations in the system. Issues related to the observed Kelvin–Helmholtz instabilities are discussed.:::http://www.uvm.edu/~cdanfort/main/publications_files/2009-ridouane-ijhmt.jpg:::Published::::::http://www.sciencedirect.com/science/article/pii/S0017931009005432
Accounting for Model Errors in Ensemble Data Assimilation:::This study addresses the issue of model errors with the ensemble Kalman filter. Observations generated from the NCEP–NCAR reanalysis fields are assimilated into a low-resolution AGCM. Without an effort to account for model errors, the performance of the local ensemble transform Kalman filter (LETKF) is seriously degraded when compared with the perfect-model scenario. Several methods to account for model errors, including model bias and system noise, are investigated. The results suggest that the two pure bias removal methods considered [Dee and Da Silva (DdSM) and low dimensional (LDM)] are not able to beat the multiplicative or additive inflation schemes used to account for the effects of total model errors. In contrast, when the bias removal methods are augmented by additive noise representing random errors (DdSM  and LDM ), they outperform the pure inflation schemes. Of these augmented methods, the LDM , where the constant bias, diurnal bias, and state-dependent errors are estimated from a large sample of 6-h forecast errors, gives the best results. The advantage of the LDM  over other methods is larger in data-sparse regions than in data-dense regions.:::http://www.uvm.edu/~cdanfort/main/publications_files/2009-li-mwr.jpg:::Published::::::http://journals.ametsoc.org/doi/abs/10.1175/2009MWR2766.1
Complex Dynamic Behavior During Transition in a Solid Combustion Model:::Through examples in a free-boundary model of solid combustion, this study concerns nonlinear transition behavior of small disturbances of front propagation and temperature as they evolve in time. This includes complex dynamics of period doubling, and quadrupling, and it eventually leads to chaotic oscillations. Within this complex dynamic domain we also observe a period six-folding. Both asymptotic and numerical solutions are studied.We show that for special parameters our asymptotic method with some dominant modes captures the formation of coherent structures. Finally, we discuss possible methods to improve our prediction of the solutions in the chaotic case.:::http://www.uvm.edu/~cdanfort/main/publications_files/2009-yu-complexity.jpg:::Published::::::http://onlinelibrary.wiley.com/doi/10.1002/cplx.20268/abstract
Dynamic Structure of Networks Updated According to Simple, Local Rules:::While most studies of deterministic network growth have been of one- or two-case models, here a more diverse and comprehensive method of deterministic network evolution is presented. The range of observed behavior is classified and the underlying causes of the various types of growth are investigated. The potential for prediction of the different types of network growth is also examined. It is discovered that a wide variety of behavior can be produced by a simple evolutionary setup and that the networks resulting from this method of evolution warrant further study.:::http://www.uvm.edu/~cdanfort/main/publications_files/2009-morrow-pre.jpg:::Published::::::http://journals.aps.org/pre/abstract/10.1103/PhysRevE.80.016103
The Impact of Uncertainty in a Blood Coagulation Model:::Deterministic mathematical models of biochemical processes operate as if the empirically derived rate constants governing the dynamics are known with certainty. Our objective in this study was to explore the sensitivity of a deterministic model of blood coagulation to variations in the values of its 44 rate constants. This was accomplished for each rate constant at a given time by defining a normalized ensemble standard deviation that accounted for the sensitivity of the predicted concentration of each protein species to variation in that rate constant (from 10 to 1000% of the accepted value). A mean coefficient of variation derived from the normalized ensemble standard deviation values for all protein species was defined to quantify the overall variation introduced into the model's predictive capacity at that time by the assumed uncertainty in that rate constant. A time-average value of the coefficient of variation over the 20-min simulation for each rate constant was then used to rank rate constants. The model's predictive capacity is particularly sensitive (50% of the aggregate variation) to uncertainty in five rate constants involved in the regulation of the formation and function of the factor VIIa–tissue factor complex. Therefore, our analysis has identified specific rate constants to which the predictive capability of this model is most sensitive and thus where improvements in measurement accuracy will yield the greatest increase in predictive capability.:::http://www.uvm.edu/~cdanfort/main/publications_files/2009-danforth-blood.jpg:::Published::::::http://imammb.oxfordjournals.org/content/26/4/323.abstract?keytype=ref
Impact of Online Empirical Model Correction on Nonlinear Error Growth:::The purpose of this study is to compare two methods of correcting the bias of a GCM; namely statistical correction performed a posteriori (offline) as a function of forecast length, and correction done within the model integration (online). The model errors of a low resolution GCM are estimated by the 6-hour forecast residual averaged over several years and used to correct the model. Both the offline and online corrections substantially reduce the model bias when applied to independent data. Their performance in correcting the model error is comparable at all lead times, but for lead times longer than 1-day the online corrected forecasts have smaller RMS forecast errors and larger anomaly correlations than offline corrected forecasts. These results indicate that the online correction reduces not only the growth of the bias but also the nonlinear growth of non-constant (state-dependent and random) forecast errors during the model integration.:::http://www.uvm.edu/~cdanfort/main/publications_files/2009-danforth-grl.jpg:::Published::::::http://onlinelibrary.wiley.com/doi/10.1029/2008GL036239/abstract
Using Singular Value Decomposition to Parameterize State-Dependent Model Errors:::The purpose of the present study is to use a new method of empirical model error correction, developed by Danforth et al. in 2007, based on estimating the systematic component of the nonperiodic errors linearly dependent on the anomalous state. The method uses singular value decomposition (SVD) to generate a basis of model errors and states. It requires only a time series of errors to estimate covariances and uses negligible additional computation during a forecast integration. As a result, it should be suitable for operational use at a relatively small computational expense. The method is tested with the Lorenz ’96 coupled system as the truth and an uncoupled version of the same system as a model. The authors demonstrate that the SVD method explains a significant component of the effect that the model’s unresolved state has on the resolved state and shows that the results are better than those obtained with Leith’s empirical correction operator. The improvement is attributed to the fact that the SVD truncation effectively reduces sampling errors. Forecast improvements of up to 1000% are seen when compared with the original model. The improvements come at the expense of weakening ensemble spread.:::http://www.uvm.edu/~cdanfort/main/publications_files/2008-danforth-jas.jpg:::Published::::::http://journals.ametsoc.org/doi/abs/10.1175/2007JAS2419.1
Estimating and Correcting Global Weather Model Error:::The purpose of the present study is to explore the feasibility of estimating and correcting systematic model errors using a simple and efficient procedure, inspired by papers by Leith as well as DelSole and Hou, that could be applied operationally, and to compare the impact of correcting the model integration with statistical corrections performed a posteriori. An elementary data assimilation scheme (Newtonian relaxation) is used to compare two simple but realistic global models, one quasigeostrophic and one based on the primitive equations, to the NCEP reanalysis (approximating the real atmosphere). The 6-h analysis corrections are separated into the model bias (obtained by time averaging the errors over several years), the periodic (seasonal and diurnal) component of the errors, and the nonperiodic errors. An estimate of the systematic component of the nonperiodic errors linearly dependent on the anomalous state is generated.Forecasts corrected during model integration with a seasonally dependent estimate of the bias remain useful longer than forecasts corrected a posteriori. The diurnal correction (based on the leading EOFs of the analysis corrections) is also successful. State-dependent corrections using the full-dimensional Leith scheme and several years of training actually make the forecasts worse due to sampling errors in the estimation of the covariance. A sparse approximation of the Leith covariance is derived using univariate and spatially localized covariances. The sparse Leith covariance results in small regional improvements, but is still computationally prohibitive. Finally, singular value decomposition is used to obtain the coupled components of the correction and forecast anomalies during the training period. The corresponding heterogeneous correlation maps are used to estimate and correct by regression the state-dependent errors during the model integration. Although the global impact of this computationally efficient method is small, it succeeds in reducing state-dependent model systematic errors in regions where they are large. The method requires only a time series of analysis corrections to estimate the error covariance and uses negligible additional computation during a forecast. As a result, it should be suitable for operational use at relatively small computational expense.:::http://www.uvm.edu/~cdanfort/main/publications_files/2007-danforth-mwr.jpg:::Published::::::http://journals.ametsoc.org/doi/abs/10.1175/MWR3289.1
Making Forecasts for Chaotic Physical Processes:::Making a prediction for a chaotic physical process involves specifying the probability associated with each possible outcome. Ensembles of solutions are frequently used to estimate this probability distribution. However, for a typical chaotic physical system H and model L of that system, no solution of L remains close to H for all time. We propose an alternative. This Letter shows how to inflate or systematically perturb the ensemble of solutions of L so that some ensemble member remains close to H for orders of magnitude longer than unperturbed solutions of L. This is true even when the perturbations are significantly smaller than the model error.:::http://www.uvm.edu/~cdanfort/main/publications_files/2006-danforth-prl.jpg:::Published::::::http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.96.144102
National Survey Respondents as Agents in a Model of Plug-In Hybrid Electric Vehicle Adoption:::::::::Unpublished::::::
Analysis of a consumer survey on plug-in hybrid electric vehicles:::Plug-in Hybrid Electric Vehicles (PHEVs) show potential to reduce greenhouse gas (GHG) emissions, increase fuel efficiency, and offer driving ranges that are not limited by battery capacity. However, these benefits will not be realized if consumers do not adopt this new technology. We administered a survey to 1000 stated U.S. residents, using Amazon Mechanical Turk, to better understand factors influencing the potential for PHEV market penetration. Our analysis of the survey results reveals quantitative patterns and correlations that expand the existing literature. For example, respondents who felt most strongly about reducing U.S. transportation energy consumption and cutting greenhouse gas emissions had, respectively, 71 and 44 times greater odds of saying they would consider purchasing a compact PHEV than those who felt least strongly about these issues. However, even the most inclined to consider a compact PHEV were not generally willing to pay more than a few thousand U.S. dollars extra for the sticker price. Consistent with prior research, we found that financial and battery-related concerns remain major obstacles to widespread PHEV market penetration. Our results may help inform governmental policies, manufacturer pricing and marketing strategies to promote consumer adoption of PHEVs.::::::Unpublished::::::http://www.cs.uvm.edu/~meppstei/personal/PHEVSurveyPaperAsSubmittedTRpartA.pdf
Minimization of Cost, Sediment Load, and Sensitivity to Climate Change in a Watershed Management Application:::One challenge of climate change adaptation is to design watershed-based stormwater management plans that meet current total maximum daily load targets and also take into consideration anticipated changes in future precipitation patterns. We present a multi-scale, multiobjective framework for generating a diverse family of stormwater best management practice (BMP) plans for entire watersheds. Each of these alternative BMP configurations are non-dominated by any other identified solution with respect to cost of the implementation of the management plan and sediment loading predicted at the outflow of the watershed; those solutions are then pruned with respect to dominance in sensitivity to predicted changes in precipitation patterns. We first use GIS data to automatically precompute a set of cost-optimal BMP configurations for each subwatershed, over its entire range of possible treatment levels. We then formulate each solution as a real-valued vector of treatment levels for the subwatersheds and employ a staged multiobjective optimization approach using differential evolution to generate sets of non- dominated solutions. Finally, selected solutions are mapped back to the corresponding preoptimized BMP configurations for each subwatershed. The integrated method is demonstrated on the Bartlett Brook mixed-used impaired watershed in South Burlington, VT, and patterns in BMP configurations along the non-dominated front are investigated. Watershed managers and other stakeholders could use this approach to assess the relative trade-offs of alternative stormwater BMP configurations.::::::Published::::::http://cs.uvm.edu/~meppstei/personal/EnvModAndSoft2013.pdf
Team Learning for Healthcare Quality Improvement:::In organized healthcare quality improvement collaboratives (QICs), teams of practitioners from different hospitals exchange information on clinical practices with the aim of improving health outcomes at their own institutions. However, what works in one hospital may not work in others with different local contexts because of nonlinear interactions among various demographics, treatments, and practices. In previous studies of collaborations where the goal is a collective problem solving, teams of diverse individuals have been shown to outperform teams of similar individuals. However, when the purpose of collaboration is knowledge diffusion in complex environments, it is not clear whether team diversity will help or hinder effective learning. In this paper, we rst use an agent-based model of QICs to show that teams comprising similar individuals outperform those with more diverse individuals under nearly all conditions, and that this advantage increases with the complexity of the landscape and level of noise in assessing performance. Examination of data from a network of real hospitals provides encouraging evidence of a high degree of similarity in clinical practices, especially within teams of hospitals engaging in QIC teams. However, our model also suggests that groups of similar hospitals could benet from larger teams and more open sharing of details on clinical outcomes than is currently the norm. To facilitate this, we propose a secure virtual collaboration system that would allow hospitals to efciently identify potentially better practices in use at other institutions similar to theirs without any institutions having to sacrice the privacy of their own data. Our results may also have implications for other types of data-driven diffusive learning such as in personalized medicine and evolutionary search in noisy, complex combinatorial optimization problems.::::::Published::::::http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=
Discovering Design Principles from Dominated Solutions:::Important progress has been made by many researchers in extracting fundamental designprinciples from patterns in design parameters along the nondominated front generated by evolutionary algorithms in biobjective optimization problems. However, to the best of our knowledge, no attention has been given to discovering design principles from the wealth of additional information available from patterns in dominated solutions. To explore the same, we use heatmaps of dominated solutions to visualize how relevant variables self-organize with respect to the objectives throughout the feasible region. We overlay ceteris paribus lines on these heatmaps to show how the objective values change when a given design variable is varied while all others are held constant. We use three biobjective optimization problems to demonstrate various ways in which these visualization techniques can provide additional useful information beyond that which can be determined from the nondominated front. Specically, we investigate a simple two-member truss design problem, a simple welded beam design problem, and a real-world watershed management design problem to illustrate: 1) how principles derived from the nondominated front alone can be misleading; 2) how new principles can be derived from the dominated solutions; and 3) how nondominated solutions can often be fragile with respect to assumptions about uncertain external forcing conditions, whereas solutions a short distance inside the front are often much more robust.::::::Published::::::http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=
Exploratory Analysis in Time-Varying data sets: a Healthcare Network Application:::We introduce a new method for exploratory analysis of large data sets with time-varying features, where the aim is to automatically discover novel relationships between features (over some time period) that are predictive of any of a number of time-varying outcomes (over some other time period). Using a genetic algorithm, we co-evolve (i) a subset of predictive features, (ii) which attribute will be predicted (iii) the time period over which to assess the predictive features, and (iv) the time period over which to assess the predicted attribute. After validating the method on 15 synthetic test problems, we used the approach for exploratory analysis of a large healthcare network data set. We discovered a strong association, with 100% sensitivity, between hospital participation in multi-institutional quality improvement collaboratives during or before 2002, and changes in the risk-adjusted rates of mortality and morbidity observed after a 1-2 year lag. The proposed approach is a potentially powerful and general tool for exploratory analysis of a wide range of time-series data sets.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/IJACSci-NarineManukyan2013.pdf
Searching the clinical fitness landscape:::Widespread unexplained variations in clinical practices and patient outcomes suggest major opportunities for improving the quality and safety of medical care. However, there is little consensus regarding how to best identify and disseminate healthcare improvements and a dearth of theory to guide the debate. Many consider multicenter randomized controlled trials to be the gold standard of evidence-based medicine, although results are often inconclusive or may not be generally applicable due to differences in the contexts within which care is provided. Increasingly, others advocate the use 'quality improvement collaboratives', in which multi-institutional teams share information to identify potentially better practices that are subsequently evaluated in the local contexts of specific institutions, but there is concern that such collaborative learning approaches lack the statistical rigor of randomized trials. Using an agent-based model, we show how and why a collaborative learning approach almost invariably leads to greater improvements in expected patient outcomes than more traditional approaches in searching simulated clinical fitness landscapes. This is due to a combination of greater statistical power and more context-dependent evaluation of treatments, especially in complex terrains where some combinations of practices may interact in affecting outcomes. The results of our simulations are consistent with observed limitations of randomized controlled trials and provide important insights into probable reasons for effectiveness of quality improvement collaboratives in the complex socio-technical environments of healthcare institutions. Our approach illustrates how modeling the evolution of medical practice as search on a clinical fitness landscape can aid in identifying and understanding strategies for improving the quality and safety of medical care.::::::Published::::::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0049901
Data-Driven Cluster Reinforcement and Visualization in Sparsely-Matched Self-Organizing Maps:::A self-organizing map (SOM) is a self-organized projection of high-dimensional data onto a typically 2-dimensional (2-D) feature map, wherein vector similarity is implicitly translated into topological closeness in the 2-D projection. However, when there are more neurons than input patterns, it can be challenging to interpret the results, due to diffuse cluster boundaries and limitations of current methods for displaying interneuron distances. In this brief, we introduce a new cluster reinforcement (CR) phase for sparsely-matched SOMs. The CR phase amplifies within-cluster similarity in an unsupervised, data-driven manner. Discontinuities in the resulting map correspond to between-cluster distances and are stored in a boundary (B) matrix. We describe a new hierarchical visualization of cluster boundaries displayed directly on feature maps, which requires no further clustering beyond what was implicitly accomplished during self-organization in SOM training. We use a synthetic benchmark problem and previously published microbial community profile data to demonstrate the benefits of the proposed methods.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/TNN-2011-B-3781.pdf
An agent-based model to study market penetration of plug-in hybrid electric vehicles:::A spatially explicit agent-based vehicle consumer choice model is developed to explore sensitivities and nonlinear interactions between various potential influences on plug-in hybrid vehicle (PHEV) market penetration. The model accounts for spatial and social effects (including threshold effects, homophily, and conformity) and media influences. Preliminary simulations demonstrate how such a model could be used to identify nonlinear interactions among potential leverage points, inform policies affecting PHEV market penetration, and help identify future data collection necessary to more accurately model the system. We examine sensitivity of the model to gasoline prices, to accuracy in estimation of fuel costs, to agent willingness to adopt the PHEV technology, to PHEV purchase price and rebates, to PHEV battery range, and to heuristic values related to gasoline usage. Our simulations indicate that PHEV market penetration could be enhanced significantly by providing consumers with ready estimates of expected lifetime fuel costs associated with different vehicles (e.g., on vehicle stickers), and that increases in gasoline prices could nonlinearly magnify the impact on fleet efficiency. We also infer that a potential synergy from a gasoline tax with proceeds is used to fund research into longer-range, lower-cost PHEV batteries.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/EnergyPolicy2011withErrata.pdf
Underdominance, Multiscale Interactions, and Self-Organizing Barriers to Gene Flow:::Understanding mechanisms for the evolution of barriers to gene flow within interbreeding populations continues to be a topic of great interest among evolutionary theorists. In this work, simulated evolving diploid populations illustrate how mild underdominance (heterozygote disadvantage) can be easily introduced at multiple loci in interbreeding populations through simultaneous or sequential mutational events at individual loci, by means of directional selection and simple forms of epistasis (non-linear gene-gene interactions). It is then shown how multiscale interactions (within-locus, between-locus, and between-individual) can cause interbreeding populations with multiple underdominant loci to self-organize into clusters of compatible genotypes, in some circumstances resulting in the emergence of reproductively isolated species. If external barriers to gene flow are also present, these can have a stabilizing effect on cluster boundaries and help to maintain underdominant polymorphisms, even when homozygotes have differential fitness. It is concluded that multiscale interactions can potentially help to maintain underdominant polymorphisms and may contribute to speciation events.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/JAEA09.pdf
Evolutionary Dynamics on Scale-Free Interaction Networks:::There has been a recent surge of interest in studying dynamical processes, including evolutionary optimization, on scale-free topologies. While various scaling parameters and assortativities have been observed in natural scale-free interaction networks, most previous studies of dynamics on scale-free graphs have employed a graph-generating algorithm that yields a topology with an uncorrelated degree distribution and a fixed scaling parameter. In this paper, we advance the understanding of selective pressure in scale-free networks by systematically investigating takeover times under local uniform selection in scale-free topologies with varying scaling exponents, assortativities, average degrees, and numbers of vertices. We demonstrate why the so-called characteristic path length of a graph is a nonlinear function of both scaling and assortativity. Neither the eigenvalues of the adjacency matrix nor the effective population size was sufficient to account for the variance in takeover times over the parameter space that was explored. Rather, we show that 97% of the variance of logarithmically transformed average takeover times, on all scale-free graphs tested, could be accounted for by a planar function of: 1) the average inverse degree (which captures the effects of scaling) and 2) the logarithm of the population size. Additionally, we show that at low scaling exponents, increasingly positive assortativities increased the variability between experiments on different random graph instances, while increasingly negative assortativities increased the variability between takeover times from different initial conditions on the same graph instances. We explore the mechanisms behind our sometimes counterintuitive findings, and discuss potential implications for evolutionary computation and other relevant disciplines.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/IEEETEC09.pdf
Pair Approximations of Takeover Dynamics in Regular Population Structures:::In complex adaptive systems, the topological properties of the interaction network are strong governing influences on the rate of flow of information throughout the system. For example, in epidemiological models, the structure of the underlying contact network has a pronounced impact on the rate of spread of infectious disease throughout a population. Similarly, in evolutionary systems, the topology of potential mating interactions (i.e., population structure) affects the rate of flow of genetic information and therefore affects selective pressure. One commonly employed method for quantifying selective pressure in evolutionary algorithms is through the analysis of the dynamics with which a single favorable mutation spreads throughout the population (a.k.a. takeover time analysis). While models of takeover dynamics have been previously derived for several specific regular population structures, these models lack generality. In contrast, so-called pair approximations have been touted as a general technique for rapidly approximating the flow of information in spatially structured populations with a constant (or nearly constant) degree of nodal connectivities, such as in epidemiological and ecological studies. In this work, we reformulate takeover time analysis in terms of the well-known Susceptible-Infectious-Susceptible model of disease spread and adapt the pair approximation for takeover dynamics. Our results show that the pair approx- imation, as originally formulated, is insufficient for approximating pre-equibilibrium dynamics, since it does not properly account for the interaction between the size and shape of the local neighborhood and the population size. After parameterizing the pair approximation to account for these influences, we demonstrate that the resulting pair approximation can serve as a general and rapid approximator for takeover dynamics on a variety of spatially-explicit regular interaction topologies with varying population sizes and varying uptake and reversion probabilities. Strengths, limitations, and potential applications of the pair approximation to evolutionary computation are discussed.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/Payne_EC_09aspublished.pdf
Genomic mining for complex disease traits with 'Random Chemistry':::Our rapidly growing knowledge regarding genetic variation in the human genome offers great potential for understanding the genetic etiology of disease. This, in turn, could revolutionize detection, treatment, and in some cases prevention of disease. While genes for most of the rare monogenic diseases have already been discovered, most common diseases are complex traits, resulting from multiple gene–gene and gene-environment interactions. Detecting epistatic genetic interactions that predispose for disease is an important, but computationally daunting, task currently facing bioinformaticists. Here, we propose a new evolutionary approach that attempts to hill-climb from large sets of candidate epistatic genetic features to smaller sets, inspired by Kauffman’s 'random chemistry' approach to detecting small auto-catalytic sets of molecules from within large sets. Although the algorithm is conceptually straightforward, its success hinges upon the creation of a fitness function able to discriminate large sets that contain subsets of interacting genetic features from those that don’t. Here, we employ an approximate and noisy fitness function based on the ReliefF data mining algorithm. We establish proof-of-concept using synthetic data sets, where individual features have no marginal effects. We show that the resulting algorithm can successfully detect epistatic pairs from up to 1,000 candidate single nucleotide polymorphisms in time that is linear in the size of the initial set, although success rate degrades as heritability declines. Research continues into seeking a more accurate fitness approximator for large sets and other algorithmic improvements that will enable us to extend the approach to larger data sets and to lower heritabilities.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/GPEM.pdf
:::::::::::::::
Noise pre-filtering techniques in fluorescence-enhanced optical tomography:::In this contribution, different measurement noise pre-filtering techniques were developed using frequency-domain fluorescence measurements of homogeneous breast phantoms. We demonstrated that implementing noise pre-filtering, based on modulation depth and measurement error in amplitude, can improve model match between experimental and simulated data under varying experimental conditions (target depths, 1-3 cm and fluorescence optical contrast, 1:0 and 100:1). Noise pre-filtering also improves the qualitative estimation of target(s) location in reconstructed images in deep target(s) when there was fluorescence in the background. Interestingly, decreases in model mismatch did not necessarily correlate with increases in reconstructed target accuracy. In addition, it was observed that pre-filtering measurement noise using different criteria can help differentiate target(s) from artifacts, thus possibly minimizing the false-positive cases in a clinical environment.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/OptExp07.pdf
Invasiveness in plant communities with feedbacks:::The detrimental effects of invasive plant species on ecosystems are well documented. While much research has focused on discovering ecological in&#64258;uences associated with invasiveness, it remains unclear how these in&#64258;uences interact, causing some introduced exotic species to become invasive threats. Here we develop a framework that incorporates the in&#64258;uences of propagule pressure, frequency independent growth rates, feedback relationships, resource competition and spatial scale of interactions. Our results show that these ecological in&#64258;uences interact in complex ways, resulting in expected outcomes ranging from inability to establish, to naturalization, to conditional invasion dependent on quantity and spatial distribution of propagules, to unconditional takeover. We propose a way to predict the likelihood of these four possible outcomes, for a species recently introduced into a given target community. Such information could enable conservation biologists to craft strategies and target remediation efforts more ef&#64257;ciently and effectively in order to help maintain biodiversity in ecological communities.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/EppsteinMolofskyEL2007.pdf
Spatio-temporal community dynamics induced by frequency dependent interactions:::A mathematical model incorporating the effects of possibly asymmetric frequency dependent interactions is proposed. Model predictions for an idealized two-species annual plant community with asymmetric linear frequency dependence are explored using (i) analytic mean field equilibrium predictions, (ii) deterministic, discrete-time, finite-population, mean field predictions, and (iii) stochastic, discrete-time, cellular automata predictions for a variety of sizes of the spatial interaction and dispersal neighborhoods. We define species interaction factors, ranging from 0 to 1, which incorporate both frequency independent and frequency dependent terms. The maximum competitive ability of a species is reduced unless species frequency is optimal based on species-specific frequency dependence coefficients, ranging from &#8722;1 to  1. Assuming that maximum competitive ability is identical for two species, they can coexist indefinitely when they have equal absolute magnitude or both have sufficiently negative frequency dependence. Although smaller scales of spatial interactions reduce the region of the parameter space in which stable coexistence is pre- dicted, the time to extinction of one species can be significantly increased or decreased by the locality of interactions, depending on whether the losing species has positive or negative frequency dependence, respectively. The sensitivity to initial conditions in the community at large is dramatically reduced as the spatial scale of interactions is decreased. As a consequence, smaller spatial interaction neighborhoods increase the ability of introduced species to invade established communities in regions of the parameter space not predicted by mean field approximations. In the 'loser positive, winner positive' regions, smaller scales of interaction dramatically increased invasiveness. In the 'loser positive, winner negative' regions of the parameter space, invasion success decreases, but time to extinction of the resident species during successful invasions increases, with an increase in the spatial scale of interactions. The 'loser negative, winner positive' regions were relatively insensitive to initial conditions, so invasion success was relatively high at a variety of spatial scales. Surprisingly, invasions in parts of this region are most often successful with intermediate neighborhood sizes, although the maximum time that the losing species could persist before being driven to extinction increases with an increase in the spatial scale of interactions. These results are explained by understanding cluster formation and density and the relative local interspecific dynamics in cluster interiors, exteriors, and boundaries. In summary, frequency dependent interactions, and the spatial scale on which these interactions occur, can have a big impact on spatio-temporal community dynamics, with implications regarding species coexistence and invasiveness. The model proposed herein provides a theoretical frame- work for studying frequency dependent interactions that may shed light on spatio-temporal dynamics in real ecological communities.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/EppsteinEtAlEcoMod06.pdf
Fluorescence Photon Migration by the Boundary Element Method:::The use of the boundary element method (BEM) is explored as an alternative to the finite element method (FEM) solution methodology for the elliptic equations used to model the generation and transport of fluorescent light in highly scattering media, without the need for an internal volume mesh. The method is appropriate for domains where it is reasonable to assume the fluorescent properties are regionally homogeneous, such as when using highly specific molecularly targeted fluorescent contrast agents in biological tissues. In comparison to analytical results on a homogeneous sphere, BEM predictions of complex emission fluence are shown to be more accurate and stable than those of the FEM. Emission fluence predictions made with the BEM using a 708-node mesh, with roughly double the inter-node spacing of boundary nodes as in a 6956-node FEM mesh, match experimental frequency-domain fluorescence emission measurements acquired on a 1087 cm^3 breast-mimicking phantom at least as well as those of the FEM, but require only 1/8 to 1/2 the computation time.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/JCP2005.pdf
Three-dimensional fluorescence lifetime tomography:::Near-infrared fluorescence tomography using molecularly targeted lifetime-sensitive, fluorescent contrast agents have applications for early-stage cancer diagnostics. Yet, although the measurement of fluorescent lifetime imaging microscopy (FLIM) is extensively used in microscopy and spectroscopy applications, demonstration of fluorescence lifetime tomography for medical imaging is limited to two-dimensional studies. Herein, the feasibility of three-dimensional fluorescence-lifetime tomography on clinically relevant phantom volumes is established, using (i) a gain- modulated intensified charge coupled device (CCD) and modulated laser diode imaging system, (ii) two fluorescent contrast agents, e.g., Indocyanine green and 3-3'-Diethylthiatricarbocyanine iodide differing in their fluorescence lifetime by 0.62 ns, and (iii) a two stage approximate extended Kalman filter reconstruction algorithm. Fluorescence measurements of phase and amplitude were acquired on the phantom surface under different target to background fluorescence absorption (70:1, 100:1) and fluorescence lifetime (1:1, 2.1:1) contrasts at target depths of 1.4–2 cm. The Bayesian tomography algorithm was employed to obtain three-dimensional images of lifetime and absorption owing to the fluorophores.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/lifetimeaspublished.pdf
Detection of single and multiple targets in tissue phantoms using fluorescence-enhanced optical imaging:::Imaging plays a central role in cancer diagnosis, therapy, and prognosis primarily through the depiction of anatomically defined abnormalities. With the wealth of information provided by the maturing fields of genomics and proteomics, the identification of molecular markers and targets promises contrast material–enhanced diagnostic imaging with specificity and sensitivity that is not otherwise possible with conventional anatomic imaging. If measurement approaches can result in the acquisition of signals from micromolar or smaller amounts of contrast agents within clinically relevant volumes, molecular imaging promises to improve diagnostic imaging and to affect the quality of care for patients with cancer.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/radiology05.pdf
Diagnostic imaging of breast cancer using fluorescence-enhanced optical tomography: phantom studies:::Molecular targeting with exogenous near-infrared excitable fluorescent agents using time-dependent imaging techniques may enable diagnostic imaging of breast cancer and prognostic imaging of sentinel lymph nodes within the breast. However, prior to the administration of unproven contrast agents, phantom studies on clinically relevant volumes are essential to assess the benefits of fluorescence-enhanced optical imaging in humans. Diagnostic 3-D fluorescence-enhanced optical tomography is demonstrated using 0.5 to 1 cm^3 single and multiple targets differentiated from their surroundings by indocyanine green (micromolar) in a breast-shaped phantom (10-cm diameter). Fluorescence measurements of referenced ac intensity and phase shift were acquired in response to point illumination measurement geometry using a homodyned intensified charge-coupled device system modulated at 100 MHz. Bayesian reconstructions show artifact-free 3-D images (3857 unknowns) from 3-D boundary surface measurements (126 to 439). In a reflectance geometry appropriate for prognostic imaging of lymph node involvement, fluorescence measurements were likewise acquired from the surface of a semi-infinite phantom (8*8*8 cm^3) in response to area illumination (12 cm^2) by excitation light. Tomographic 3-D reconstructions (24,123 unknowns) were recovered from 2-D boundary surface measurements (3194) using the modified truncated Newton’s method. These studies represent the first 3-D tomographic images from physiologically relevant geometries for breast imaging.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/JBO.pdf
Fluorescence-enhanced optical imaging of large phantoms using single and simultaneous dual point illumination geometries:::Fluorescence-enhanced optical tomography is typically performed using single point illumination and multiple point collection measurement geometry. Single point illumination is often insufficient to illuminate greater volumes of large phantoms and results in an inadequate fluorescent signal to noise ratio (SNR) for the majority of measurements. In this work, the use of simultaneous multiple point illumination geometry is proposed for acquiring a large number of fluorescent measurements with a sufficiently high SNR. As a feasibility study, dual point excitation sources, which are in-phase, were used in order to acquire surface measurements and perform three-dimensional reconstructions on phantoms of large volume and/or significant penetration depth. Measurements were acquired in the frequency–domain using a modulated intensified CCD imaging system under different experimental conditions of target depth (1.4–2.8 cm deep) with a perfect uptake optical contrast. Three-dimensional reconstructions of the fluorescence absorption from the dual point illumination geometry compare well with the reconstructions from the single point illumination geometry. Targets located up to 2 cm deep were located successfully, establishing the feasibility of reconstructions from simultaneous multiple point excitation sources. With improved excitation light rejection, multiple point illumination geometry may prove useful in reconstructing more challenging domains containing deeply embedded targets. Image quality assessment tools are required to determine the optimal measurement geometry for the largest set off imaging tasks.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/medphy-published-feb04.pdf
Coupled complex adjoint sensitivities for frequency-domain fluorescence tomography: theory and vectorized implementation:::We present a computationally efficient and accurate adjoint method for calculating coupled sensitivities of complex frequency-domain excitation and emission fluence to any underlying optical parameters in highly scattering media. The method is shown to be general and accurate. Novel vectorized implementations for finite element global matrix assembly and adjoint sensitivity calculations are shown to speed up calculations by orders of magnitude over traditional loop implementations, thereby making least-squares approaches to fluorescence tomography computationally practical.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/JCP03.pdf
The influence of the refractive index-mismatch at the boundaries measured in fluorescence-enhanced frequency-domain photon migration imaging:::Over the past decade, developments towards near-infrared (NIR) optical tomography involve the recovery of interior optical maps from boundary measurements using the first principles of light propagation models. The refractive-index mismatch parameter in the boundary condition of the light propagation model, namely the diffusion equation, can significantly impact model prediction of measurements and therefore image recovery. In this contribution, the influence of refractive-index mismatch parameter between predictions and referenced measurements of fluorescence-enhanced frequency-domain photon migration (FDPM) are established; its greater influence on emission over excitation predictions are demonstrated, and the methods to accurately determine refractive index mismatch parameter from basic principles are reviewed.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/OptExp02.pdf
Three-dimensional, Bayesian image reconstruction from sparse and noisy data sets: Near-infrared fluorescence tomography:::A method for inverting measurements made on the surfaces of tissues for recovery of interior optical property maps is demonstrated for sparse near-infrared (NIR) fluorescence measurement sets on large tissue-simulating volumes with highly variable signal-to-noise ratio. A Bayesian minimum-variance reconstruction algorithm compensates for the spatial variability in signal-to-noise ratio that must be expected to occur in actual NIR contrast-enhanced diagnostic medical imaging. Image reconstruction is demonstrated by using frequency-domain photon migration measurements on 256-cm^3 tissue-mimicking phantoms containing none, one, or two 1-cm^3 heterogeneities with 50- to 100-fold greater concentration of Indocyanine Green dye over background levels. The spatial parameter estimate of absorption owing to the dye was reconstructed from only 160 to 296 surface measurements of emission light at 830 nm in response to incident 785-nm excitation light modulated at 100 MHz. Measurement error of acquired fluence at fluorescent emission wavelengths is shown to be highly variable. Convergence and quality of image reconstructions are improved by Bayesian conditioning incorporating (i) experimentally determined measurement error variance, (ii) recursively updated estimates of parameter uncertainty, and (iii) dynamic zonation. The results demonstrate that, to employ NIR fluorescence-enhanced optical imaging for large volumes, reconstruction approaches must account for the large range of signal-to-noise ratio associated with the measurements.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/PNAS02.pdf
Error Consideration in Contrast-Enhanced Three Dimensional Optical Tomography:::We present three-dimensional tomographic images of the absorption coefficient that is due to the presence of a fluorophore reconstructed from frequency domain fluence measurements of a tissue phantom containing a single, fluorescence contrast-enhanced inclusion. We show that such a reconstruction may be improved when the importance of measurement error correlations between relative phase shift and amplitude is assessed and when measurements are preprocessed to reduce the magnitude and the bias of system error.::::::Published::::::http://www.opticsinfobase.org/ol/abstract.cfm?uri=ol-26-10-704
3-D Bayesian optical image reconstruction with domain decomposition:::Most current efforts in near-infrared optical tomography are effectively limited to two-dimensional reconstructions due to the computationally intensive nature of full three-dimensional (3-D) data inversion. Previously, we described a new computationally efficient and statistically powerful inversion method APPRIZE (automatic progressive parameter-reducing inverse zonation and estimation). The APPRIZE method computes minimum-variance estimates of parameter values (here, spatially variant absorption due to a fluorescent contrast agent) and covariance, while simultaneously estimating the number of parameters needed as well as the size, shape, and location of the spatial regions that correspond to those parameters. Estimates of measurement and model error are explicitly incorporated into the procedure and implicitly regularize the inversion in a physically based manner. The optimal estimation of parameters is bounds-constrained, precluding infeasible values. In this paper, the APPRIZE method for optical imaging is extended for application to arbitrarily large 3-D domains through the use of domain decomposition. The effect of sub-domain size on the performance of the method is examined by assessing the sensitivity for identifying 112 randomly located single-voxel heterogeneities in 58 3-D domains. Also investigated are the effects of unmodeled heterogeneity in background optical properties. The method is tested on simulated frequency-domain photon migration measurements at 100 MHz in order to recover absorption maps owing to fluorescent contrast agent. This study provides a new approach for computationally tractable 3-D optical tomography.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/tmi01.pdf
Biomedical optical tomography using dynamic parameterization and Bayesian conditioning on photon migration measurements:::Stochastic reconstruction techniques are developed for mapping the interior optical properties of tissues from exterior frequency-domain photon migration measurements at the air-tissue interface. Parameter fields of absorption cross section, fluorescence lifetime, and quantum efficiency are accurately reconstructed from simulated noisy measurements of phase shift and amplitude modulation by use of a recursive, Bayesian, minimum-variance estimator known as the approximate extended Kalman filter. Parameter field updates are followed by data-driven zonation to improve the accuracy, stability, and computational efficiency of the method by moving the system from an underdetermined toward an overdetermined set of equations. These methods were originally developed by Eppstein and Dougherty [Water Resources Res. 32, 3321 (1996)] for applications in geohydrology. Estimates are constrained to within feasible ranges by modeling of parameters as beta-distributed random variables. No arbitrary smoothing, regularization, or interpolation is required. Results are compared with those determined by use of Newton-Raphson-based inversions. The speed and accuracy of these preliminary Bayesian reconstructions suggest the near-future application of this inversion technology to three-dimensional biomedical imaging with frequency-domain photon migration.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/AO99.pdf
Efficient 3-D data inversion: Soil characterization and moisture monitoring from crosswell GPR at a Vermont test site:::We extend our methodology for three-dimensional parameter structure and value estimation and apply it to a Vermont test site. Ground-penetrating radar (GPR) cross-well travel times are inverted for estimation of heterogeneous GPR soil velocities before and after a controlled release of salt water in the unsaturated zone. The method, which is based on an approximation of the extended Kalman filter in conjunction with data-driven zonation, automatically estimates not only distributed zone values but also the number of zones, zone geometry, and zone covariance. Resultant GPR velocity estimates are shown to reduce travel time estimation errors and to be consistent with independent cone penetrometer measurements at all five walls at the site. Comparison of velocity estimates before and after forced injection of salt water is used to detect and visualize soil moisture patterns in three dimensions. By varying the 'cluster tolerance criterion' in the data-driven zonation process, the user can obtain a desired resolution of heterogeneity (number of zones used) in the resultant model.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/WRR98scanned.pdf
Simultaneous estimation of transmissivity values and zonation:::The extended Kalman &#64257;lter (EKF) has long been recognized as a powerful, yet computationally intensive, methodology for stochastic parameter estimation. Three improvements to traditional algorithms are presented and applied to heterogeneous transmissivity estimation. First, the costly EKF covariance updates are replaced by more ef&#64257;cient approximations. Second, the zonation structure of the distributed parameter &#64257;eld being estimated is dynamically determined and re&#64257;ned using a partitional clustering algorithm. Third, a new method of merging &#64257;rst and second moments of random &#64257;elds that have heterogeneous statistics is introduced. We apply this method, called random &#64257;eld union, as an alternative to conventional random &#64257;eld averaging for the systematic shrinking of covariance matrices as the dimensionality of the parameter space is reduced. The effects of these three improvements are examined. In applications to steady state groundwater &#64258;ow test problems, we show that the &#64257;rst and second improvements reduce the computational time requirements dramatically, while the second and third can improve the accuracy and stability of the results. The resulting integrated method is successfully applied to a larger, more realistic calibration test case under steady and cyclostationary &#64258;ow conditions (similar to regular seasonal &#64258;uctuations). When &#64258;ow is steady, the method can be viewed as iterative; when &#64258;ow is transient, the method is fully recursive.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/96WR02283.pdf
A comparative study of PVM workstation cluster implementations of a two-phase subsurface flow model:::Four versions of a message-passing, distributed-memory, distributed-disk multicomputer application for simulating two-phase fluid flow in porous media are studied. The FORTRAN 77 finite element code, solved in parallel using an iterative domain-decomposition method, was ported from a shared-memory multiprocessor implementation to a network of heterogeneous computers using the PVM (Parallel Virtual Machine) software system. Issues concerning the calculation of speedups in cluster computing are discussed, and the performance of the four versions is compared. Parallel efficiencies of up to nearly 60% were achieved using 17 homogeneous networked workstations; speedups were comparable to the shared-memory implementation. We conclude that network multicomputing using PVM is an attractive alternative to hardware multiprocessors for large-grained parallel implementations of computationally intensive problems in hydrology.::::::Published::::::http://search.proquest.com/docview/16643014/A8459193D6D2441FPQ/1?accountid=14679
The pit-trapping foraging strategy of the ant lion, Myrmeleon immaculatus:::Larvae of Myrmeleon immaculatus in large pits captured both large and small prey, while larvae in small pits captured only the small prey. Larvae in small pits did not respond to large ants, although they always responded by sand-flinging to small ants. Larvae in medium-sized pits often captured large ants only after prolonged and vigorous sand-flipping. Larvae in large pits usually captured large ants with relatively little sand-flipping. Pit enlargement and pit relocation in the laboratory were not significantly correlated with reduction of rations in the first 3 weeks after a pit was built. However, after a month without food, larvae on the average moved once every 10 days, built successively smaller pits, and moved longer distances before building a new pit. In the field pits were dug primarily in response to microclimatological factors and possibly edge-effects. The presence or absence of suitable prey at a site, per se, had no effect on whether or not a larva would dig a pit there. We conclude that these sit-and-wait predators have a relatively large repertoire of behavior that enhances their foraging success, and we contrast it with previously made optimal foraging models relating to pit locations, pit relocations, pit size and ant lion responses.::::::Published::::::http://link.springer.com/article/10.1007/BF00291906
Size and caste in temperature regulation by bumblebees:::Bumblebees ranging in mass from 65 mg (the smallest workers of the smallest species) to 830 mg (the largest queens of the largest species examined) maintained similar average thoracic temperatures (TsubTh) while foraging, even though the passive cooling rates of these bees over this size range varied fourfold. Although the bees regulated TsubTh between apparent lower and upper set points despite wide ranges of body size (and over wide ranges of ambient temperature), they allowed TsubTh to fluctuate between set points. The foraging activity of queens was relatively independent of ambient temperature (Tsuba), but workers (and particularly the smallest workers were often excluded at low Tsuba. Although the size-related rates of passive cooling of drones was similar to that of workers, they did not maintain the same TsubTh as workers on some kinds of flowers.::::::Published::::::http://www.jstor.org/stable/30155878
Heterothermia in foraging workers and drones of the bumblebee Bombus terricola:::While foraging from the dense inflorescences of spiraea (Spiraea latifolia) and goldenrods (Solidago sp.), both workers and drones often allowed thoracic temperature (TsubTh) to fall below the minimum for flight. The bees were physiologically capable of maintaining a high TsubTh, but the periodic decrease in TsubTh was strongly correlated with ambient temperature (Tsuba). Decreases of TsubTh were unrelated to fuel reserves carried in the honey stomach. Drones foraging from the inflorescences were more likely to have low TsubTh than workers, even though on the average they carried several times greater fuel reserves in their honey stomach. Within workers, old or parasitized (by conopid flies) individuals were more likely to forage with low TsubTh than young an unparasitized individuals. Workers, unlike drones, showed an increasing tendency to decrease TsubTh with decreasing body mass. Although the decrease in TsubTh while foraging ('torpor'), with its associated sluggishness, appears to function as an energy-conservation mechanism, it could also be a risk-averting mechanism. By maintaining a high TsubTh and flight readiness, workers can 'gamble' on the chance of finding a new food source, unlike drones who do not have the hive's energy resources to fall back on if they deplete their supply of stored fuel.::::::Published::::::http://www.jstor.org/stable/30155879
Improving Uniformity of Solution Spacing in Biobjective Evolution:::We introduce a new synergistic combination of features, some of which have previously been used individually but not together, to improve uniformity of spacing in evolved non-dominated sets, especially in biobjective problems. On five standard biobjective benchmark tests, these features are shown to enhance performance in distinct and complementary ways.::::::Published::::::http://cs.uvm.edu/~meppstei/personal/USMDE_Gecco_2013.pdf
Team Structure and Quality Improvement in Collaborative Environments:::Teams comprising diverse individuals have been shown to increase the collective creativity in jointly solving problems. However, in contexts where the purpose of collaboration is knowledge diffusion in complex environments, it is not clear whether team diversity will help or hinder effective learning. For example, in organized quality improvement collaboratives (QICs), healthcare institutions exchange information on clinical practices and outcomes with the aim of improving health outcomes at their own institutions. However, what works in one hospital may not work in others with different local contexts, due to non-linear interactions among various treatments and practices. While there is limited evidence that some QICs have resulted in improved care, it is not yet clear what factors contribute to the effectiveness of these team collaborations. In this study, we use an agent-based model to study how different strategies of team formation, including team diversity and size, affect quality improvement in simulated collaborative environments. We show that, in this context, teams comprising similar individuals outperform those with more diverse teams, and that this advantage increases with the complexity of the landscape and level of noise in assessing fitness. Furthermore, we show that larger teams of relatively homogeneous agents perform better than smaller teams, and that effective learning through team collaborations is dependent on the level of knowledge of team members’ performance levels. Thus, our results suggest that groups of similar hospitals should collaborate as a single team and openly share detailed information regarding their clinical practices and outcomes. To facilitate this, we propose a virtual collaboration framework that would allow hospitals to efficiently identify potentially better practices in use at other institutions similar to theirs, without any institutions having to sacrifice the privacy of their own data. Our results may also have implications for other types of data-driven diffusive learning, such as in personalized medicine.::::::Published::::::http://cs.uvm.edu/~meppstei/personal/CTS2013Corrected.pdf
Evolutionary Feature Selection for Classification: A Plug-In Hybrid Vehicle Adoption Application:::We present a real-world application utilizing a Genetic Algorithm (GA) for exploratory multivariate association analysis of a large consumer survey designed to assess potential consumer adoption of Plug-in Hybrid Electric Vehicles (PHEVs). The GA utilizes an intersection/union crossover operator, in conjunction with high background mutation rates, to achieve rapid multivariate feature selection. We experimented with two alternative fitness measures based on classification results of a naïve Bayes quadratic discriminant analysis; one fitness function rewarded only for correct classifications, and the other penalized for the degree of misclassification using a quadratic penalty function. We achieved high classification accuracy for three different survey outcome questions (with 3-, 5-, and 7- outcome classes, respectively). The quadratic penalty function yielded better overall results, returning smaller feature sets and overall more accurate contingency tables of predicted classes. Our results help to identify what consumer attributes best predict their likelihood of purchasing a PHEV. These findings will be used to better inform an existing agent-based model of PHEV market penetration, with the ultimate aim of helping auto manufacturers and policy makers identify leverage points in the system that will encourage PHEV market adoption.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/GECCO_Krupa2012.pdf
Evolutionary Mining for Multivariate Associations in Large Time-Varying data sets: a Healthcare Network Application:::We introduce a new method for exploratory analysis of large data sets with time-varying features, where the aim is to automatically discover novel relationships between features (over some time period) that are predictive of any of a number of time-varying outcomes (over some other time period). Using a genetic algorithm, we co-evolve (i) a subset of predictive features, (ii) which attribute will be predicted (iii) the time period over which to assess the predictive features, and (iv) the time period over which to assess the predicted attribute. After validating the method on 15 synthetic test problems, we used the approach for exploratory analysis of a large healthcare network data set. We discovered a strong association, with 100% sensitivity, between hospital participation in multi-institutional quality improvement collaboratives during or before 2002, and changes in the risk-adjusted rates of mortality and morbidity observed after a 1-2 year lag. The results provide indirect evidence that these quality improvement collaboratives may have had the desired effect of improving health care practices at participating hospitals. The proposed approach is a potentially powerful and general tool for exploratory analysis of a wide range of time-series data sets.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/GeccoPaperNarine2012.pdf
Differential Evolution of Constants in Genetic  Programming Improves Efficacy and Bloat:::We employ a variant of Differential Evolution (DE) for co-&#65532;evolution of real coefficients in Genetic Programming (GP). &#65532;This GP DE method is applied to 30 randomly generated symbolic regression problems of varying difficulty. Expressions were evolved on sparsely sampled points, but were evaluated for accuracy using densely sampled points over much wider ranges of inputs. The GP DE had successful runs on 25 of 30 problems, &#65532;whereas GP using Ephemeral Random Constants succeeded on only 6 and the multi-objective GP Eureqa on only 18. Although nesting DE slows down each GP generation significantly, successful GP DE runs required many fewer GP generations than the other methods and, in nearly all cases, the number of nodes in the best evolved trees were smaller in GP DE than with the other &#65532;GP methods.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/GPDE_GECCO_LBA_Shreya2012.pdf
An Agent-Based Model for Estimating Consumer Adoption of PHEV Technology:::This study presents a prototype of a spatially-explicit and socially-embedded agent based model to study adoption of plug-in hybrid vehicle (PHEV) technology under a variety of scenarios. Heterogeneous agents decide whether or not to buy a PHEV by weighing environmental benefits and financial considerations (based on their personal driving habits, their projections of future gas prices, and how accurately they estimate fuel costs), subject to various social influences. Proof-of-concept results are presented to illustrate the types of questions which could be addressed by such a model, and how they may help to inform policy-makers and/or vehicle manufacturers. For example, our results indicate that simple web-based tools for helping consumers to more accurately estimate relative fuel costs could dramatically increase PHEV adoption.::::::Published::::::http://www.uvm.edu/~transctr/publications/TRB_2010/10-3303.pdf
Up-scaling Agent-Based Discrete-Choice Transportation Models using Artificial Neural Networks:::Agent based models (ABMs) can be used for simulating consumer transportation discrete choices, while incorporating the effects of heterogeneous agent behaviors and social influences. However, the application of ABMs at large&#8208;scales may be computationally prohibitive (e.g., for millions of agents). In an attempt to harness the modeling capabilities of ABMs at large scales, we develop a recurrent artificial neural network (ANN) to replicate nonlinear spatio&#8208;temporal discrete choice patterns produced by a spatially&#8208;explicit ABM with social influence. This particular ABM has been developed to model consumer decision making between purchasing a Prius&#8208;like hybrid or plug&#8208;in hybrid electric vehicle (PHEV) for a given geographic region (e.g., city or town). Our goal is to see if an ANN trained at the city scale can operate as a “fast function approximator” to estimate nonlinear dynamic response functions (e.g., fleet distribution, environmental attitudes, etc.) based on city&#8208;wide attributes (e.g., socio&#8208;economic distributions). Recurrent feedback connections were added to the ANN to leverage the temporal history and correlations and improve forecasts in time. Outputs from the city&#8208;scale ABM, run for a variety of population sizes and initial and input conditions, were used to train and test the ANN. Initial results suggest the ABM may be replaced by ANNs that interact with each other and other agents (e.g., manufacturing agents) to investigate PHEV penetration at the national scale.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/ANN_TRB2010.pdf
Very Large Scale ReliefF for Genome-Wide Association Analysis:::The genetic causes of many monogenic diseases have already been discovered. However, most common diseases are actually the result of complex nonlinear interactions between multiple genetic and environmental components. There is thus a pressing need for new computational methods capable of detecting nonlinearly interacting single nucleotide polymorphism (SNPs) that are associated with disease, from amidst up to hundreds of thousands of candidate SNPs. Recently, some progress has been made using feature selection algorithms based on weights from the ReliefF data mining algorithm on sets of up to 1500 SNPs. However, the accuracy of ReliefF does not scale up to the sizes needed for truly large genome-scale SNP association studies. We propose a population-based variant dubbed VLSReliefF, which mitigates this performance drop by stochastically applying ReliefF to SNP subsets, and then assigning each SNP the maximum ReliefF weight it achieved in any subset. A heuristic method is proposed for determining the optimal subset size as a function of heritability, sample size, and order of interactions. The method is validated using a variety of computational experiments on synthetic datasets of up to 100,000 SNPs.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/EppsteinHaakeCIBCBfinal.pdf
The influence of Scaling and Assortativity on Takeover Times in Scale-Free Topologies:::In evolving systems, the topological characteristics of population structure have a pronounced impact on the rate of spread of advantageous alleles, and therefore affect selective pressure. One common method for quantifying the influence of population structure on selective pressure is through the analysis of the expected number of generations required for a single favorable allele to saturate an entire population (a.k.a. takeover time analysis). While takeover times have been thoroughly investigated in regular population structures, the selective pressures induced by irregular interaction topologies, such as scale-free graphs, have received much less attention. In this study, we systematically investigate the influence of scaling and assortativity, two frequently overlooked topological properties, on takeover times in scale-free population structures. Our results demonstrate that the scaling parameter and the magnitude and sign of assortativity have profound and unexpected nonlinear influences on takeover times in scale-free interaction topologies. We explore the reasons behind these results and suggest ways in which they may be exploited in future studies.::::::Published::::::http://dl.acm.org/citation.cfm?id=1389133
Parameterizing Pair Approximations for Takeover Dynamics:::Pair approximations have often been used to predict equilibrium conditions in spatially-explicit epidemiological and ecological systems. In this work, we investigate whether this method can be used to approximate takeover dynamics in spatially structured evolutionary algorithms. Our results show that the pair approximation, as originally formulated, is insufficient for approximating pre-equibilibrium dynamics, since it does not properly account for the interaction between the size and shape of the local neighborhood and the population size. After parameterizing the pair approximation to account for these influences, we demonstrate that the resulting system of differential equations can serve as a general and rapid approximator for takeover dynamics on a variety of spatially-explicit regular interaction topologies with varying population sizes. Strengths, limitations, and potential applications of the pair approximation to evolutionary computation are discussed.::::::Published::::::http://dl.acm.org/citation.cfm?id=1389047
Using Pair Approximations to Predict Takeover Dynamics in Spatially Structured Populations:::The topological properties of a network directly impact the flow of information through a system. For example, in natural populations, the network of inter-individual contacts affects the rate of flow of infectious disease. Similarly, in evolutionary systems, the topological properties of the underlying population structure affect the rate of flow of genetic information, and thus affect selective pressure. One commonly employed method for quantifying the influence of the population structure on selective pressure is through the analysis of takeover time. In this study, we reformulate takeover time analysis in terms of the well-known Susceptible-Infectious-Susceptible (SIS) model of disease spread. We then adapt an analytical technique, called the pair approximation, to provide a general model of takeover dynamics. We compare the results of this model to simulation data on a total of six regular population structures and discuss the strengths and limitations of the approximation.::::::Published::::::http://dl.acm.org/citation.cfm?id=1274025
Takeover Times on Scale-Free Topologies:::The topological properties of a network directly impact the flow of information through a system. In evolving populations, the topology of inter-individual interactions affects the rate of dissemination of advantageous genetic information and thus affects selective pressure. In this study, we investigate the selective pressures induced by several scale-free population structures using takeover time analysis. Previous results have shown that the selective pressures induced by scale-free interaction topologies are at least as strong as those induced by random and panmictic interaction topologies. In contrast, our results show that the selective pressures induced by scale-free interaction topologies are heavily influenced by their underlying topological properties, and can be tuned from a selective pressure close to that of a random or panmictic topology to a selective pressure that is weaker than that of a two-dimensional toroidal lattice with 3x3 rectangular neighborhoods of interactions. We also provide a detailed topological analysis of these population structures and discuss their influence on the observed dynamics in takeover times. We show that the expected takeover times observed on all population structures considered herein can be rapidly estimated using only a few readily computable metrics of the underlying topology, precluding the need to run expensive simulations or recursive probabilistic formulations.::::::Published::::::http://dl.acm.org/citation.cfm?id=1277018
Sensitivity of Self-Organized Speciation to Long Distance Dispersal:::Previous work has shown that speciation can result from the self-organized accumulation of multiple mildly underdominant (nearly neutral) loci in a continuous population, when mating is spatially localized. In contrast, when mating is panmictic, underdominance is quickly eliminated and the population always converges on a single genotype, as predicted by mean-field approximations. The focus of this work is to examine the sensitivity of selforganizing speciation to the assumption of purely localized interactions. We alter the interaction topology from nearest neighbor interactions to panmictic interactions in two ways: (i) by increasing the size of the contiguous mating neighborhoods and (ii) by allowing for long-distance dispersal of individuals with increasing probability. Our results show self-organized speciation to be robust to mating neighborhood sizes significantly larger than nearest neighbor interactions and to probabilities of long-distance dispersal that fall well into the range of so called “small-world ” interaction topologies::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/Payne_Speciation_IEEE07_aspublished.pdf
Why your mates shouldn’t date:::The topological properties of inter-individual interaction networks play a large role in governing the flow of genetic information throughout an evolving population. While a large amount of research has focused on the relationship between the topology of potential mating interactions (i.e. the population structure) and evolutionary dynamics, the relationship between the topology of actual mating interactions and evolutionary dynamics has received little attention. In a recent study [1], the concept of an emergent mating topology (EMT) was introduced in the context of generational genetic algorithms. One interesting observation made in [1] was that the clustering coefficient observed in each EMT was exceptionally small, even when the population evolved on a highly clustered population structure. In this study, we systematically investigate the relationship between increased clustering in the EMTs of panmictic genetic algorithms and evolutionary dynamics. This is achieved through the introduction of a new selection mechanism, referred to as Triad Selection, which allows for a tunable degree of clustering in the EMT.::::::Published::::::http://delivery.acm.org/10.1145/1280000/1277257/p1528-payne.pdf?ip=132.198.9.255
Feature Selection and Classification in Noisy Epistatic Problems using a Hybrid Evolutionary Approach:::A hybrid evolutionary approach is proposed for the combined problem of feature selection (using a genetic algorithm with Intersection/Union recombination and a fitness function based on a counter-propagation artificial neural network) and subsequent classifier construction (using strongly-typed genetic programming), for use in nonlinear association studies with relatively large potential feature sets and noisy class data. The method was tested using synthetic data with various degrees of injected noise, based on a proposed mental health database.allResults show the algorithm has good potential for feature selection, classification and function characterization.::::::Published::::::http://dl.acm.org/citation.cfm?id=1277331
Hill-climbing through 'random chemistry' for detecting epistasis:::There are estimated to be on the order of 10 6 single nucleotide polymorphisms (SNPs) existing as standing variation in the human genome. Certain combinations of these SNPs can interact in complex ways to predispose individuals for a variety of common diseases, even though individual SNPs may have no ill effects. Detecting these epistatic combinations is a computationally daunting task. Trying to use individual or growing subsets of SNPs as building blocks for detection of larger combinations of purely epistatic SNPs (e.g., via genetic algorithms or genetic programming) is no better than random search, since there is no predictive power in subsets of the correct set of epistatically interacting SNPs. Here, we explore the potential for hill-climbing from the other direction; that is, from large sets of candidate SNPs to smaller ones.::::::Published::::::http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.9930
Speciation by Self-Organizing Barriers to Gene Flow in Simulated Populations with Localized Mating:::Speciation caused by intrinsically forming barriers to gene flow is demonstrated using simulated populations. Although theory predicts that underdominance would be quickly eliminated from randomly mating populations, herein it is shown that when mating interactions are localized, mild underdominance can persist for long periods, as interbreeding populations self-organize into patches of compatible types separated by viable hybrid zones. Under certain types of even mild epistasis, hybrid zones will coalesce to create intrinsic barriers to gene flow between subgroups, resulting in speciation. Since underdominance, epistasis, and spatially localized mating/dispersal have all been observed in natural populations, the proposed mechanism is feasible and parsimonious. This model of speciation does not require any pre-mating isolation mechanisms, such as geographic isolation or assortative mating interactions due to niche differentiation or sexual selection. However, the presence of these would enhance the effects and reduce the time to speciation. It is probable that in natural systems, many mechanisms are operating simultaneously to cause speciation.::::::Published::::::http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.111.4966
Emergent Mating Topologies in Spatially Structured Genetic Algorithms:::The application of network analysis to emergent mating topologies in spatially structured genetic algorithms is presented in this preliminary study as a framework for inferring evolutionary dynamics in recombinant evolutionary search. Emergent mating topologies of populations evolving on regular, scale-free, and small-world imposed spatial topologies are analyzed. When the population evolves on a scale-free imposed spatial topology, the topology of mating interactions is also found to be scale-free. However, due to the random initial placement of individuals in the spatial topology, the scale-free mating topology lacks correlation between fitness and vertex connectivity, resulting in highly variable convergence rates. Scale-free mating topologies are also shown to emerge on regular imposed spatial topologies under high selection pressure. Since these scale-free emergent mating topologies self-organize such that the most-fit individuals are inherently located in highly connected vertices, such emergent mating topologies are shown to promote rapid convergence on the test problem considered herein. The emergent mating topologies of populations evolving on small-world imposed spatial topologies are not found to possess scale-free or small-world characteristics. However, due to the decrease in the characteristic path length of the emergent mating topology, the rate of population convergence is shown to increase as the imposed spatial topology is tuned from regular to small-world.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/Gecco06EMT.pdf
A Hybrid Genetic Algorithm with Pattern Search for finding Heavy Atoms in Protein Crystals:::One approach for determining the molecular structure of proteins is a technique called iso-morphous replacement, in which crystallographers dope protein crystals with heavy atoms, such as mercury or platinum. By comparing measured amplitudes of diffracted x-rays through protein crystals with and without the heavy atoms, the locations of the heavy atoms can be estimated. Once the locations of the heavy atoms are known, the phases of the diffracted x-rays through the protein crystal can be estimated, which in turn enables the structure of the protein to be estimated. Unfortunately, the key step in this process is the estimation of the locations of the heavy atoms, and this is a multi-modal, non-linear inverse problem. We report results of a pilot study that show that a 2-stage hybrid algorithm, using a stochastic genetic algorithm for stage 1 followed by a deterministic pattern search algorithm for stage 2, can successfully locate up to 5 heavy atoms in computer simulated crystals using noise free data. We conclude that the method may be a viable approach for finding heavy atoms in protein crystals, and suggest ways in which the approach can be scaled up to larger problems.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/payneEppsteinGecco2005.pdf
Fluorescence-enhanced optical tomography on large phantoms using dual point illumination geometry:::Three-dimensional fluorescence-enhanced optical tomography is demonstrated for the first time on large phantom volumes using simultaneous dual in-phase point illuminating sources. A rapid data acquiring gain-modulated intensified CCD imaging system was employed for these studies.::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?uri=BIO-2004-SA7
Fluorescence-enhanced optical tomography: Absorption and lifetime contrast studies:::Three-dimensional fluorescence-enhanced optical tomography on fluorescence absorption and fluorescence lifetime is demonstrated on clinically relevant phantom volumes employing a rapid data acquiring gain-modulated intensified CCD imaging system in the frequency-domain.::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?uri=BIO-2004-ThF20
Boundary Element Solution of the Coupled Fluorescence Diffusion Equations:::The use of the boundary element method is explored as an alternative solution methodology for the coupled elliptic equations used to model generation and transport of fluorescent light in highly scattering media.::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?uri=BIO-2004-ThF19
Codons in Evolutionary Computation:::A new method is developed for representation and encoding in population-based evolutionary algorithms. The method is inspired by the biological genetic code and utilizes a many-to-one, codon-based, genotype-to- phenotype translation scheme. A genetic algorithm was implemented with this codon-based representation using three different codon translation tables, each with different characteristics. A standard genetic algorithm is compared to the codon-based genetic algorithms on two difficult search problems; a dynamic knapsack problem and a static problem involving many suboptima. Results on these two problems indicate that the codon-based representation may promote rapid adaptation to changing environments and the ability to find global minima in highly non-convex problems.::::::Published::::::http://www.cs.uvm.edu/~meppstei/personal/gilberteppstein.pdf
Advances in 3-D frequency domain fluorescence tomography:::We present results of ongoing research in 3-D fluorescence tomography on large clinically-relevant tissue-mimicking domains. Finite element predictions of excitation and emission phase shift and amplitude attenuation are compared to experimental data from both column-shaped and breast-shaped tissue mimicking phantoms containing embedded fluorophore; system noise and measurement noise are characterized and utilized in image reconstruction using the Bayesian APPRIZE algorithm.::::::Published::::::http://spie.org/Publications/Proceedings/Paper/10.1117/12.478183
The benefits of vectorization in optical tomography:::In large 3-D finite element optical tomography problems, computation times for forward and adjoint solutions and for calculation of sensitivities can become prohibitive. Parallelization of computer codes can be used to obtain speedups approaching the number of processors employed, but parallel codes and computer systems can be difficult and expensive to develop and maintain. We show that by employing highly vectorized code that takes advantage of pipelining capabilities in the microprocessor we achieve dramatic speedups for forward and adjoint sensitivity calculations on a single processor microcomputer, and that these speedups actually increase as the problem size increases. Our vectorized implementations involve replication of large amounts of data and are thus memory intensive, however we effectively remove memory constraints by using domain decomposition to control the use of virtual memory. We show that global matrix assembly for a large (98,304 element) mesh is speeded up by a factor of 6.5 and adjoint sensitivity calculations of emission fluence with respect to fluorescence absorption are speeded up by a factor of 502 on a single-processor 2.2 GHz Pentium IV.::::::Published::::::http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=890660
Fluorescence-enhanced tomographic imaging in large phantoms using gain-modulated ICCD camera:::A frequency-domain photon migration (FDPM) imager employing an image-intensified CCD camera for fast data acquisition on a large tissue-mimicking phantom (1087 ml) is described. Fluorescence-enhanced imaging is performed employing frequency-domain techniques at 100 MHz in order to obtain the boundary measurements of phase and amplitude and to recover the interior optical maps using the first principles of light propagation. The effect of refractive-index parameter in the boundary condition of the light propagation model is not significant due to the large phantom volume and its curvilinear nature. Initial experiments were performed under perfect (1:0 contrast) and imperfect (100:1 contrast) uptake cases using indocyanine green as the contrast agent. Preliminary 3D image reconstructions using the approximate extended Kalman filter (AEKF) algorithm are presented.::::::Published::::::http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=890220
Experimental Frequency Domain Fluorescence Tomography:::::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?uri=BIO-2002-TuD3
Minimizing mismatch of forward model and experimental measurements for fluorescence-enhanced optical imaging:::::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?id=117757
A Method to Determine the Optimal Number of Measurements for Three-Dimensional Optical Tomography for a Physiologically Realistic Geometry:::::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?uri=BIO-2002-SuD27
Generalized Adjoint Sensitivities of the Coupled Frequency Domain Fluorescence Diffusion Equations:::The adjoint sensitivity method is applied to the coupled partial differential equations approximating complex fluence in fluorescing system. General equations are derived for Jacobian sensitivity matrices of complex fluence, at both excitation and emission wavelengths, with respect to arbitrary optical parameters. Finite element implementations of these equations are found to be computationally efficient and accurate.::::::Published::::::http://savannah.gatech.edu/people/ffedele/Research/adjoint_sensitivities.pdf
Measurement and Model Error Assessment of a Single Pixel, Frequency Domain Photon Migration Apparatus and Diffusion Model for Imaging Applications:::Research into the near-infrared biomedical optical imaging has produced a multitude of inverse imaging algorithms. Recent experience has shown that when these algorithms are tested with experimental data, they falter due to a mismatch between observed and simulated measurements. When considering measurements for imaging, one must consider both measurement and model error. If data is recorded properly, then measurement error tends to be normally distributed with a mean of zero. Model error can be biased and spatially correlated due to inaccuracies in the diffusion approximation, inaccurate parameter estimates, numerical error, and other factors. This contribution discusses trends in the measurement and model error observed from measurements on a single-pixel, frequency domain photon migration system developed for biomedical optical imaging. In order to reduce the model error bias, an empirical approach was applied to find experimental variables that significantly affect it. This approach reduced the mean of the model error on a test data set and produced a slight smoothing effect on its distribution. Image reconstruction attempts show that the modified data set produces an improved image over the image reconstructed from the raw data set. To our knowledge, this is the first time that model and measurement error information have been incorporated into a three dimensional image reconstruction algorithm.::::::Published::::::http://spie.org/Publications/Proceedings/Paper/10.1117/12.407630
Three-dimensional fluorescence absorption imaging with domain decomposition:::::::::Published::::::http://www.opticsinfobase.org/abstract.cfm?uri=BOSD-2000-MA3
Stochastic optical tomography using beta-distributed parameters to model absorption, lifetime, and quantum efficiency:::Stochastic methods originally devised for geophysical tomography are adapted to the biomedical optical tomography problem. Frequency domain measurements of modulated NIR light are inverted using a Bayesian approximate extended Kalman filter. Minimum variance updates for the linearized problem are calculated from explicit models of the parameters error covariance, the covariance of the system noise, and the measurement error covariance. The method is not iterative per se, but may be applied iteratively to account for strong nonlinearities. Data-driven zonation is used to dynamically reduce the parameterization for improved efficiency, sensitivity, and stability of the inversion. By modeling the parameters as beta distributed random variables, estimates are kept within feasible limits without ad hoc adjustments. In preliminary studies using synthetic domains we have successfully resolved spatially heterogeneous parameters such as absorption, fluorescence lifetime, and quantum efficiency. The method is shown to be much more accurate and computationally efficient than a more traditional Newton-Raphson method. On a 33 by 33 grid, distributed values of a single unknown parameter can be accurately identified in under 2 minutes on a 350 MHz Pentium.::::::Published::::::http://spie.org/Publications/Proceedings/Paper/10.1117/12.351037
3-D computed subsurface tomography:::We demonstrate a method for 3-D subsurface tomography in which crosswell tomographic signals are inverted in a novel way to yield 3-D parameter estimates. The inversion is accomplished via an approximate extended Kalman filter, in which geostatistics are used to implicitly smooth and regularize the inversion. Cluster analysis is used to dynamically determine the parameter dimensionality and structure; the ultimate resolution of heterogeneity is controlled by a cluster tolerance criterion. The size of the domain is incrementally increased as the estimation progresses, which keeps the computational requirements manageable. The method ultimately estimates the number, geometry, values and covariance of parameters.::::::Published::::::http://library.witpress.com/pages/PaperInfo.asp?PaperID=6356
Optimal 3-D geophysical tomography:::Acoustic and electromagnetic tomographic methods attempt to provide accurate and highly resolved estimates of spatially varying parameters. High resolution is obtained by discretizing the domain into a large number of parameters to be estimated. Because the resulting problem size is large, most implementations rely on iterative methods that attempt to minimize the output least-squares criterion through the repetitive application of relatively simple parameter updates.::::::Published::::::https://ccts.uvm.edu/catsearch/citations/3191
A practical parallel retrofit of a 3-Dimensional surface water model:::Laible [2] has implemented a sequential Fortran-77 code for a 3-dimensional surface water flow model. The model uses the Galerkin Finite Element Method (FEM) with triangular elements and linear basis functions to solve the vertically integrated wave formulation of the shallow water equations for surface water elevations in the 2-d horizontal domain. The full 3-dimensional velocity field is subsequently solved for by the Galerkin FEM along the 3rd (vertical) dimension using 1-dimensional linear elements at each of the nodes in the 2-D horizontal mesh.::::::Published::::::http://library.witpress.com/pages/PaperInfo.asp?PaperID=9994
Parameter estimation with data-driven zonation:::::::::Published::::::https://ccts.uvm.edu/catsearch/citations/3178
Parallel groundwater computations using PVM:::This paper reports an implementation of a PVM-FORTRAN code for two phase flow in porous media using PVM on both homogeneous and heterogeneous processor networks. Each processor runs the same source code asynchronously using single-program-multiple-data (SPDM) approach and synchronizes at convergence tests.::::::Published::::::https://ccts.uvm.edu/catsearch/citations/3179
Quantifying Information Flow During Emergencies:::Recent advances on human dynamics have focused on the normal patterns of human activities, with the quantitative understanding of human behavior under extreme events remaining a crucial missing chapter. This has a wide array of potential applications, ranging from emergency response and detection to traffic control and management. Previous studies have shown that human communications are both temporally and spatially localized following the onset of emergencies, indicating that social propagation is a primary means to propagate situational awareness. We study real anomalous events using country-wide mobile phone data, finding that information flow during emergencies is dominated by repeated communications. We further demonstrate that the observed communication patterns cannot be explained by inherent reciprocity in social networks, and are universal across different demographics.:::https://www.technologyreview.com/sites/default/files/images/Emergency response.png:::Published:::http://arxiv.org/abs/1401.1274:::http://www.nature.com/srep/2014/140206/srep03997/full/srep03997.html
Robustness of skeletons and salient features in networks:::Real world network datasets often contain a wealth of complex topological information. In the face of these data, researchers often employ methods to extract reduced networks containing the most important structures or pathways, sometimes known as 'skeletons' or 'backbones'. Numerous such methods have been developed. Yet data are often noisy or incomplete, with unknown numbers of missing or spurious links. Relatively little effort has gone into understanding how salient network extraction methods perform in the face of noisy or incomplete networks. We study this problem by comparing how the salient features extracted by two popular methods change when networks are perturbed, either by deleting nodes or links, or by randomly rewiring links. Our results indicate that simple, global statistics for skeletons can be accurately inferred even for noisy and incomplete network data, but it is crucial to have complete, reliable data to use the exact topologies of skeletons or backbones. These results also help us understand how skeletons respond to damage to the network itself, as in an attack scenario.:::http://comnet.oxfordjournals.org/content/early/2014/01/09/comnet.cnt019/F1.medium.gif:::Published:::http://arxiv.org/abs/1309.3797:::http://comnet.oxfordjournals.org/content/early/2014/01/09/comnet.cnt019
Eyjafjallajökull and 9/11: The Impact of Large-Scale Disasters on Worldwide Mobility:::Large-scale disasters that interfere with globalized socio-technical infrastructure, such as mobility and transportation networks, trigger high socio-economic costs. Although the origin of such events is often geographically confined, their impact reverberates through entire networks in ways that are poorly understood, difficult to assess, and even more difficult to predict. We investigate how the eruption of volcano Eyjafjallajökull, the September 11th terrorist attacks, and geographical disruptions in general interfere with worldwide mobility. To do this we track changes in effective distance in the worldwide air transportation network from the perspective of individual airports. We find that universal features exist across these events: airport susceptibilities to regional disruptions follow similar, strongly heterogeneous distributions that lack a scale. On the other hand, airports are more uniformly susceptible to attacks that target the most important hubs in the network, exhibiting a well-defined scale. The statistical behavior of susceptibility can be characterized by a single scaling exponent. Using scaling arguments that capture the interplay between individual airport characteristics and the structural properties of routes we can recover the exponent for all types of disruption. We find that the same mechanisms responsible for efficient passenger flow may also keep the system in a vulnerable state. Our approach can be applied to understand the impact of large, correlated disruptions in financial systems, ecosystems and other systems with a complex interaction structure between heterogeneous components.:::http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0069829.g001:::Published::::::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0069829
Social Networks in Emergency Response:::Large-scale emergencies and disasters are an ever-present threat to human society.With growing populations and looming threat of global climate change, the numbersof people at risk will continue to grow. Thus there is great need to optimize responseefforts from search and rescue to food and resource disbursement. Human dynamicsresearch offers a promising avenue to understand the behaviors of large populations, andmodern datasets derived from cutting-edge telecommunications such as online socialmedia and pervasive mobile phone systems bring a wealth of potential new information.Such massive data offers a promising complement to existing research efforts in disastersociology, which primarily focus on eyewitness interviews, surveys and other in-depthbut small-scale data.::::::Published::::::http://bagrow.com/pdf/2012-emergbook.pdf
The role of caretakers in disease dynamics:::One of the key challenges in modeling the dynamics of contagion phenomena is to understand how the structure of social interactions shapes the time course of a disease. Complex network theory has provided significant advances in this context. However, awareness of an epidemic in a population typically yields behavioral changes that correspond to changes in the network structure on which the disease evolves. This feedback mechanism has not been investigated in depth. For example, one would intuitively expect susceptible individuals to avoid other infecteds. However, doctors treating patients or parents tending sick children may also increase the amount of contact made with an infecteds, in an effort to speed up recovery but also exposing themselves to higher risks of infection. We study the role of these caretaker links in an adaptive network models where individuals react to a disease by increasing or decreasing the amount of contact they make with infected individuals. We find that pure avoidance, with only few caretaker links, is the best strategy for curtailing an SIS disease in networks that possess a large topological variability. In more homogeneous networks, disease prevalence is decreased for low concentrations of caretakers whereas a high prevalence emerges if caretaker concentration passes a well defined critical value.:::http://link.springer.com/static-content/images/773/art%3A10.1007%2Fs10955-013-0787-8/MediaObjects/10955_2013_787_Fig1_HTML.gif:::Published:::http://arxiv.org/abs/1209.2419:::http://link.springer.com/article/10.1007/s10955-013-0787-8
Natural emergence of clusters and bursts in network evolution:::Network models with preferential attachment, where new nodes are injected into the network and form links with existing nodes proportional to their current connectivity, have been well studied for some time. Extensions have been introduced where nodes attach proportionally to arbitrary fitness functions. However, in these models, attaching to a node always increases the ability of that node to gain more links in the future. We study network growth where nodes attach proportionally to the clustering coefficients, or local densities of triangles, of existing nodes. Attaching to a node typically lowers its clustering coefficient, in contrast to preferential attachment or rich-get-richer models. This simple modification naturally leads to a variety of rich phenomena, including aging, non-Poissonian bursty dynamics, and community formation. This theoretical model shows that complex network structure can be generated without artificially imposing multiple dynamical mechanisms and may reveal potentially overlooked mechanisms present in complex systems.:::http://journals.aps.org/prx/article/10.1103/PhysRevX.3.021016/figures/1/medium:::Published:::http://arxiv.org/abs/1209.3307:::http://journals.aps.org/prx/abstract/10.1103/PhysRevX.3.021016
Is coaching experience associated with effective use of timeouts in basketball?:::Experience is an important asset in almost any professional activity. In basketball, there is believed to be a positive association between coaching experience and effective use of team timeouts. Here, we analyze both the extent to which a team's change in scoring margin per possession after timeouts deviate from the team's average scoring margin per possession---what we called timeout factor, and the extent to which this performance measure is associated with coaching experience across all teams in the National Basketball Association over the 2009-2012 seasons. We find that timeout factor plays a minor role in the scoring dynamics of basketball. Surprisingly, we find that timeout factor is negatively associated with coaching experience. Our findings support empirical studies showing that, under certain conditions, mentors early in their careers can have a stronger positive impact on their teams than later in their careers.::::::Published:::http://arxiv.org/abs/1205.1492:::http://dx.doi.org/10.1038/srep00676
Communities and bottlenecks: Trees and treelike networks have high modularity:::Much effort has gone into understanding the modular nature of complex networks. Communities, also known as clusters or modules, are typically considered to be densely interconnected groups of nodes that are only sparsely connected to other groups in the network. Discovering high quality communities is a difficult and important problem in a number of areas. The most popular approach is the objective function known as modularity, used both to discover communities and to measure their strength. To understand the modular structure of networks it is then crucial to know how such functions evaluate different topologies, what features they account for, and what implicit assumptions they may make. We show that trees and treelike networks can have unexpectedly and often arbitrarily high values of modularity. This is surprising since trees are maximally sparse connected graphs and are not typically considered to possess modular structure, yet the nonlocal null model used by modularity assigns low probabilities, and thus high significance, to the densities of these sparse tree communities. We further study the practical performance of popular methods on model trees and on a genealogical data set and find that the discovered communities also have very high modularity, often approaching its maximum value. Statistical tests reveal the communities in trees to be significant, in contrast with known results for partitions of sparse, random graphs.:::http://journals.aps.org/pre/article/10.1103/PhysRevE.85.066118/figures/1/medium:::Published:::http://arxiv.org/abs/1201.0745:::http://journals.aps.org/pre/abstract/10.1103/PhysRevE.85.066118
Mesoscopic structure and social aspects of human mobility:::The individual movements of large numbers of people are important in many contexts, from urban planning to disease spreading. Datasets that capture human mobility are now available and many interesting features have been discovered, including the ultra-slow spatial growth of individual mobility. However, the detailed substructures and spatiotemporal flows of mobility - the sets and sequences of visited locations - have not been well studied. We show that individual mobility is dominated by small groups of frequently visited, dynamically close locations, forming primary 'habitats' capturing typical daily activity, along with subsidiary habitats representing additional travel. These habitats do not correspond to typical contexts such as home or work. The temporal evolution of mobility within habitats, which constitutes most motion, is universal across habitats and exhibits scaling patterns both distinct from all previous observations and unpredicted by current models. The delay to enter subsidiary habitats is a primary factor in the spatiotemporal growth of human travel. Interestingly, habitats correlate with non-mobility dynamics such as communication activity, implying that habitats may influence processes such as information spreading and revealing new connections between human mobility and social networks.:::http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0037676.g001:::Published:::http://arxiv.org/abs/1202.0224:::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0037676
Flavor network and the principles of food pairing:::The cultural diversity of culinary practice, as illustrated by the variety of regional cuisines, raises the question of whether there are any general patterns that determine the ingredient combinations used in food today or principles that transcend individual tastes and recipes. We introduce a flavor network that captures the flavor compounds shared by culinary ingredients. Western cuisines show a tendency to use ingredient pairs that share many flavor compounds, supporting the so-called food pairing hypothesis. By contrast, East Asian cuisines tend to avoid compound sharing ingredients. Given the increasing availability of information on food preparation, our data-driven investigation opens new avenues towards a systematic understanding of culinary practice.:::http://www.technologyreview.com/blog/arxiv/files/77421/Flavour network.png:::Published:::http://arxiv.org/abs/1111.6074:::http://dx.doi.org/10.1038/srep00196
Visualizing relations using the 'Observable Representation':::The observable representation provides an embedding of a discrete space in a low dimensional continuous space. Typically, the discrete space is a model of a complex system. This graphical representation is known to highlight significant properties of the original space and can serendipitously reveal unanticipated relationships. We report on the current status of this technique and give examples of its applications and rationale.::::::Published::::::http://www.worldscientific.com/doi/abs/10.1142/S0219525911003463
More Voices than Ever? Quantifying Bias in Social and Mainstream Media:::Social media, such as blogs, are often seen as democratic entities that allow more voices to be heard than the conventional mass or elite media. Some also feel that social media exhibits a balancing force against the arguably slanted elite media. A systematic comparison between social and mainstream media is necessary but challenging due to the scale and dynamic nature of modern communication. Here we propose empirical measures to quantify the extent and dynamics of social (blog) and mainstream (news) media bias. We focus on a particular form of bias - coverage quantity - as applied to stories about the 111th US Congress. We compare observed coverage of Members of Congress against a null model of unbiased coverage, testing for biases with respect to political party, popular front runners, regions of the country, and more. Our measures suggest distinct characteristics in news and blog media. A simple generative model, in agreement with data, reveals differences in the process of coverage selection between the two media.::::::Published:::http://arxiv.org/abs/1111.1227:::http://bagrow.com/pdf/Lin_MoreVoices_ICWSM2011.pdf
Collective Response of Human Populations to Large-Scale Emergencies:::Despite recent advances in uncovering the quantitative features of stationary human activity patterns, many applications, from pandemic prediction to emergency response, require an understanding of how these patterns change when the population encounters unfamiliar conditions. To explore societal response to external perturbations we identified real-time changes in communication and mobility patterns in the vicinity of eight emergencies, such as bomb attacks and earthquakes, comparing these with eight non-emergencies, like concerts and sporting events. We find that communication spikes accompanying emergencies are both spatially and temporally localized, but information about emergencies spreads globally, resulting in communication avalanches that engage in a significant manner the social network of eyewitnesses. These results offer a quantitative view of behavioral changes in human activity under extreme conditions, with potential long-term impact on emergency detection and response.:::http://www.plosone.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pone.0017680.g001:::Published:::http://arxiv.org/abs/1106.0560:::http://www.plosone.org/article/info:doi/10.1371/journal.pone.0017680
Robustness and modular structure in networks:::Many complex systems, from power grids and the internet, to the brain and society, can be modeled using modular networks. Modules, densely interconnected groups of elements, often overlap due to elements that belong to multiple modules. The elements and modules of these networks perform individual and collective tasks such as generating and consuming electrical load, transmitting data, or executing parallelized computations. We study the robustness of these systems to the failure of random elements. We show that it is possible for the modules themselves to become isolated or uncoupled (non-overlapping) well before the network falls apart. When modular organization is critical to overall functionality, networks may be far more vulnerable than expected.::::::Published:::http://arxiv.org/abs/1102.5085:::http://bagrow.com/pdf/robustness_modules_BLA.pdf
Link communities reveal multiscale complexity in networks:::Networks have become a key approach to understanding systems of interacting objects, unifying the study of diverse phenomena including biological organisms and human society. One crucial step when studying the structure and dynamics of networks is to identify communities: groups of related nodes that correspond to functional subunits such as protein complexes or social spheres. Communities in networks often overlap such that nodes simultaneously belong to several groups. Meanwhile, many networks are known to possess hierarchical organization, where communities are recursively grouped into a hierarchical structure. However, the fact that many real networks have communities with pervasive overlap, where each and every node belongs to more than one group, has the consequence that a global hierarchy of nodes cannot capture the relationships between overlapping groups. Here we reinvent communities as groups of links rather than nodes and show that this unorthodox approach successfully reconciles the antagonistic organizing principles of overlapping communities and hierarchy. In contrast to the existing literature, which has entirely focused on grouping nodes, link communities naturally incorporate overlap while revealing hierarchical organization. We find relevant link communities in many networks, including major biological networks such as protein-protein interaction and metabolic networks, and show that a large social network contains hierarchically organized community structures spanning inner-city to regional scales while maintaining pervasive overlap. Our results imply that link communities are fundamental building blocks that reveal overlap and hierarchical organization in networks to be two aspects of the same phenomenon.::::::Published:::http://arxiv.org/abs/0903.3178:::http://www.nature.com/nature/journal/v466/n7307/full/nature09182.html
Investigating Bimodal Clustering in Human Mobility:::We apply a simple clustering algorithm to a large dataset of cellular telecommunication records, reducing the complexity of mobile phone users' full trajectories and allowing for simple statistics to characterize their properties. For the case of two clusters, we quantify how clustered human mobility is, how much of a user's spatial dispersion is due to motion between clusters, and how spatially and temporally separated clusters are from one another.::::::Published:::http://arxiv.org/abs/0911.0674:::http://www.computer.org/csdl/proceedings/cse/2009/3823/04/3823e944-abs.html
Dynamic Computation of Network Statistics via Updating Schema:::In this paper we derive an updating scheme for calculating some important network statistics such as degree, clustering coefficient, etc., aiming at reduce the amount of computation needed to track the evolving behavior of large networks; and more importantly, to provide efficient methods for potential use of modeling the evolution of networks. Using the updating scheme, the network statistics can be computed and updated easily and much faster than re-calculating each time for large evolving networks. The update formula can also be used to determine which edge/node will lead to the extremal change of network statistics, providing a way of predicting or designing evolution rule of networks.::::::Published:::http://arxiv.org/abs/0809.4707:::http://journals.aps.org/pre/abstract/10.1103/PhysRevE.79.036116
Kleinberg navigation on anisotropic lattices:::We study the Kleinberg problem of navigation in Small World networks when the underlying lattice is stretched along a preferred direction. Extensive simulations confirm that maximally efficient navigation is attained when the length r of long-range links is taken from the distribution P(r)~r^-&#945;, when the exponent &#945; is equal to 2, the dimension of the underlying lattice, regardless of the amount of anisotropy, but only in the limit of infinite lattice size, L&#8594;&#8734;. For finite size lattices we find an optimal &#945;(L) that depends strongly on L. The convergence to &#945;=2 as L&#8594;&#8734; shows interesting power-law dependence on the anisotropy strength.::::::Published:::http://arxiv.org/abs/0805.0807:::http://dx.doi.org/10.1155/2008/346543
Phase transition in the rich-get-richer mechanism due to finite-size effects:::The rich-get-richer mechanism (agents increase their 'wealth' randomly at a rate proportional to their holdings) is often invoked to explain the Pareto power-law distribution observed in many physical situations, such as the degree distribution of growing scale free nets. We use two different analytical approaches, as well as numerical simulations, to study the case where the number of agents is fixed and finite (but large), and the rich-get-richer mechanism is invoked a fraction r of the time (the remainder of the time wealth is disbursed by a homogeneous process). At short times, we recover the Pareto law observed for an unbounded number of agents. In later times, the (moving) distribution can be scaled to reveal a phase transition with a Gaussian asymptotic form for r < 1/2 and a Pareto-like tail (on the positive side) and a novel stretched exponential decay (on the negative side) for r > 1/2.::::::Published:::http://arxiv.org/abs/0712.2220:::http://iopscience.iop.org/1751-8121/41/18/185001
Evaluating Local Community Methods in Networks:::We present a new benchmarking procedure that is unambiguous and specific to local community-finding methods, allowing one to compare the accuracy of various methods. We apply this to new and existing algorithms. A simple class of synthetic benchmark networks is also developed, capable of testing properties specific to these local methods.::::::Published:::http://arxiv.org/abs/0706.3880:::http://dx.doi.org/10.1088/1742-5468/2008/05/P05001
Portraits of Complex Networks:::We propose a method for characterizing large complex networks by introducing a new matrix structure, unique for a given network, which encodes structural information; provides useful visualization, even for very large networks; and allows for rigorous statistical comparison between networks. Dynamic processes such as percolation can be visualized using animations. Applications to graph theory are discussed, as are generalizations to weighted networks, real-world network similarity testing, and applicability to the graph isomorphism problem.::::::Published:::http://arxiv.org/abs/cond-mat/0703470:::http://iopscience.iop.org/0295-5075/81/6/68004/
Network Structure Revealed by Short Cycles:::This article explores the relationship between communities and short cycles in complex networks, based on the fact that nodes more densely connected amongst one another are more likely to be linked through short cycles. By identifying combinations of 3-, 4- and 5-edge-cycles, a subnetwork is obtained which contains only those nodes and links belonging to such cycles, which can then be used to highlight community structure. Examples are shown using a theoretical model (Sznajd networks) and a real-world network (NCAA football).::::::Published:::http://arxiv.org/abs/cond-mat/0612502:::http://bagrow.com/pdf/CyclesCommunities.pdf
On the Google-Fame of Scientists and Other Populations:::We study the fame distribution of scientists and other social groups as measured by the number of Google hits garnered by individuals in the population. Past studies have found that the fame distribution decays either in power-law [arXiv:cond-mat/0310049] or exponential [Europhys. Lett., 67, (4) 511-516 (2004)] fashion, depending on whether individuals in the social group in question enjoy true fame or not. In our present study we examine critically Google counts as well as the methods of data analysis. While the previous findings are corroborated in our present study, we find that, in most situations, the data available does not allow for sharp conclusions.::::::Published:::http://arxiv.org/abs/physics/0504034:::http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.2008594
A Local Method for Detecting Communities:::We propose a novel method of community detection that is computationally inexpensive and possesses physical significance to a member of a social network. This method is unlike many divisive and agglomerative techniques and is local in the sense that a community can be detected within a network without requiring knowledge of the entire network. A global application of this method is also introduced. Several artificial and real-world networks, including the famous Zachary Karate club, are analyzed.::::::Published:::http://arxiv.org/abs/cond-mat/0412482:::http://journals.aps.org/pre/abstract/10.1103/PhysRevE.72.046108
How Famous is a Scientist? -- Famous to Those Who Know Us:::Following a recent idea, to measure fame by the number of Google hits found in a search on the WWW, we study the relation between fame (Google hits) and merit (number of papers posted on an electronic archive) for a random group of scientists in condensed matter and statistical physics. Our findings show that fame and merit in science are linearly related, and that the probability distribution for a certain level of fame falls off exponentially. This is in sharp contrast with the original findings about WW II ace pilots, for which fame is exponentially related to merit (number of downed planes), and the probability of fame decays in power-law fashion. Other groups in our study show similar patterns of fame as for ace pilots.::::::Published:::http://arxiv.org/abs/cond-mat/0404515:::http://dx.doi.org/10.1209/epl/i2004-10104-y
Photometric Separation of Physical Properties of Stars:::Photometry collected using Sloan Digital Sky Survey &#64257;lter systems shows that it is possible to photometrically separate low metallicity stars with 0.5 < g &#8722;r < 0.8 using ugri &#64257;lters, and to separate stars with &#8722;0.2 < (g &#8722; r) < 0.25 by surface gravity using ugriz &#64257;lters. This con&#64257;rms the result of Lenz et al. 1998, which predicted from Kurucz model atmospheres that for G/K stars there was a relationship between metallicity and the l-parameter, l = &#8722;0.436(u&#8722;g)   0.693(g&#8722;r) 0.574(r&#8722;i) 0.199; and for A stars there was a relationship between surface gravity and the v-parameter, v = 0.283(u&#8722;g)&#8722;0.354(g &#8722;r) 0.455(r&#8722; i) 0.766(i&#8722;z). Photometric metallicities are a rough guide to sort metallicities, but can give very incorrect metallicities for unusual stars such as carbon stars and X-ray sources. The photometric metallicity determinations may make it possible to study the statistics of Galactic populations without time-consuming spectroscopic analysis, thus leveraging our ability to study Galactic structure and abundance gradients in the Galactic halo and thick disk. Application of the l-parameter to tens of millions of Galactic stars in the SDSS catalogs will allow us to select low metallicity candidates for further spectroscopic analysis. This technique is already being used for target selection in SEGUE, which is part of SDSS II, the three year extension to the SDSS.::::::Published::::::http://bagrow.com/pdf/RPI_REU_2002_publication.pdf
